{"slug": "amazon", "canonical_url": "https://www.johndcook.com/blog/amazon/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/Amazon.html", "title": "Amazon", "heading": null, "description": null, "summary": "If you buy something from Amazon starting here, I make a small commission. Thanks!", "word_count": 14, "blocks": [{"tag": "p", "text": "If you buy something from Amazon starting here, I make a small commission. Thanks!"}], "content": "If you buy something from Amazon starting here, I make a small commission. Thanks!"}
{"slug": "bessel_functions", "canonical_url": "https://www.johndcook.com/blog/bessel_functions/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/Bessel_functions.html", "title": "Relations between Bessel functions", "heading": "Relations between Bessel functions", "description": "A diagram with notes explaining how the various kinds of Bessel functions are related.", "summary": "The relationships between the various kinds of Bessel functions are summarized in the diagram below.", "word_count": 454, "blocks": [{"tag": "h1", "text": "Relations between Bessel functions"}, {"tag": "p", "text": "The relationships between the various kinds of Bessel functions are summarized in the diagram below."}, {"tag": "p", "text": "The functions in the left column are functions “of the first kind” and the functions in the right column are functions “of the second kind.” The functions in the middle column are “Hankel functions.” The functions in each row satisfy a particular differential equations. The lines between functions represent simple relationships. Details are given below."}, {"tag": "h2", "text": "Basic Bessel functions"}, {"tag": "p", "text": "The most basic Bessel functions are the functions Jν(z) imaginatively named “Bessel functions of the first kind.” I won’t go into their definition here, but I’ll explain how all the other functions in the Bessel function family reduce to these functions. For definitions and numerous identities, see functions.wolfram.com."}, {"tag": "p", "text": "The functions Yν(z) are called the “Bessel functions of the second kind.” They are linear combinations of Jν and J-ν:"}, {"tag": "p", "text": "Why define the functions Yν? They Yν functions are independent solutions to the differential equation that motivated the functions Jν. Specifically, Jν and Yν form a basis for the solutions to Bessel’s equation"}, {"tag": "p", "text": "In general, functions “of the second kind” were created to form indendent solutions to a differential equation satisfied by the corresponding function of the first kind."}, {"tag": "h2", "text": "Hankel functions"}, {"tag": "p", "text": "The Hankel functions are linear combinations of the Bessel functions of the first and second kind."}, {"tag": "h2", "text": "Spherical Bessel functions"}, {"tag": "p", "text": "Next we come to the spherical Bessel functions, so named because they arise when solving the Helmholtz equation in spherical coordinates."}, {"tag": "p", "text": "For integers n, the functions jn and yn are related to the functions Jn + ½ and Yn + ½."}, {"tag": "h2", "text": "Spherical Hankel functions"}, {"tag": "p", "text": "The spherical Bessel functions have their Hankel function counterparts h(1)n and h(2)n. These are formed from jn and yn exactly the same way H(1)n and H(2)n are formed from Jn and Yn."}, {"tag": "h2", "text": "Modified Bessel functions"}, {"tag": "p", "text": "Finally we have the functions Iν and Kν, the “modified Bessel functions of the first and second kind.” Iν is essentially Jν with its argument rotated. Specifically"}, {"tag": "p", "text": "Also, the functions Kν are related to the functions Iν analogously to the way the Yν are related to the Jν, namely"}, {"tag": "p", "text": "The functions Iν and Kν are independent solutions to the modified Bessel differential equation"}, {"tag": "p", "text": "The modified Bessel functions of the second kind were also known by several names that are seldom used anymore."}, {"tag": "li", "text": "Basset functions"}, {"tag": "li", "text": "Modified Bessel functions of the 3rd kind"}, {"tag": "li", "text": "Modified Hankel functions"}, {"tag": "li", "text": "Macdonald's functions"}, {"tag": "p", "text": "The diagram would be symmetric if Kν were as simply related to Yν as Iν is related to Jν. Unfortunately, that is not the case."}, {"tag": "h2", "text": "Other diagrams on this site"}, {"tag": "p", "text": "See this page for more diagrams on this site including diagrams for probability and statistics, analysis, topology, and category theory."}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Relations between Bessel functions\n\nThe relationships between the various kinds of Bessel functions are summarized in the diagram below.\n\nThe functions in the left column are functions “of the first kind” and the functions in the right column are functions “of the second kind.” The functions in the middle column are “Hankel functions.” The functions in each row satisfy a particular differential equations. The lines between functions represent simple relationships. Details are given below.\n\nBasic Bessel functions\n\nThe most basic Bessel functions are the functions Jν(z) imaginatively named “Bessel functions of the first kind.” I won’t go into their definition here, but I’ll explain how all the other functions in the Bessel function family reduce to these functions. For definitions and numerous identities, see functions.wolfram.com.\n\nThe functions Yν(z) are called the “Bessel functions of the second kind.” They are linear combinations of Jν and J-ν:\n\nWhy define the functions Yν? They Yν functions are independent solutions to the differential equation that motivated the functions Jν. Specifically, Jν and Yν form a basis for the solutions to Bessel’s equation\n\nIn general, functions “of the second kind” were created to form indendent solutions to a differential equation satisfied by the corresponding function of the first kind.\n\nHankel functions\n\nThe Hankel functions are linear combinations of the Bessel functions of the first and second kind.\n\nSpherical Bessel functions\n\nNext we come to the spherical Bessel functions, so named because they arise when solving the Helmholtz equation in spherical coordinates.\n\nFor integers n, the functions jn and yn are related to the functions Jn + ½ and Yn + ½.\n\nSpherical Hankel functions\n\nThe spherical Bessel functions have their Hankel function counterparts h(1)n and h(2)n. These are formed from jn and yn exactly the same way H(1)n and H(2)n are formed from Jn and Yn.\n\nModified Bessel functions\n\nFinally we have the functions Iν and Kν, the “modified Bessel functions of the first and second kind.” Iν is essentially Jν with its argument rotated. Specifically\n\nAlso, the functions Kν are related to the functions Iν analogously to the way the Yν are related to the Jν, namely\n\nThe functions Iν and Kν are independent solutions to the modified Bessel differential equation\n\nThe modified Bessel functions of the second kind were also known by several names that are seldom used anymore.\n\nBasset functions\n\nModified Bessel functions of the 3rd kind\n\nModified Hankel functions\n\nMacdonald's functions\n\nThe diagram would be symmetric if Kν were as simply related to Yν as Iν is related to Jν. Unfortunately, that is not the case.\n\nOther diagrams on this site\n\nSee this page for more diagrams on this site including diagrams for probability and statistics, analysis, topology, and category theory.\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "bessel_python", "canonical_url": "https://www.johndcook.com/blog/bessel_python/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/Bessel_python.html", "title": "Bessel functions in SciPy", "heading": "Bessel functions in SciPy", "description": "An overview of Bessel functions in Python’s SciPy library.", "summary": "Python’ SciPy library has many functions for evaluating Bessel functions. All are contained in the scipy.special subpackage.", "word_count": 424, "blocks": [{"tag": "h1", "text": "Bessel functions in SciPy"}, {"tag": "p", "text": "Python’ SciPy library has many functions for evaluating Bessel functions. All are contained in the scipy.special subpackage."}, {"tag": "p", "text": "There are several varieties of Bessel functions, all closely related as summarized in the diagram below."}, {"tag": "p", "text": "Each letter is the conventional symbol for a family of Bessel functions. The lines between letters indicates a simple relationship between function families. For more on the definition of each function and how they are related, see Relations between Bessel functions."}, {"tag": "p", "text": "Each family of Bessel functions is indexed by a parameter called the order. By convention, the order is denoted n when integer-valued and ν (Greek nu) when more generally real-valued. These notes will focus on the general case of real orders."}, {"tag": "p", "text": "The table below summarizes the correspondence between Bessel functions and their SciPy implementations."}, {"tag": "p", "text": "In general SciPy names use a Roman letter v at the end of a function corresponding a mathematical function with a Greek ν subscript. Similarly, SciPy uses n for integer-order specializations that would be denoted with a subscript n. The Python functions jv, yv, and kv have integer specializations jn, yn, and kn."}, {"tag": "p", "text": "All SciPy functions in the table take two arguments. The first is the order and the second is the parameter. For example, Jv(z) is evaluated using jn(v, z)."}, {"tag": "p", "text": "The Hankel functions H(1) and H(2) are implemented in the SciPy functions hankel1 and hankel2. SciPy does not contain functions for implementing the spherical Hankel functions h(1) and h(2), though these functions can be easily be computed in terms of the spherical Bessel functions. See relationship here."}, {"tag": "p", "text": "The spherical Bessel functions jn and yn have integer order and are implemented in the SciPy functions sph_jn and sph_yn. These SciPy functions are unusual in two ways: they return values of the functions up to and including the specified order, and they return derivative values as well. For example, sph_jn(2, 3.4) returns a pair of arrays. The first array contains j0(3.4), j1(3.4), and j2(3.4) and the second array contains j'0(3.4), j'1(3.4), and j'2(3.4)."}, {"tag": "p", "text": "SciPy includes optimized specializations for Bessel functions of orders 0 and 1. The functions j0 and j1 are optimized versions of jv with first argument 0 and 1 respectively. There are analogous optimized versions of yv, iv, and kv."}, {"tag": "p", "text": "SciPy contains many other functions related to Bessel functions: exponentially scaled Bessel functions, derivatives and integrals of Bessel functions, and zeros of Bessel functions. It also contains implementations of Ricatti-Bessel functions and Struve functions, functions closely related to the Bessel functions discussed here."}, {"tag": "p", "text": "Related page: Gamma and related functions in SciPy."}], "content": "Bessel functions in SciPy\n\nPython’ SciPy library has many functions for evaluating Bessel functions. All are contained in the scipy.special subpackage.\n\nThere are several varieties of Bessel functions, all closely related as summarized in the diagram below.\n\nEach letter is the conventional symbol for a family of Bessel functions. The lines between letters indicates a simple relationship between function families. For more on the definition of each function and how they are related, see Relations between Bessel functions.\n\nEach family of Bessel functions is indexed by a parameter called the order. By convention, the order is denoted n when integer-valued and ν (Greek nu) when more generally real-valued. These notes will focus on the general case of real orders.\n\nThe table below summarizes the correspondence between Bessel functions and their SciPy implementations.\n\nIn general SciPy names use a Roman letter v at the end of a function corresponding a mathematical function with a Greek ν subscript. Similarly, SciPy uses n for integer-order specializations that would be denoted with a subscript n. The Python functions jv, yv, and kv have integer specializations jn, yn, and kn.\n\nAll SciPy functions in the table take two arguments. The first is the order and the second is the parameter. For example, Jv(z) is evaluated using jn(v, z).\n\nThe Hankel functions H(1) and H(2) are implemented in the SciPy functions hankel1 and hankel2. SciPy does not contain functions for implementing the spherical Hankel functions h(1) and h(2), though these functions can be easily be computed in terms of the spherical Bessel functions. See relationship here.\n\nThe spherical Bessel functions jn and yn have integer order and are implemented in the SciPy functions sph_jn and sph_yn. These SciPy functions are unusual in two ways: they return values of the functions up to and including the specified order, and they return derivative values as well. For example, sph_jn(2, 3.4) returns a pair of arrays. The first array contains j0(3.4), j1(3.4), and j2(3.4) and the second array contains j'0(3.4), j'1(3.4), and j'2(3.4).\n\nSciPy includes optimized specializations for Bessel functions of orders 0 and 1. The functions j0 and j1 are optimized versions of jv with first argument 0 and 1 respectively. There are analogous optimized versions of yv, iv, and kv.\n\nSciPy contains many other functions related to Bessel functions: exponentially scaled Bessel functions, derivatives and integrals of Bessel functions, and zeros of Bessel functions. It also contains implementations of Ricatti-Bessel functions and Struve functions, functions closely related to the Bessel functions discussed here.\n\nRelated page: Gamma and related functions in SciPy."}
{"slug": "cauchy_estimation", "canonical_url": "https://www.johndcook.com/blog/cauchy_estimation/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/Cauchy_estimation.html", "title": "Cauchy distribution parameter estimation", "heading": "Cauchy distribution parameter estimation", "description": "Illustrating the sample mean and sample median for the Cauchy distribution", "summary": "Suppose a Martian gives you a black box. It has a button on top and a display on the side. Every time you press the button, the box displays a number. You want to figure out the pattern to the numbers, so you make a record of the outputs and keep a running average. You hit the button 100 times, ...", "word_count": 593, "blocks": [{"tag": "h1", "text": "Cauchy distribution parameter estimation"}, {"tag": "p", "text": "Suppose a Martian gives you a black box. It has a button on top and a display on the side. Every time you press the button, the box displays a number. You want to figure out the pattern to the numbers, so you make a record of the outputs and keep a running average. You hit the button 100 times, but the average keeps moving around. (There will be some theory further down the page, but first we start with some graphs to motivate the discussion.)"}, {"tag": "p", "text": "(The graph plots the cumulative sample average as a function of sample number.)"}, {"tag": "p", "text": "So you decide to keep going until you've hit the button 1000 times. Hmm, maybe the average is trending down lower than it looked like after 100 samples."}, {"tag": "p", "text": "To be sure, you hit the button another 1000 times."}, {"tag": "p", "text": "Maybe the average is settling around 3. To be sure, you go up to 10,000 samples."}, {"tag": "p", "text": "If we'd stopped after 4,000 samples we would have had a different idea of what's going on than it looks after 10,000 samples. How do we know when to stop?"}, {"tag": "p", "text": "What kind of devilish box has the Martian given you? The samples used to create the graphs above were generated from a Cauchy(3,1) distribution. The Cauchy distribution has the remarkable property that the average of N samples, for any positive integer N, has the same distribution as the original distribution. The average will not settle down no matter how many samples you take. That's why the plots above have a fractal-like quality. The graphs will look similar no matter how many samples we take."}, {"tag": "p", "text": "For a comparison, we will look briefly at the analogous graphs for a normal distribution. Here is a plot of the averages after each of the first 100 samples."}, {"tag": "p", "text": "And here's the corresponding graph after 1,000 samples."}, {"tag": "p", "text": "Unlike the Cauchy samples, the running average of the normal samples quickly converges to a nearly constant value."}, {"tag": "p", "text": "The reason the sample mean for the Cauchy distribution is so erratic is that the Cauchy distribution has no mean for the sample mean to converge to. For any distribution that has a mean and a variance, the variance of the sample mean is inversely proportional to the number of samples. That's why the running average for the normal settles down while the running average for the Cauchy keeps wandering."}, {"tag": "p", "text": "For normal distributions, the sample mean converges to the population mean. The first time you see that result you may think \"Well of course it does.\" The example above helps show that there is substance to the normal sampling theory. The sample mean exists for the Cauchy even though the Cauchy itself does not have a mean."}, {"tag": "p", "text": "We would do better if we kept track of the median of our samples rather than the mean. Here are some plots of the sample median for the same Cauchy samples. After 100 samples we have the following."}, {"tag": "p", "text": "Here's the corresponding plot after 1000 samples."}, {"tag": "p", "text": "The Cauchy distribution does have a median, and the sample median converges to that median. (Incidentally, the sample median would have worked well for normal samples, just not quite as well as the sample mean did.)"}, {"tag": "p", "text": "The distribution for the sample median of a Cauchy distribution is know. See Order Statistics, page 50. The book credits P. R. Rider, JASA 55: 322-3."}, {"tag": "p", "text": "The PDF of the median of n = 2k + 1 samples from a standard Cauchy distribution is given below."}, {"tag": "p", "text": "The variance of this distribution, given below, is finite for k ≥ 2."}], "content": "Cauchy distribution parameter estimation\n\nSuppose a Martian gives you a black box. It has a button on top and a display on the side. Every time you press the button, the box displays a number. You want to figure out the pattern to the numbers, so you make a record of the outputs and keep a running average. You hit the button 100 times, but the average keeps moving around. (There will be some theory further down the page, but first we start with some graphs to motivate the discussion.)\n\n(The graph plots the cumulative sample average as a function of sample number.)\n\nSo you decide to keep going until you've hit the button 1000 times. Hmm, maybe the average is trending down lower than it looked like after 100 samples.\n\nTo be sure, you hit the button another 1000 times.\n\nMaybe the average is settling around 3. To be sure, you go up to 10,000 samples.\n\nIf we'd stopped after 4,000 samples we would have had a different idea of what's going on than it looks after 10,000 samples. How do we know when to stop?\n\nWhat kind of devilish box has the Martian given you? The samples used to create the graphs above were generated from a Cauchy(3,1) distribution. The Cauchy distribution has the remarkable property that the average of N samples, for any positive integer N, has the same distribution as the original distribution. The average will not settle down no matter how many samples you take. That's why the plots above have a fractal-like quality. The graphs will look similar no matter how many samples we take.\n\nFor a comparison, we will look briefly at the analogous graphs for a normal distribution. Here is a plot of the averages after each of the first 100 samples.\n\nAnd here's the corresponding graph after 1,000 samples.\n\nUnlike the Cauchy samples, the running average of the normal samples quickly converges to a nearly constant value.\n\nThe reason the sample mean for the Cauchy distribution is so erratic is that the Cauchy distribution has no mean for the sample mean to converge to. For any distribution that has a mean and a variance, the variance of the sample mean is inversely proportional to the number of samples. That's why the running average for the normal settles down while the running average for the Cauchy keeps wandering.\n\nFor normal distributions, the sample mean converges to the population mean. The first time you see that result you may think \"Well of course it does.\" The example above helps show that there is substance to the normal sampling theory. The sample mean exists for the Cauchy even though the Cauchy itself does not have a mean.\n\nWe would do better if we kept track of the median of our samples rather than the mean. Here are some plots of the sample median for the same Cauchy samples. After 100 samples we have the following.\n\nHere's the corresponding plot after 1000 samples.\n\nThe Cauchy distribution does have a median, and the sample median converges to that median. (Incidentally, the sample median would have worked well for normal samples, just not quite as well as the sample mean did.)\n\nThe distribution for the sample median of a Cauchy distribution is know. See Order Statistics, page 50. The book credits P. R. Rider, JASA 55: 322-3.\n\nThe PDF of the median of n = 2k + 1 samples from a standard Cauchy distribution is given below.\n\nThe variance of this distribution, given below, is finite for k ≥ 2."}
{"slug": "IEEE_exceptions_in_cpp", "canonical_url": "https://www.johndcook.com/blog/IEEE_exceptions_in_cpp/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/IEEE_exceptions_in_cpp.html", "title": "IEEE floating point exceptions: NaNs, 1.#INF, 1.#IND, etc", "heading": "IEEE floating-point exceptions in C++", "description": "IEEE floating point exceptions in C++. All about 1.#INF, 1.#IND, 1.#QNAN, etc.", "summary": "This page will answer the following questions.", "word_count": 1635, "blocks": [{"tag": "h1", "text": "IEEE floating-point exceptions in C++"}, {"tag": "p", "text": "This page will answer the following questions."}, {"tag": "li", "text": "My program just printed out 1.#IND or 1.#INF (on Windows) or nan or inf (on Linux). What happened?"}, {"tag": "li", "text": "How can I tell if a number is really a number and not a NaN or an infinity?"}, {"tag": "li", "text": "How can I find out more details at runtime about kinds of NaNs and infinities?"}, {"tag": "li", "text": "Do you have any sample code to show how this works?"}, {"tag": "li", "text": "Where can I learn more?"}, {"tag": "p", "text": "These questions have to do with floating point exceptions. If you get some strange non-numeric output where you're expecting a number, you've either exceeded the finite limits of floating point arithmetic or you've asked for some result that is undefined. To keep things simple, I'll stick to working with the double floating point type. Similar remarks hold for float types."}, {"tag": "h2", "text": "Debugging 1.#IND, 1.#INF, nan, and inf"}, {"tag": "p", "text": "If your operation would generate a larger positive number than could be stored in a double, the operation will return 1.#INF on Windows or inf on Linux. Similarly your code will return -1.#INF or -inf if the result would be a negative number too large to store in a double. Dividing a positive number by zero produces a positive infinity and dividing a negative number by zero produces a negative infinity. Example code at the end of this page will demonstrate some operations that produce infinities."}, {"tag": "p", "text": "Some operations don't make mathematical sense, such as taking the square root of a negative number. (Yes, this operation makes sense in the context of complex numbers, but a double represents a real number and so there is no double to represent the result.) The same is true for logarithms of negative numbers. Both sqrt(-1.0) and log(-1.0) would return a NaN, the generic term for a \"number\" that is \"not a number\". Windows displays a NaN as -1.#IND (\"IND\" for \"indeterminate\") while Linux displays nan. Other operations that would return a NaN include 0/0, 0*∞, and ∞/∞. See the sample code below for examples."}, {"tag": "p", "text": "In short, if you get 1.#INF or inf, look for overflow or division by zero. If you get 1.#IND or nan, look for illegal operations. Maybe you simply have a bug. If it's more subtle and you have something that is difficult to compute numerically."}, {"tag": "h2", "text": "Testing for NaNs and infinities"}, {"tag": "p", "text": "Next suppose you want to test whether a number is an infinity or a NaN. For example, you may want to write to a log file print a debug message when a numerical result goes bad, or you may want to execute some sort of alternate logic in your code. There are simple, portable ways to get summary information and more complicated, less portable ways to get more information."}, {"tag": "p", "text": "First, the simple solution. If you want to test whether a double variable contains a valid number, you can check whether x == x. This looks like it should always be true, but it's not! Ordinary numbers always equal themselves, but NaNs do not. I've used this trick on Windows, Linux, and Mac OSX. If you ever use this trick, put big bold comments around your code so that some well-meaning person won't come behind you and delete what he or she things is useless code. Better yet, put the test in a well-documented function in a library that has controlled access. The following function will test whether x is a (possibly infinite) number."}, {"tag": "pre", "text": "bool IsNumber(double x) \n    {\n        // This looks like it should always be true, \n        // but it's false if x is a NaN.\n        return (x == x); \n    }"}, {"tag": "p", "text": "To test whether a variable contains a finite number, (i.e. not a NaN and not an infinity) you can use code like the following."}, {"tag": "pre", "text": "bool IsFiniteNumber(double x) \n    {\n        return (x <= DBL_MAX && x >= -DBL_MAX); \n    }"}, {"tag": "p", "text": "Here DBL_MAX is a constant defined in float.h as the largest double that can be represented. Comparisons with NaNs always fail, even when comparing to themselves, and so the test above will fail for a NaN. If x is not a NaN but is infinite, one of the two tests will fail depending on whether it is a positive infinity or negative infinity."}, {"tag": "h2", "text": "Getting more information programmatically"}, {"tag": "p", "text": "To get more detail about the type of a floating point number, there is a function _fpclass on Windows and a corresponding function fp_class_d on Linux. I have not been able to get the corresponding Linux code to work and so I'll stick to what I've tested and just talk about Windows from here on out."}, {"tag": "p", "text": "The Windows function _fpclass returns one of the following values:"}, {"tag": "pre", "text": "_FPCLASS_SNAN   // signaling NaN\n\t_FPCLASS_QNAN   // quiet NaN\n\t_FPCLASS_NINF   // negative infinity\n\t_FPCLASS_NN     // negative normal\n\t_FPCLASS_ND     // negative denormal\n\t_FPCLASS_NZ     // -0\n\t_FPCLASS_PZ     // +0\n\t_FPCLASS_PD     // positive denormal\n\t_FPCLASS_PN     // positive normal\n\t_FPCLASS_PINF   // positive infinity"}, {"tag": "p", "text": "The following code illustrates which kinds of operations result in which kinds of numbers. To port this code to Linux, the FPClass function would need to use fp_class_d and its corresponding constants."}, {"tag": "pre", "text": "#include <cfloat>\n\t#include <iostream>\n\t#include <sstream>\n\t#include <cmath>\n\n\tusing namespace std;\n\n\tstring FPClass(double x)\n\t{\n\t    int i = _fpclass(x);\n\t    string s;\n\t    switch (i)\n\t    {\n\t    case _FPCLASS_SNAN: s = \"Signaling NaN\";                break;\n\t    case _FPCLASS_QNAN: s = \"Quiet NaN\";                    break; \n\t    case _FPCLASS_NINF: s = \"Negative infinity (-INF)\";     break; \n\t    case _FPCLASS_NN:   s = \"Negative normalized non-zero\"; break;\n\t    case _FPCLASS_ND:   s = \"Negative denormalized\";        break; \n\t    case _FPCLASS_NZ:   s = \"Negative zero (-0)\";           break; \n\t    case _FPCLASS_PZ:   s = \"Positive 0 (+0)\";              break; \n\t    case _FPCLASS_PD:   s = \"Positive denormalized\";        break; \n\t    case _FPCLASS_PN:   s = \"Positive normalized non-zero\"; break; \n\t    case _FPCLASS_PINF: s = \"Positive infinity (+INF)\";     break;\n\t    }\n\t    return s;\n\t}\n\n\tstring HexDump(double x)\n\t{\n\t    unsigned long* pu;\n\t    pu = (unsigned long*)&x;\n\t    ostringstream os;\n\t    os << hex << pu[0] << \" \" << pu[1];\n\t    return os.str();\n\t}\n\n\t// ----------------------------------------------------------------------------\n\tint main()\n\t{\n\t    double x, y, z;\n\n\t    cout << \"Testing z = 1/0\\n\";\n\t    // cannot set x = 1/0 directly or would produce compile error.\n\t    x = 1.0; y = 0; z = x/y;\n\t    cout << \"z = \" << x/y << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = -1/0\\n\";\n\t    x = -1.0; y = 0; z = x/y;\n\t    cout << \"z = \" << x/y << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = sqrt(-1)\\n\";\n\t    x = -1.0;\n\t    z = sqrt(x);\n\t    cout << \"z = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = log(-1)\\n\";\n\t    x = -1.0;\n\t    z = sqrt(x);\n\t    cout << \"z = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting overflow\\n\";\n\t    z = DBL_MAX;\n\t    cout << \"z = DBL_MAX = \" << z; \n\t    z *= 2.0;\n\t    cout << \"; 2z = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting denormalized underflow\\n\";\n\t    z = DBL_MIN;\n\t    cout << \"z = DBL_MIN = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\t    z /= pow(2.0, 52);\n\t    cout << \"z = DBL_MIN / 2^52= \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\t    z /= 2;\n\t    cout << \"z = DBL_MIN / 2^53= \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = +infinity + -infinty\\n\";\n\t    x = 1.0; y = 0.0; x /= y; y = -x;\n\t    cout << x << \" + \" << y << \" = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = 0 * infinity\\n\";\n\t    x = 1.0; y = 0.0; x /= y; z = 0.0*x;\n\t    cout << \"x = \" << x << \"; z = 0*x = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting 0/0\\n\";\n\t    x = 0.0; y = 0.0; z = x/y;\n\t    cout << \"z = 0/0 = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = infinity/infinity\\n\";\n\t    x = 1.0; y = 0.0; x /= y; y = x; z = x/y;\n\t    cout << \"x = \" << x << \"; z = x/x = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting x fmod 0\\n\";\n\t    x = 1.0; y = 0.0; z = fmod(x, y);\n\t    cout << \"fmod(\" << x << \", \" << y << \") = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting infinity fmod x\\n\";\n\t    y = 1.0; x = 0.0; y /= x; z = fmod(y, x);\n\t    cout << \"fmod(\" << y << \", \" << x << \") = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nGetting cout to print QNAN\\n\";\n\t    unsigned long nan[2]={0xffffffff, 0x7fffffff};\n\t    z = *( double* )nan;\n\t    cout << \"z = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    return 0;\n\t}"}, {"tag": "h2", "text": "To learn more"}, {"tag": "p", "text": "For a brief explanation of numerical limits and how floating point numbers are laid out in memory, see Anatomy of a floating point number."}, {"tag": "p", "text": "For much more detail regarding exceptions and IEEE arithmetic in general, see What every computer scientist should know about floating-point arithmetic."}, {"tag": "h2", "text": "Other C++ articles"}, {"tag": "li", "text": "Random number generation"}, {"tag": "li", "text": "Strings"}, {"tag": "li", "text": "Regular expressions"}, {"tag": "li", "text": "Math.h"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}, {"tag": "p", "text": "jdc"}], "content": "IEEE floating-point exceptions in C++\n\nThis page will answer the following questions.\n\nMy program just printed out 1.#IND or 1.#INF (on Windows) or nan or inf (on Linux). What happened?\n\nHow can I tell if a number is really a number and not a NaN or an infinity?\n\nHow can I find out more details at runtime about kinds of NaNs and infinities?\n\nDo you have any sample code to show how this works?\n\nWhere can I learn more?\n\nThese questions have to do with floating point exceptions. If you get some strange non-numeric output where you're expecting a number, you've either exceeded the finite limits of floating point arithmetic or you've asked for some result that is undefined. To keep things simple, I'll stick to working with the double floating point type. Similar remarks hold for float types.\n\nDebugging 1.#IND, 1.#INF, nan, and inf\n\nIf your operation would generate a larger positive number than could be stored in a double, the operation will return 1.#INF on Windows or inf on Linux. Similarly your code will return -1.#INF or -inf if the result would be a negative number too large to store in a double. Dividing a positive number by zero produces a positive infinity and dividing a negative number by zero produces a negative infinity. Example code at the end of this page will demonstrate some operations that produce infinities.\n\nSome operations don't make mathematical sense, such as taking the square root of a negative number. (Yes, this operation makes sense in the context of complex numbers, but a double represents a real number and so there is no double to represent the result.) The same is true for logarithms of negative numbers. Both sqrt(-1.0) and log(-1.0) would return a NaN, the generic term for a \"number\" that is \"not a number\". Windows displays a NaN as -1.#IND (\"IND\" for \"indeterminate\") while Linux displays nan. Other operations that would return a NaN include 0/0, 0*∞, and ∞/∞. See the sample code below for examples.\n\nIn short, if you get 1.#INF or inf, look for overflow or division by zero. If you get 1.#IND or nan, look for illegal operations. Maybe you simply have a bug. If it's more subtle and you have something that is difficult to compute numerically.\n\nTesting for NaNs and infinities\n\nNext suppose you want to test whether a number is an infinity or a NaN. For example, you may want to write to a log file print a debug message when a numerical result goes bad, or you may want to execute some sort of alternate logic in your code. There are simple, portable ways to get summary information and more complicated, less portable ways to get more information.\n\nFirst, the simple solution. If you want to test whether a double variable contains a valid number, you can check whether x == x. This looks like it should always be true, but it's not! Ordinary numbers always equal themselves, but NaNs do not. I've used this trick on Windows, Linux, and Mac OSX. If you ever use this trick, put big bold comments around your code so that some well-meaning person won't come behind you and delete what he or she things is useless code. Better yet, put the test in a well-documented function in a library that has controlled access. The following function will test whether x is a (possibly infinite) number.\n\nbool IsNumber(double x) \n    {\n        // This looks like it should always be true, \n        // but it's false if x is a NaN.\n        return (x == x); \n    }\n\nTo test whether a variable contains a finite number, (i.e. not a NaN and not an infinity) you can use code like the following.\n\nbool IsFiniteNumber(double x) \n    {\n        return (x <= DBL_MAX && x >= -DBL_MAX); \n    }\n\nHere DBL_MAX is a constant defined in float.h as the largest double that can be represented. Comparisons with NaNs always fail, even when comparing to themselves, and so the test above will fail for a NaN. If x is not a NaN but is infinite, one of the two tests will fail depending on whether it is a positive infinity or negative infinity.\n\nGetting more information programmatically\n\nTo get more detail about the type of a floating point number, there is a function _fpclass on Windows and a corresponding function fp_class_d on Linux. I have not been able to get the corresponding Linux code to work and so I'll stick to what I've tested and just talk about Windows from here on out.\n\nThe Windows function _fpclass returns one of the following values:\n\n_FPCLASS_SNAN   // signaling NaN\n\t_FPCLASS_QNAN   // quiet NaN\n\t_FPCLASS_NINF   // negative infinity\n\t_FPCLASS_NN     // negative normal\n\t_FPCLASS_ND     // negative denormal\n\t_FPCLASS_NZ     // -0\n\t_FPCLASS_PZ     // +0\n\t_FPCLASS_PD     // positive denormal\n\t_FPCLASS_PN     // positive normal\n\t_FPCLASS_PINF   // positive infinity\n\nThe following code illustrates which kinds of operations result in which kinds of numbers. To port this code to Linux, the FPClass function would need to use fp_class_d and its corresponding constants.\n\n#include <cfloat>\n\t#include <iostream>\n\t#include <sstream>\n\t#include <cmath>\n\n\tusing namespace std;\n\n\tstring FPClass(double x)\n\t{\n\t    int i = _fpclass(x);\n\t    string s;\n\t    switch (i)\n\t    {\n\t    case _FPCLASS_SNAN: s = \"Signaling NaN\";                break;\n\t    case _FPCLASS_QNAN: s = \"Quiet NaN\";                    break; \n\t    case _FPCLASS_NINF: s = \"Negative infinity (-INF)\";     break; \n\t    case _FPCLASS_NN:   s = \"Negative normalized non-zero\"; break;\n\t    case _FPCLASS_ND:   s = \"Negative denormalized\";        break; \n\t    case _FPCLASS_NZ:   s = \"Negative zero (-0)\";           break; \n\t    case _FPCLASS_PZ:   s = \"Positive 0 (+0)\";              break; \n\t    case _FPCLASS_PD:   s = \"Positive denormalized\";        break; \n\t    case _FPCLASS_PN:   s = \"Positive normalized non-zero\"; break; \n\t    case _FPCLASS_PINF: s = \"Positive infinity (+INF)\";     break;\n\t    }\n\t    return s;\n\t}\n\n\tstring HexDump(double x)\n\t{\n\t    unsigned long* pu;\n\t    pu = (unsigned long*)&x;\n\t    ostringstream os;\n\t    os << hex << pu[0] << \" \" << pu[1];\n\t    return os.str();\n\t}\n\n\t// ----------------------------------------------------------------------------\n\tint main()\n\t{\n\t    double x, y, z;\n\n\t    cout << \"Testing z = 1/0\\n\";\n\t    // cannot set x = 1/0 directly or would produce compile error.\n\t    x = 1.0; y = 0; z = x/y;\n\t    cout << \"z = \" << x/y << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = -1/0\\n\";\n\t    x = -1.0; y = 0; z = x/y;\n\t    cout << \"z = \" << x/y << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = sqrt(-1)\\n\";\n\t    x = -1.0;\n\t    z = sqrt(x);\n\t    cout << \"z = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = log(-1)\\n\";\n\t    x = -1.0;\n\t    z = sqrt(x);\n\t    cout << \"z = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting overflow\\n\";\n\t    z = DBL_MAX;\n\t    cout << \"z = DBL_MAX = \" << z; \n\t    z *= 2.0;\n\t    cout << \"; 2z = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting denormalized underflow\\n\";\n\t    z = DBL_MIN;\n\t    cout << \"z = DBL_MIN = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\t    z /= pow(2.0, 52);\n\t    cout << \"z = DBL_MIN / 2^52= \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\t    z /= 2;\n\t    cout << \"z = DBL_MIN / 2^53= \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = +infinity + -infinty\\n\";\n\t    x = 1.0; y = 0.0; x /= y; y = -x;\n\t    cout << x << \" + \" << y << \" = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = 0 * infinity\\n\";\n\t    x = 1.0; y = 0.0; x /= y; z = 0.0*x;\n\t    cout << \"x = \" << x << \"; z = 0*x = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting 0/0\\n\";\n\t    x = 0.0; y = 0.0; z = x/y;\n\t    cout << \"z = 0/0 = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting z = infinity/infinity\\n\";\n\t    x = 1.0; y = 0.0; x /= y; y = x; z = x/y;\n\t    cout << \"x = \" << x << \"; z = x/x = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting x fmod 0\\n\";\n\t    x = 1.0; y = 0.0; z = fmod(x, y);\n\t    cout << \"fmod(\" << x << \", \" << y << \") = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nTesting infinity fmod x\\n\";\n\t    y = 1.0; x = 0.0; y /= x; z = fmod(y, x);\n\t    cout << \"fmod(\" << y << \", \" << x << \") = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    cout << \"\\nGetting cout to print QNAN\\n\";\n\t    unsigned long nan[2]={0xffffffff, 0x7fffffff};\n\t    z = *( double* )nan;\n\t    cout << \"z = \" << z << \"\\n\";\n\t    cout << HexDump(z) << \" _fpclass(z) = \" << FPClass(z) << \"\\n\";\n\n\t    return 0;\n\t}\n\nTo learn more\n\nFor a brief explanation of numerical limits and how floating point numbers are laid out in memory, see Anatomy of a floating point number.\n\nFor much more detail regarding exceptions and IEEE arithmetic in general, see What every computer scientist should know about floating-point arithmetic.\n\nOther C++ articles\n\nRandom number generation\n\nStrings\n\nRegular expressions\n\nMath.h\n\nHome\n\nBlog\n\nConsulting\n\nContact\n\njdc"}
{"slug": "powershellcookbook", "canonical_url": "https://www.johndcook.com/blog/powershellcookbook/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/PowerShellCookbook.html", "title": "PowerShell Cookbook", "heading": "PowerShell Cookbook", "description": "Examples of how to use Microsoft PowerShell to accomplish common tasks, especially things a beginner is likely to need to know.", "summary": "This is a list of items my colleagues and I have found useful in our work. It's not intended to be encyclopedic, but rather a place to record things a beginner would find useful, especially things that might take a long time to figure out.", "word_count": 1533, "blocks": [{"tag": "h1", "text": "PowerShell Cookbook"}, {"tag": "p", "text": "This is a list of items my colleagues and I have found useful in our work. It's not intended to be encyclopedic, but rather a place to record things a beginner would find useful, especially things that might take a long time to figure out."}, {"tag": "p", "text": "This page was started before O'Reilly published Windows PowerShell Cookbook by Lee Holmes. I recommend O'Reilly's book, but this page is unrelated to it."}, {"tag": "p", "text": "See also PowerShell gotchas and the e-booklet PowerShell Day 1."}, {"tag": "p", "text": "Translate this page:"}, {"tag": "p", "text": "Getting started/installation Install PowerShell Install Community Extensions library Get rid of the “execution of scripts is disabled” message Set up your “scripts:” PowerShell drive location Configure your command prompt Miscellaneous tasks Write to the screen Prompt user for input Manipulate dates Send email Sleep List a directory (recursively) Test whether a file exists Read file lines into an array Write to a file Rename a file Delete a file Work with regular expressions Split and join strings Grab a web page Read and write to registry Understand quotes Time a command Text to speech Access the Windows clipboard Load a .NET assembly Execute code created at run time Execute a program with a space in its path"}, {"tag": "h2", "text": "Getting started/installation"}, {"tag": "h3", "text": "Install PowerShell"}, {"tag": "p", "text": "You can obtain PowerShell from: https://www.microsoft.com/windowsserver2003/technologies/management/powershell/download.mspx"}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Install Community Extensions library"}, {"tag": "p", "text": "The PowerShell Community Extensions (PSCX) library is available here: https://www.codeplex.com/Wiki/View.aspx?ProjectName=PowerShellCX"}, {"tag": "p", "text": "This is a useful collection of cmdlets and functions. It installs a “PowerShell Here” context menu in WindowsExplorer that opens a PowerShell command window in the folder that you right-click on. Also, PSCX provides a sample Profile.ps1 file."}, {"tag": "p", "text": "Once PSCX is installed, type man about_pscx to see an overview of the cmdlets, providers, etc. that come with PSCX."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Get rid of the “execution of scripts is disabled” message"}, {"tag": "p", "text": "The first problem you are likely to encounter when running PowerShell scripts is something like this:"}, {"tag": "pre", "text": "PSH> .\\myScript.ps1\n\nFile D:\\myScript.ps1 cannot be loaded. The file\nD:\\myScript.ps1 is not digitally signed. The script will not execute on the\nsystem. Please see \"get-help about_signing\" for more details..\n\nAt line:1 char:13 + .\\myScript.ps1 <<<<"}, {"tag": "p", "text": "This is due the to the default security setting in PowerShell that prohibits malicious scripts from running."}, {"tag": "p", "text": "You can alter this behavior by doing this:"}, {"tag": "p", "text": "PSH> set-executionPolicy RemoteSigned"}, {"tag": "p", "text": "Now only scripts coming from remote sources will require digital signing."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Set up your “scripts:” PowerShell drive location"}, {"tag": "p", "text": "Once you've written a number of PowerShell scripts, you might find it useful to collect them in one place and create a PSDrive named scripts: to find them quickly. You could add the following to your PSCX profile to create such a PSDrive."}, {"tag": "p", "text": "New-PSdrive -name scripts -PSprovider filesystem -root C:\\bin\\PowerShellScripts"}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Configure your command prompt"}, {"tag": "p", "text": "To configure your command prompt, simply create a function named prompt and put it in your profile. For example, the following will cause the working directory to be displayed."}, {"tag": "p", "text": "function prompt { \"$pwd> \" }"}, {"tag": "p", "text": "Note that you could put any code you want inside the prompt function and that code will run every time you hit return."}, {"tag": "p", "text": "top"}, {"tag": "h2", "text": "Miscellaneous tasks"}, {"tag": "h3", "text": "Write to the screen"}, {"tag": "p", "text": "Use Write-Host to write output. You can optionally specify -foregroundcolor to color the output. For example, errors and warnings may be highlighted in red."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "Write-Host \"Hello world\" -foregroundcolor red"}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Prompt user for input"}, {"tag": "p", "text": "Use Read-Host."}, {"tag": "p", "text": "Example:"}, {"tag": "pre", "text": "$a = Read-Host \"Enter your name\"\nWrite-Host \"Hello\" $a"}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Manipulate dates"}, {"tag": "p", "text": "The cmdlet get-date returns a DateTime object. With no argument, it returns the current time and date. With a string argument, no need to quote, it parses the string into a date."}, {"tag": "p", "text": "Example: Find the number of days and hours since the US bicentennial."}, {"tag": "p", "text": "(get-date) - (get-date 7/4/1976)"}, {"tag": "p", "text": "You can also cast strings to DateTime object by using the cast [datetime]."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Send email"}, {"tag": "p", "text": "Use the Send-SmtpMail cmdlet from PowerShell Community Extensions."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "Send-SmtpMail -SmtpHost wotan.mdacc.tmc.edu -from cook@mdanderson.org -to cook@mdanderson.org -body \"hello world\""}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Sleep"}, {"tag": "p", "text": "Use the Start-Sleep cmdlet. Takes seconds by default, but has a -milliseconds option."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "Write-Host \"hello\"; Start-Sleep 5; Write-Host \"world\""}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "List a directory (recursively)"}, {"tag": "p", "text": "You can use the dir command, which is really an alias to Get-ChildItem. To list a directory recursively, just add the -recurse option."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "dir f:\\bin -recurse"}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Test whether a file exists"}, {"tag": "p", "text": "Use test-path."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Read file lines into an array"}, {"tag": "p", "text": "Use get-content."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "$a = (get-content foo.txt)"}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Write to a file"}, {"tag": "p", "text": "Use Out-File. Unless you want Unicode output, specify -encoding ASCII"}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "$foo | out-file -encoding ASCII foo.txt"}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Rename a file"}, {"tag": "p", "text": "Use Rename-Item or its alias ren."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Delete a file"}, {"tag": "p", "text": "Use Delete-Item or its alias del."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Work with regular expressions"}, {"tag": "p", "text": "See Regular expressions in PowerShell and Perl."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Split and join strings"}, {"tag": "p", "text": "Strings have a split method. The argument to split() is a string containing characters to split on. The function [regex]::split is more general. Its first argument is the string to split, and its second argument is a regular expression on which to split."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "$a.split(\"`t \")"}, {"tag": "p", "text": "will break the string $a into an array of strings, splitting wherever there is a tab or a space."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "$a = \"123,456,apple\"; [regex]::split($a, \",(?!\\d)\")"}, {"tag": "p", "text": "will split $a on commas not followed by digits. So the split would return \"123,456\" and \"apple\"."}, {"tag": "p", "text": "If the string you're splitting is a Windows path, you may want to use the specialized Split-Path command."}, {"tag": "p", "text": "To join an array into a single string, use the [string]::join method. The first argument is the separation character and the second is the array."}, {"tag": "p", "text": "Example:"}, {"tag": "pre", "text": "$a = 1, 2, 3;\n#note: this is an array, not a string\n$b = [string]::join(\"*\", $a)"}, {"tag": "p", "text": "Now $b contains \"1*2*3\"."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Grab a web page"}, {"tag": "p", "text": "Use the either Get-Url from PSCX or the net.webclient object from .NET. The former is more succinct but the latter allows more options."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "$w = Get-Url \"https://www.w3.org\""}, {"tag": "p", "text": "$w = (new-object net.webclient).DownloadString(\"https://www.w3.org\")"}, {"tag": "p", "text": "The code (new-object net.webclient) creates a .NET WebClient object with many useful methods. For example, you could call DownladFile to save directly to disk rather than to a string. There are also UploadData and UploadFile methods."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Read and write to registry"}, {"tag": "p", "text": "You can navigate the registry as you would the file system, though there are some details to know."}, {"tag": "p", "text": "Each hive of the registry is like a drive. So, for example, to explore HKEY_CURRENT_USER, you can cd HKCU: and then use cd to work your way down the tree. Use Get-ItemProperty to read and Set-ItemProperty to write. You can also use Remove-ItemProperty to delete."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "The commands below will list the contents of the registry node that configures the cmd.exe shell. By setting CompletionChar to 9, we turn on tab completion."}, {"tag": "pre", "text": "cd HKCU:\\Software\\Microsoft\\Command Processor\nGet-ItemProperty .\nSet-ItemProperty . -name CompletionChar -Value 9"}, {"tag": "p", "text": "This TechNet article goes into more details."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Understand quotes"}, {"tag": "p", "text": "PowerShell has four kinds of quotes: single and double ordinary quotes, and single and double here-strings. Inside double quotes and double here-strings, variable names are replaced with their values and PowerShell escape sequences are interpreted. In single quotes and single here-strings, variable names are not expanded and escape sequences are not interpreted. For example, the code snippet"}, {"tag": "pre", "text": "$a = \"bar\"\n    'foo`t$a', \"foo`t$a\""}, {"tag": "p", "text": "will produce the output"}, {"tag": "pre", "text": "foo`t$a\n    foo    bar"}, {"tag": "p", "text": "because inside the double quote, `t expands to a tab and $a expands to \"bar\"."}, {"tag": "p", "text": "PowerShell here-strings are like here-documents in Perl. Inside a here-string, double and single quotes are not special but quoted literally. Also, line breaks are preserved."}, {"tag": "p", "text": "A here-string begins with either @\" or @' and ends with the same symbols in the opposite order. There must be a newline following the opening marker and before the closing marker."}, {"tag": "p", "text": "Example"}, {"tag": "pre", "text": "$a = \"bar\"\n    $b = @\"\n    foo \"baz\"\n    'qux' $a\n    \"@\n    $b"}, {"tag": "p", "text": "produces"}, {"tag": "pre", "text": "foo \"baz\"\n    'qux' bar"}, {"tag": "p", "text": "and"}, {"tag": "pre", "text": "$a = \"bar\"\n    $b = @'\n    foo \"baz\"\n    'qux' $a\n    '@\n    $b"}, {"tag": "p", "text": "produces"}, {"tag": "pre", "text": "foo \"baz\"\n    'qux' $a"}, {"tag": "p", "text": "Note that there must be no white space following the quote opening a here-string. Otherwise you will get the message “Unrecognized token in source text.”"}, {"tag": "p", "text": "Although they look similar, PowerShell here-strings and C# verbatim strings are fairly different. The following table summarizes the differences."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Time a command"}, {"tag": "p", "text": "Use Measure-Command to time how long a task takes to complete."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Text to speech"}, {"tag": "p", "text": "Use Out-Speech from PSCX."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Access the Windows clipboard"}, {"tag": "p", "text": "Use the PSCX cmdlets Get-Clipboard and Out-Clipboard to read from and write to the clipboard."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Load a .NET assembly"}, {"tag": "p", "text": "You can call LoadWithPartialName. For example, to load the assembly for the Team Foundation Server API, you would execute"}, {"tag": "p", "text": "[void][System.Reflection.Assembly]::LoadWithPartialName(\"Microsoft.TeamFoundation.Client\")"}, {"tag": "p", "text": "See also Resolve-Assembly from PSCX."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Execute code created at run time"}, {"tag": "p", "text": "The command Invoke-Expression is analogous to eval in Perl or JavaScript. Invoke-Expression $str executes the code contained in the string $str."}, {"tag": "p", "text": "top"}, {"tag": "h3", "text": "Execute a program with a space in its path"}, {"tag": "p", "text": "Put quotation marks around the path and stick an ampersand in front. For example,"}, {"tag": "p", "text": "& \"C:\\Program Files\\WinEdt Team\\WinEdt\\WinEdt.exe\""}, {"tag": "p", "text": "will launch the executable WinEdt.exe which has two spaces in its full path."}, {"tag": "p", "text": "Note that you cannot leave out the ampersand. Unlike cmd.exe, PowerShell does not assume all strings are commands. Without the ampersand, a quoted path is just a string."}, {"tag": "p", "text": "top"}, {"tag": "p", "text": "More PowerShell resources"}, {"tag": "p", "text": "PowerShell introduction PowerShell Day 1 booklet PowerShell gotchas Regular expressions in PowerShell and Perl"}], "content": "PowerShell Cookbook\n\nThis is a list of items my colleagues and I have found useful in our work. It's not intended to be encyclopedic, but rather a place to record things a beginner would find useful, especially things that might take a long time to figure out.\n\nThis page was started before O'Reilly published Windows PowerShell Cookbook by Lee Holmes. I recommend O'Reilly's book, but this page is unrelated to it.\n\nSee also PowerShell gotchas and the e-booklet PowerShell Day 1.\n\nTranslate this page:\n\nGetting started/installation Install PowerShell Install Community Extensions library Get rid of the “execution of scripts is disabled” message Set up your “scripts:” PowerShell drive location Configure your command prompt Miscellaneous tasks Write to the screen Prompt user for input Manipulate dates Send email Sleep List a directory (recursively) Test whether a file exists Read file lines into an array Write to a file Rename a file Delete a file Work with regular expressions Split and join strings Grab a web page Read and write to registry Understand quotes Time a command Text to speech Access the Windows clipboard Load a .NET assembly Execute code created at run time Execute a program with a space in its path\n\nGetting started/installation\n\nInstall PowerShell\n\nYou can obtain PowerShell from: https://www.microsoft.com/windowsserver2003/technologies/management/powershell/download.mspx\n\ntop\n\nInstall Community Extensions library\n\nThe PowerShell Community Extensions (PSCX) library is available here: https://www.codeplex.com/Wiki/View.aspx?ProjectName=PowerShellCX\n\nThis is a useful collection of cmdlets and functions. It installs a “PowerShell Here” context menu in WindowsExplorer that opens a PowerShell command window in the folder that you right-click on. Also, PSCX provides a sample Profile.ps1 file.\n\nOnce PSCX is installed, type man about_pscx to see an overview of the cmdlets, providers, etc. that come with PSCX.\n\ntop\n\nGet rid of the “execution of scripts is disabled” message\n\nThe first problem you are likely to encounter when running PowerShell scripts is something like this:\n\nPSH> .\\myScript.ps1\n\nFile D:\\myScript.ps1 cannot be loaded. The file\nD:\\myScript.ps1 is not digitally signed. The script will not execute on the\nsystem. Please see \"get-help about_signing\" for more details..\n\nAt line:1 char:13 + .\\myScript.ps1 <<<<\n\nThis is due the to the default security setting in PowerShell that prohibits malicious scripts from running.\n\nYou can alter this behavior by doing this:\n\nPSH> set-executionPolicy RemoteSigned\n\nNow only scripts coming from remote sources will require digital signing.\n\ntop\n\nSet up your “scripts:” PowerShell drive location\n\nOnce you've written a number of PowerShell scripts, you might find it useful to collect them in one place and create a PSDrive named scripts: to find them quickly. You could add the following to your PSCX profile to create such a PSDrive.\n\nNew-PSdrive -name scripts -PSprovider filesystem -root C:\\bin\\PowerShellScripts\n\ntop\n\nConfigure your command prompt\n\nTo configure your command prompt, simply create a function named prompt and put it in your profile. For example, the following will cause the working directory to be displayed.\n\nfunction prompt { \"$pwd> \" }\n\nNote that you could put any code you want inside the prompt function and that code will run every time you hit return.\n\ntop\n\nMiscellaneous tasks\n\nWrite to the screen\n\nUse Write-Host to write output. You can optionally specify -foregroundcolor to color the output. For example, errors and warnings may be highlighted in red.\n\nExample:\n\nWrite-Host \"Hello world\" -foregroundcolor red\n\ntop\n\nPrompt user for input\n\nUse Read-Host.\n\nExample:\n\n$a = Read-Host \"Enter your name\"\nWrite-Host \"Hello\" $a\n\ntop\n\nManipulate dates\n\nThe cmdlet get-date returns a DateTime object. With no argument, it returns the current time and date. With a string argument, no need to quote, it parses the string into a date.\n\nExample: Find the number of days and hours since the US bicentennial.\n\n(get-date) - (get-date 7/4/1976)\n\nYou can also cast strings to DateTime object by using the cast [datetime].\n\ntop\n\nSend email\n\nUse the Send-SmtpMail cmdlet from PowerShell Community Extensions.\n\nExample:\n\nSend-SmtpMail -SmtpHost wotan.mdacc.tmc.edu -from cook@mdanderson.org -to cook@mdanderson.org -body \"hello world\"\n\ntop\n\nSleep\n\nUse the Start-Sleep cmdlet. Takes seconds by default, but has a -milliseconds option.\n\nExample:\n\nWrite-Host \"hello\"; Start-Sleep 5; Write-Host \"world\"\n\ntop\n\nList a directory (recursively)\n\nYou can use the dir command, which is really an alias to Get-ChildItem. To list a directory recursively, just add the -recurse option.\n\nExample:\n\ndir f:\\bin -recurse\n\ntop\n\nTest whether a file exists\n\nUse test-path.\n\ntop\n\nRead file lines into an array\n\nUse get-content.\n\nExample:\n\n$a = (get-content foo.txt)\n\ntop\n\nWrite to a file\n\nUse Out-File. Unless you want Unicode output, specify -encoding ASCII\n\nExample:\n\n$foo | out-file -encoding ASCII foo.txt\n\ntop\n\nRename a file\n\nUse Rename-Item or its alias ren.\n\ntop\n\nDelete a file\n\nUse Delete-Item or its alias del.\n\ntop\n\nWork with regular expressions\n\nSee Regular expressions in PowerShell and Perl.\n\ntop\n\nSplit and join strings\n\nStrings have a split method. The argument to split() is a string containing characters to split on. The function [regex]::split is more general. Its first argument is the string to split, and its second argument is a regular expression on which to split.\n\nExample:\n\n$a.split(\"`t \")\n\nwill break the string $a into an array of strings, splitting wherever there is a tab or a space.\n\nExample:\n\n$a = \"123,456,apple\"; [regex]::split($a, \",(?!\\d)\")\n\nwill split $a on commas not followed by digits. So the split would return \"123,456\" and \"apple\".\n\nIf the string you're splitting is a Windows path, you may want to use the specialized Split-Path command.\n\nTo join an array into a single string, use the [string]::join method. The first argument is the separation character and the second is the array.\n\nExample:\n\n$a = 1, 2, 3;\n#note: this is an array, not a string\n$b = [string]::join(\"*\", $a)\n\nNow $b contains \"1*2*3\".\n\ntop\n\nGrab a web page\n\nUse the either Get-Url from PSCX or the net.webclient object from .NET. The former is more succinct but the latter allows more options.\n\nExample:\n\n$w = Get-Url \"https://www.w3.org\"\n\n$w = (new-object net.webclient).DownloadString(\"https://www.w3.org\")\n\nThe code (new-object net.webclient) creates a .NET WebClient object with many useful methods. For example, you could call DownladFile to save directly to disk rather than to a string. There are also UploadData and UploadFile methods.\n\ntop\n\nRead and write to registry\n\nYou can navigate the registry as you would the file system, though there are some details to know.\n\nEach hive of the registry is like a drive. So, for example, to explore HKEY_CURRENT_USER, you can cd HKCU: and then use cd to work your way down the tree. Use Get-ItemProperty to read and Set-ItemProperty to write. You can also use Remove-ItemProperty to delete.\n\nExample:\n\nThe commands below will list the contents of the registry node that configures the cmd.exe shell. By setting CompletionChar to 9, we turn on tab completion.\n\ncd HKCU:\\Software\\Microsoft\\Command Processor\nGet-ItemProperty .\nSet-ItemProperty . -name CompletionChar -Value 9\n\nThis TechNet article goes into more details.\n\ntop\n\nUnderstand quotes\n\nPowerShell has four kinds of quotes: single and double ordinary quotes, and single and double here-strings. Inside double quotes and double here-strings, variable names are replaced with their values and PowerShell escape sequences are interpreted. In single quotes and single here-strings, variable names are not expanded and escape sequences are not interpreted. For example, the code snippet\n\n$a = \"bar\"\n    'foo`t$a', \"foo`t$a\"\n\nwill produce the output\n\nfoo`t$a\n    foo    bar\n\nbecause inside the double quote, `t expands to a tab and $a expands to \"bar\".\n\nPowerShell here-strings are like here-documents in Perl. Inside a here-string, double and single quotes are not special but quoted literally. Also, line breaks are preserved.\n\nA here-string begins with either @\" or @' and ends with the same symbols in the opposite order. There must be a newline following the opening marker and before the closing marker.\n\nExample\n\n$a = \"bar\"\n    $b = @\"\n    foo \"baz\"\n    'qux' $a\n    \"@\n    $b\n\nproduces\n\nfoo \"baz\"\n    'qux' bar\n\nand\n\n$a = \"bar\"\n    $b = @'\n    foo \"baz\"\n    'qux' $a\n    '@\n    $b\n\nproduces\n\nfoo \"baz\"\n    'qux' $a\n\nNote that there must be no white space following the quote opening a here-string. Otherwise you will get the message “Unrecognized token in source text.”\n\nAlthough they look similar, PowerShell here-strings and C# verbatim strings are fairly different. The following table summarizes the differences.\n\ntop\n\nTime a command\n\nUse Measure-Command to time how long a task takes to complete.\n\ntop\n\nText to speech\n\nUse Out-Speech from PSCX.\n\ntop\n\nAccess the Windows clipboard\n\nUse the PSCX cmdlets Get-Clipboard and Out-Clipboard to read from and write to the clipboard.\n\ntop\n\nLoad a .NET assembly\n\nYou can call LoadWithPartialName. For example, to load the assembly for the Team Foundation Server API, you would execute\n\n[void][System.Reflection.Assembly]::LoadWithPartialName(\"Microsoft.TeamFoundation.Client\")\n\nSee also Resolve-Assembly from PSCX.\n\ntop\n\nExecute code created at run time\n\nThe command Invoke-Expression is analogous to eval in Perl or JavaScript. Invoke-Expression $str executes the code contained in the string $str.\n\ntop\n\nExecute a program with a space in its path\n\nPut quotation marks around the path and stick an ampersand in front. For example,\n\n& \"C:\\Program Files\\WinEdt Team\\WinEdt\\WinEdt.exe\"\n\nwill launch the executable WinEdt.exe which has two spaces in its full path.\n\nNote that you cannot leave out the ampersand. Unlike cmd.exe, PowerShell does not assume all strings are commands. Without the ampersand, a quoted path is just a string.\n\ntop\n\nMore PowerShell resources\n\nPowerShell introduction PowerShell Day 1 booklet PowerShell gotchas Regular expressions in PowerShell and Perl"}
{"slug": "r_language_for_programmers", "canonical_url": "https://www.johndcook.com/blog/r_language_for_programmers/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/R_language_for_programmers.html", "title": "The R programming language for programmers coming from other programming languages", "heading": "R programming for those coming from other languages", "description": "Things a programmer coming from another language needs to know about R, especially those things that might be most surprising", "summary": "Introduction Assignment and underscore Variable name gotchas Vectors Sequences Types Boolean operators Lists Matrices Missing values and NaNs Comments Functions Scope Misc. Other resources", "word_count": 2473, "blocks": [{"tag": "h1", "text": "R programming for those coming from other languages"}, {"tag": "p", "text": "Introduction Assignment and underscore Variable name gotchas Vectors Sequences Types Boolean operators Lists Matrices Missing values and NaNs Comments Functions Scope Misc. Other resources"}, {"tag": "h2", "text": "Introduction"}, {"tag": "p", "text": "I have written software professionally in perhaps a dozen programming languages, and the hardest language for me to learn has been R. The language is actually fairly simple, but it is unconventional. These notes are intended to make the language easier to learn for someone used to more commonly used languages such as C++, Java, Perl, etc."}, {"tag": "p", "text": "R is more than a programming language. It is an interactive environment for doing statistics. I find it more helpful to think of R as having a programming language than being a programming language. The R language is the scripting language for the R environment, just as VBA is the scripting language for Microsoft Excel. Some of the more unusual features of the R language begin to make sense when viewed from this perspective."}, {"tag": "p", "text": "This document is a work in progress. Corrections and comments are welcome."}, {"tag": "h2", "text": "Assignment and underscore"}, {"tag": "p", "text": "The assignment operator in R is <- as in"}, {"tag": "p", "text": "e <- m*c^2."}, {"tag": "p", "text": "It is also possible, though uncommon, to reverse the arrow and put the receiving variable on the right, as in"}, {"tag": "p", "text": "m*c^2 -> e."}, {"tag": "p", "text": "It is sometimes possible to use = for assignment, though I don't understand when this is and is not allowed. Most people avoid the issue by always using the arrow."}, {"tag": "p", "text": "However, when supplying default function arguments or calling functions with named arguments, you must use the = operator and cannot use the arrow."}, {"tag": "p", "text": "At some time in the past R, or its ancestor S, used underscore as assignment. This meant that the C convention of using underscores as separators in multi-word variable names was not only disallowed but produced strange side effects. For example, first_name would not be a single variable but rather the instruction to assign the value of name to the variable first! S-PLUS still follows this use of the underscore. However, R allows underscore as a variable character and not as an assignment operator."}, {"tag": "h2", "text": "Variable name gotchas"}, {"tag": "p", "text": "Because the underscore was not allowed as a variable character, the convention arose to use dot as a name separator. Unlike its use in many object oriented languages, the dot character in R has no special significance, with two exceptions. First, the ls() function in R lists active variables much as the ls Unix shell command lists directory contents. As the ls shell command does not list files that begin with a dot, neither does the ls() function in R function show variables that begin with dot by default. Second, ... is used to indicate a variable number of function arguments."}, {"tag": "p", "text": "R uses $ in a manner analogous to the way other languages use dot."}, {"tag": "p", "text": "R has several one-letter reserved words: c, q, s, t, C, D, F, I, and T."}, {"tag": "p", "text": "(Actually, these are not reserved, but it's best to think of them as reserved. For example, c is a built-in function for creating vectors, though you could also create a variable named c. Worse, T and F are not synonyms for TRUE and FALSE but variables that have the expected values by default. So someone could include the code T <- FALSE; F <- TRUE and reverse their meanings!)"}, {"tag": "h2", "text": "Vectors"}, {"tag": "p", "text": "The primary data type in R is the vector. Before describing how vectors work in R, it is helpful to distinguish two ideas of vectors in order to set the correct expectations"}, {"tag": "p", "text": "The first idea of a vector is what I will call a container vector. This is an ordered collection of numbers with no other structure, such as the vector<> container in C++. The length of a vector is the number of elements in the container. Operations are applied componentwise. For example, given two vectors x and y of equal length, x*y would be the vector whose nth component is the product of the nth components of x and y. Also, log(x) would be the vector whose nth component is the logarithm of the nth component of x."}, {"tag": "p", "text": "The other idea of a vector is a mathematical vector, an element of a vector space. In this context \"length\" means geometrical length determined by an inner product; the number of components is called \"dimension.\" In general, operations are not applied componentwise. The expression x*y is a single number, the inner product of the vectors. The expression log(x) is meaningless."}, {"tag": "p", "text": "A vector in R is a container vector, a statistician's collection of data, not a mathematical vector. The R language is designed around the assumption that a vector is an ordered set of measurements rather than a geometrical position or a physical state. (R supports mathematical vector operations, but they are secondary in the design of the language.) This helps explain, for example, R's otherwise inexplicable vector recycling feature."}, {"tag": "p", "text": "Adding a vector of length 22 and a vector of length 45 in most languages would raise an exception; the language designers would assume the programmer has made an error and the program is now in an undefined state. However, R allows adding two vectors regardless of their relative lengths. The elements of the shorter summand are recycled as often as necessary to create a vector the length of the longer summand. This is not attempting to add physical vectors that are incompatible for addition, but rather a syntactic convenience for manipulating sets of data. (R does issue a warning when adding vectors of different lengths and the length of the longer vector is not an integer multiple of the length of the shorter vector. So, for example, adding vectors of lengths 3 and 7 would cause a warning, but adding vectors of length 3 and 6 would not.)"}, {"tag": "p", "text": "The R language has no provision for scalars, nothing like a double in C-family languages. The only way to represent a single number in a variable is to use a vector of length one. And while it is possible to iterate through vectors as one might do in a for loop in C, it is usually clearer and more efficient in R to operate on vectors as a whole."}, {"tag": "p", "text": "Vectors are created using the c function. For example, p <- c(2,3,5,7) sets p to the vector containing the first four prime numbers."}, {"tag": "p", "text": "Vectors in R are indexed starting with 1 and matrices in are stored in column-major order. In both of these ways R resembles FORTRAN."}, {"tag": "p", "text": "Elements of a vector can be accessed using []. So in the above example, p[3] is 5."}, {"tag": "p", "text": "Vectors automatically expand when assigning to an index past the end of the vector, as in Perl."}, {"tag": "p", "text": "Negative indices are legal, but they have a very different meaning than in some other languages. If x is an array in Python or Perl, x[-n] returns the nth element from the end of the vector. In R, x[-n] returns a copy of x with the nth element removed."}, {"tag": "p", "text": "Boolean values can also be used as indices, and they behave differently than integers. See Five kinds of subscripts in R."}, {"tag": "h2", "text": "Sequences"}, {"tag": "p", "text": "The expression seq(a, b, n) creates a closed interval from a to b in steps of size n. For example, seq(1, 10, 3) returns the vector containing 1, 4, 7, and 10. This is similar to range(a, b, n) in Python, except Python uses open intervals and so the 10 would not be included in this example. The step size argument n defaults to 1 in both R and Python."}, {"tag": "p", "text": "The notation a:b is an abbreviation for seq(a, b, 1)."}, {"tag": "p", "text": "The notation seq(a, b, length=n) is a variation that will set the step size to (b-a)/(n-1) so that the sequence has n points."}, {"tag": "h2", "text": "Types"}, {"tag": "p", "text": "The type of a vector is the type of the elements it contains and must be one of the following: logical, integer, double, complex, character, or raw. All elements of a vector must have the same underlying type. This restriction does not apply to lists."}, {"tag": "p", "text": "Type conversion functions have the naming convention as.xxx for the function converts its argument to type xxx. For example, as.integer(3.2) returns the integer 3, and as.character(3.2) returns the string \"3.2\"."}, {"tag": "h2", "text": "Boolean operators"}, {"tag": "p", "text": "You can input T or TRUE for true values and F or FALSE for false values."}, {"tag": "p", "text": "The operators & and | apply element-wise on vectors. The operators && and || are often used in conditional statements and use lazy evaluation as in C: the operators will not evaluate their second argument if the return value is determined by the first argument."}, {"tag": "h2", "text": "Lists"}, {"tag": "p", "text": "Lists are like vectors, except elements need not all have the same type. For example, the first element of a list could be an integer and the second element be a string or a vector of Boolean values."}, {"tag": "p", "text": "Lists are created using the list function. Elements can be access by position using [[]]. Named elements may be accessed either by position or by name."}, {"tag": "p", "text": "Named elements of lists act like C structs, except a dollar sign rather than a dot is used to access elements. For example, consider,"}, {"tag": "pre", "text": "a <- list(name=\"Joe\", 4, foo=c(3,8,9))"}, {"tag": "p", "text": "Now a[[1]] and a$name both equal the string \"Joe\"."}, {"tag": "p", "text": "If you attempt to access a non-existent element of a list, say a[[4]] above, you will get an error. However, you can assign to a non-existent element of a list, thus extending the list. If the index you assign to is more than one past the end of the list, intermediate elements are created and assigned NULL values. You can also assign to non-existent named fields, such as saying a$baz = TRUE."}, {"tag": "h2", "text": "Matrices"}, {"tag": "p", "text": "In a sense, R does not support matrices, only vectors. But you can change the dimension of a vector, essentially making it a matrix."}, {"tag": "p", "text": "For example, m <- array( c(1,2,3,4,5,6), dim=c(2,3) ) creates a matrix m. However, it may come as a surprise that the first row of m has elements 1, 3, and 5. This is because by default, R fills matrices by column, like FORTRAN. To fill m by row, add the argument by.row = TRUE to the call to the array function."}, {"tag": "h2", "text": "Missing values and NaNs"}, {"tag": "p", "text": "As in other programming languages, the result of an operation on numbers may return NaN, the symbol for \"not a number.\" For example, an operation might overflow the finite range of a machine number, or a program might request an undefined operation, such as dividing by zero."}, {"tag": "p", "text": "R also has a different type of non-number, NA for \"not applicable.\" NA is used to indicate missing data, and is unfortunately fairly common in data sets. NA in R is similar to NULL in SQL or nullable types in C#. However, one must be more careful about NA values in R than about nulls in SQL or C#. The designer of database or the author of a piece of C# code specifies which values are nullable and can avoid the issue by simply not allowing such values. The author of an R function, however, has no control over the data his function will receive because NA is a legal value inside an R vector. There is no way to specify that a function takes only vectors with non-null components. You must handle NA values, even if you handle them by returning an error."}, {"tag": "p", "text": "The function is.nan will return TRUE for those components of its argument that are NaN. The function is.na will return true for those components that are NA or NaN."}, {"tag": "h2", "text": "Comments"}, {"tag": "p", "text": "Comments begin with # and continue to the end of the line, as in Python or Perl."}, {"tag": "h2", "text": "Functions"}, {"tag": "p", "text": "The function definition syntax of R is similar to that of JavaScript. For example:"}, {"tag": "pre", "text": "f <- function(a, b)\n{\n    return (a+b)\n}"}, {"tag": "p", "text": "The function function returns a function, which is usually assigned to a variable, f in this case, but need not be. You may use the function statement to create an anonymous function (lambda expression)."}, {"tag": "p", "text": "Note that return is a function; its argument must be contained in parentheses, unlike C where parentheses are optional. The use of return is optional; otherwise the value of the last line executed in a function is its return value."}, {"tag": "p", "text": "Default values are defined similarly to C++. In the following example, b is set to 10 by default."}, {"tag": "pre", "text": "f <- function(a, b=10)\n{\n    return (a+b)\n}"}, {"tag": "p", "text": "So f(5, 1) would return 6, and f(5) would return 15. R allows more sophisticated default values than does C++. A default value in R need not be a static type but could, for example, be a function of other arguments."}, {"tag": "p", "text": "C++ requires that if an argument has a default value then so do all values to the right. This is not the case in R, though it is still a good idea. The function definition"}, {"tag": "pre", "text": "f <- function(a=10, b)\n{\n    return (a+b)\n}"}, {"tag": "p", "text": "is legal, but calling f(5) would cause an error. The argument a would be assigned 5, but no value would be assigned to b. The reason such a function definition is not illegal is that one could still call the function with one named argument. For example, f(b=2) would return 12."}, {"tag": "p", "text": "Function arguments are passed by value. The most common mechanism for passing variables by reference is to use non-local variables. (Not necessarily global variables, but variables in the calling routine's scope.) A safer alternative is to explicitly pass in all needed values and return a list as output."}, {"tag": "h2", "text": "Scope"}, {"tag": "p", "text": "R uses lexical scoping while S-PLUS uses static scope. The difference can be subtle, particularly when using closures."}, {"tag": "p", "text": "Since variables cannot be declared — they pop into existence on first assignment — it is not always easy to determine the scope of a variable. You cannot tell just by looking at the source code of a function whether a variable is local to that function."}, {"tag": "h2", "text": "Misc."}, {"tag": "p", "text": "Here are a few miscellaneous facts about R that may be useful."}, {"tag": "li", "text": "help(fctn) displays help on any function fctn, as in Python."}, {"tag": "li", "text": "To invoke complex arithmetic, add 0i to a number. For example, sqrt(-1) returns NaN, but sqrt(-1 + 0i) returns 0 + 1i."}, {"tag": "li", "text": "sessionInfo() prints the R version, OS, packages loaded, etc."}, {"tag": "li", "text": "ls() shows which objects are defined."}, {"tag": "li", "text": "rm(list=ls()) clears all defined objects."}, {"tag": "li", "text": "dev.new() opens a new plotting window without overwriting the previous one."}, {"tag": "li", "text": "The function sort() does not change its argument."}, {"tag": "li", "text": "Distribution function prefixes d, p, q, r stand for density (PDF), probability (CDF), quantile (CDF-1), and random sample. For example, dnorm is the density function of a normal random variable and rnorm generates a sample from a normal random variable. The corresponding functions for a uniform random variable are dunif and runif."}, {"tag": "h2", "text": "Other resources"}, {"tag": "p", "text": "The R Project"}, {"tag": "p", "text": "The R Book"}, {"tag": "p", "text": "Radford Neal's series on design flaws in R. Part I, II, III."}, {"tag": "p", "text": "The R Inferno"}, {"tag": "p", "text": "StackOverflow questions tagged 'R'"}, {"tag": "p", "text": "My blog posts about R"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Twitter"}, {"tag": "li", "text": "Consulting"}, {"tag": "p", "text": "jdc"}], "content": "R programming for those coming from other languages\n\nIntroduction Assignment and underscore Variable name gotchas Vectors Sequences Types Boolean operators Lists Matrices Missing values and NaNs Comments Functions Scope Misc. Other resources\n\nIntroduction\n\nI have written software professionally in perhaps a dozen programming languages, and the hardest language for me to learn has been R. The language is actually fairly simple, but it is unconventional. These notes are intended to make the language easier to learn for someone used to more commonly used languages such as C++, Java, Perl, etc.\n\nR is more than a programming language. It is an interactive environment for doing statistics. I find it more helpful to think of R as having a programming language than being a programming language. The R language is the scripting language for the R environment, just as VBA is the scripting language for Microsoft Excel. Some of the more unusual features of the R language begin to make sense when viewed from this perspective.\n\nThis document is a work in progress. Corrections and comments are welcome.\n\nAssignment and underscore\n\nThe assignment operator in R is <- as in\n\ne <- m*c^2.\n\nIt is also possible, though uncommon, to reverse the arrow and put the receiving variable on the right, as in\n\nm*c^2 -> e.\n\nIt is sometimes possible to use = for assignment, though I don't understand when this is and is not allowed. Most people avoid the issue by always using the arrow.\n\nHowever, when supplying default function arguments or calling functions with named arguments, you must use the = operator and cannot use the arrow.\n\nAt some time in the past R, or its ancestor S, used underscore as assignment. This meant that the C convention of using underscores as separators in multi-word variable names was not only disallowed but produced strange side effects. For example, first_name would not be a single variable but rather the instruction to assign the value of name to the variable first! S-PLUS still follows this use of the underscore. However, R allows underscore as a variable character and not as an assignment operator.\n\nVariable name gotchas\n\nBecause the underscore was not allowed as a variable character, the convention arose to use dot as a name separator. Unlike its use in many object oriented languages, the dot character in R has no special significance, with two exceptions. First, the ls() function in R lists active variables much as the ls Unix shell command lists directory contents. As the ls shell command does not list files that begin with a dot, neither does the ls() function in R function show variables that begin with dot by default. Second, ... is used to indicate a variable number of function arguments.\n\nR uses $ in a manner analogous to the way other languages use dot.\n\nR has several one-letter reserved words: c, q, s, t, C, D, F, I, and T.\n\n(Actually, these are not reserved, but it's best to think of them as reserved. For example, c is a built-in function for creating vectors, though you could also create a variable named c. Worse, T and F are not synonyms for TRUE and FALSE but variables that have the expected values by default. So someone could include the code T <- FALSE; F <- TRUE and reverse their meanings!)\n\nVectors\n\nThe primary data type in R is the vector. Before describing how vectors work in R, it is helpful to distinguish two ideas of vectors in order to set the correct expectations\n\nThe first idea of a vector is what I will call a container vector. This is an ordered collection of numbers with no other structure, such as the vector<> container in C++. The length of a vector is the number of elements in the container. Operations are applied componentwise. For example, given two vectors x and y of equal length, x*y would be the vector whose nth component is the product of the nth components of x and y. Also, log(x) would be the vector whose nth component is the logarithm of the nth component of x.\n\nThe other idea of a vector is a mathematical vector, an element of a vector space. In this context \"length\" means geometrical length determined by an inner product; the number of components is called \"dimension.\" In general, operations are not applied componentwise. The expression x*y is a single number, the inner product of the vectors. The expression log(x) is meaningless.\n\nA vector in R is a container vector, a statistician's collection of data, not a mathematical vector. The R language is designed around the assumption that a vector is an ordered set of measurements rather than a geometrical position or a physical state. (R supports mathematical vector operations, but they are secondary in the design of the language.) This helps explain, for example, R's otherwise inexplicable vector recycling feature.\n\nAdding a vector of length 22 and a vector of length 45 in most languages would raise an exception; the language designers would assume the programmer has made an error and the program is now in an undefined state. However, R allows adding two vectors regardless of their relative lengths. The elements of the shorter summand are recycled as often as necessary to create a vector the length of the longer summand. This is not attempting to add physical vectors that are incompatible for addition, but rather a syntactic convenience for manipulating sets of data. (R does issue a warning when adding vectors of different lengths and the length of the longer vector is not an integer multiple of the length of the shorter vector. So, for example, adding vectors of lengths 3 and 7 would cause a warning, but adding vectors of length 3 and 6 would not.)\n\nThe R language has no provision for scalars, nothing like a double in C-family languages. The only way to represent a single number in a variable is to use a vector of length one. And while it is possible to iterate through vectors as one might do in a for loop in C, it is usually clearer and more efficient in R to operate on vectors as a whole.\n\nVectors are created using the c function. For example, p <- c(2,3,5,7) sets p to the vector containing the first four prime numbers.\n\nVectors in R are indexed starting with 1 and matrices in are stored in column-major order. In both of these ways R resembles FORTRAN.\n\nElements of a vector can be accessed using []. So in the above example, p[3] is 5.\n\nVectors automatically expand when assigning to an index past the end of the vector, as in Perl.\n\nNegative indices are legal, but they have a very different meaning than in some other languages. If x is an array in Python or Perl, x[-n] returns the nth element from the end of the vector. In R, x[-n] returns a copy of x with the nth element removed.\n\nBoolean values can also be used as indices, and they behave differently than integers. See Five kinds of subscripts in R.\n\nSequences\n\nThe expression seq(a, b, n) creates a closed interval from a to b in steps of size n. For example, seq(1, 10, 3) returns the vector containing 1, 4, 7, and 10. This is similar to range(a, b, n) in Python, except Python uses open intervals and so the 10 would not be included in this example. The step size argument n defaults to 1 in both R and Python.\n\nThe notation a:b is an abbreviation for seq(a, b, 1).\n\nThe notation seq(a, b, length=n) is a variation that will set the step size to (b-a)/(n-1) so that the sequence has n points.\n\nTypes\n\nThe type of a vector is the type of the elements it contains and must be one of the following: logical, integer, double, complex, character, or raw. All elements of a vector must have the same underlying type. This restriction does not apply to lists.\n\nType conversion functions have the naming convention as.xxx for the function converts its argument to type xxx. For example, as.integer(3.2) returns the integer 3, and as.character(3.2) returns the string \"3.2\".\n\nBoolean operators\n\nYou can input T or TRUE for true values and F or FALSE for false values.\n\nThe operators & and | apply element-wise on vectors. The operators && and || are often used in conditional statements and use lazy evaluation as in C: the operators will not evaluate their second argument if the return value is determined by the first argument.\n\nLists\n\nLists are like vectors, except elements need not all have the same type. For example, the first element of a list could be an integer and the second element be a string or a vector of Boolean values.\n\nLists are created using the list function. Elements can be access by position using [[]]. Named elements may be accessed either by position or by name.\n\nNamed elements of lists act like C structs, except a dollar sign rather than a dot is used to access elements. For example, consider,\n\na <- list(name=\"Joe\", 4, foo=c(3,8,9))\n\nNow a[[1]] and a$name both equal the string \"Joe\".\n\nIf you attempt to access a non-existent element of a list, say a[[4]] above, you will get an error. However, you can assign to a non-existent element of a list, thus extending the list. If the index you assign to is more than one past the end of the list, intermediate elements are created and assigned NULL values. You can also assign to non-existent named fields, such as saying a$baz = TRUE.\n\nMatrices\n\nIn a sense, R does not support matrices, only vectors. But you can change the dimension of a vector, essentially making it a matrix.\n\nFor example, m <- array( c(1,2,3,4,5,6), dim=c(2,3) ) creates a matrix m. However, it may come as a surprise that the first row of m has elements 1, 3, and 5. This is because by default, R fills matrices by column, like FORTRAN. To fill m by row, add the argument by.row = TRUE to the call to the array function.\n\nMissing values and NaNs\n\nAs in other programming languages, the result of an operation on numbers may return NaN, the symbol for \"not a number.\" For example, an operation might overflow the finite range of a machine number, or a program might request an undefined operation, such as dividing by zero.\n\nR also has a different type of non-number, NA for \"not applicable.\" NA is used to indicate missing data, and is unfortunately fairly common in data sets. NA in R is similar to NULL in SQL or nullable types in C#. However, one must be more careful about NA values in R than about nulls in SQL or C#. The designer of database or the author of a piece of C# code specifies which values are nullable and can avoid the issue by simply not allowing such values. The author of an R function, however, has no control over the data his function will receive because NA is a legal value inside an R vector. There is no way to specify that a function takes only vectors with non-null components. You must handle NA values, even if you handle them by returning an error.\n\nThe function is.nan will return TRUE for those components of its argument that are NaN. The function is.na will return true for those components that are NA or NaN.\n\nComments\n\nComments begin with # and continue to the end of the line, as in Python or Perl.\n\nFunctions\n\nThe function definition syntax of R is similar to that of JavaScript. For example:\n\nf <- function(a, b)\n{\n    return (a+b)\n}\n\nThe function function returns a function, which is usually assigned to a variable, f in this case, but need not be. You may use the function statement to create an anonymous function (lambda expression).\n\nNote that return is a function; its argument must be contained in parentheses, unlike C where parentheses are optional. The use of return is optional; otherwise the value of the last line executed in a function is its return value.\n\nDefault values are defined similarly to C++. In the following example, b is set to 10 by default.\n\nf <- function(a, b=10)\n{\n    return (a+b)\n}\n\nSo f(5, 1) would return 6, and f(5) would return 15. R allows more sophisticated default values than does C++. A default value in R need not be a static type but could, for example, be a function of other arguments.\n\nC++ requires that if an argument has a default value then so do all values to the right. This is not the case in R, though it is still a good idea. The function definition\n\nf <- function(a=10, b)\n{\n    return (a+b)\n}\n\nis legal, but calling f(5) would cause an error. The argument a would be assigned 5, but no value would be assigned to b. The reason such a function definition is not illegal is that one could still call the function with one named argument. For example, f(b=2) would return 12.\n\nFunction arguments are passed by value. The most common mechanism for passing variables by reference is to use non-local variables. (Not necessarily global variables, but variables in the calling routine's scope.) A safer alternative is to explicitly pass in all needed values and return a list as output.\n\nScope\n\nR uses lexical scoping while S-PLUS uses static scope. The difference can be subtle, particularly when using closures.\n\nSince variables cannot be declared — they pop into existence on first assignment — it is not always easy to determine the scope of a variable. You cannot tell just by looking at the source code of a function whether a variable is local to that function.\n\nMisc.\n\nHere are a few miscellaneous facts about R that may be useful.\n\nhelp(fctn) displays help on any function fctn, as in Python.\n\nTo invoke complex arithmetic, add 0i to a number. For example, sqrt(-1) returns NaN, but sqrt(-1 + 0i) returns 0 + 1i.\n\nsessionInfo() prints the R version, OS, packages loaded, etc.\n\nls() shows which objects are defined.\n\nrm(list=ls()) clears all defined objects.\n\ndev.new() opens a new plotting window without overwriting the previous one.\n\nThe function sort() does not change its argument.\n\nDistribution function prefixes d, p, q, r stand for density (PDF), probability (CDF), quantile (CDF-1), and random sample. For example, dnorm is the density function of a normal random variable and rnorm generates a sample from a normal random variable. The corresponding functions for a uniform random variable are dunif and runif.\n\nOther resources\n\nThe R Project\n\nThe R Book\n\nRadford Neal's series on design flaws in R. Part I, II, III.\n\nThe R Inferno\n\nStackOverflow questions tagged 'R'\n\nMy blog posts about R\n\nHome\n\nBlog\n\nTwitter\n\nConsulting\n\njdc"}
{"slug": "accented_letters_page", "canonical_url": "https://www.johndcook.com/blog/accented_letters_page/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/accented_letters.html", "title": "Accented letters in HTML, TeX, and Microsoft Word", "heading": "Accented letters in HTML, TeX, and Microsoft Word", "description": "Representing accented letters in HTML, TeX, and Microsoft Word.", "summary": "This page explains the patterns behind how HTML, TeX, and Microsoft Word represent accented letters. The page begins with explanations for HTML, LaTeX, and Word and concludes with a chart summarizing how accents are handled across languages.", "word_count": 556, "blocks": [{"tag": "h1", "text": "Accented letters in HTML, TeX, and Microsoft Word"}, {"tag": "p", "text": "This page explains the patterns behind how HTML, TeX, and Microsoft Word represent accented letters. The page begins with explanations for HTML, LaTeX, and Word and concludes with a chart summarizing how accents are handled across languages."}, {"tag": "h2", "text": "HTML"}, {"tag": "p", "text": "HTML uses “entities”, escape sequences, to represent accented letters (diacritics) with only ASCII characters. For example, à is encoded as &agrave;. The general pattern for constructing an HTML entity for an accented letter is"}, {"tag": "p", "text": "& + letter + accent code + ;"}, {"tag": "p", "text": "The letters that can be accented this way are a, c, e, i, n, o, u, and y, in both their lower-case and upper-case forms."}, {"tag": "p", "text": "The possible accent codes are grave, acute, circ (circumflex), tilde, and uml (umlaut)."}, {"tag": "p", "text": "Not all combinations of letters and accents are possible. For example, the entity &agrave; places a grave accent on the letter “a”, but there is no entity &ngrave; to put such an accent on top of a letter “n.”"}, {"tag": "p", "text": "There are three additional accent codes that can only be applied to a single letter. The code ring can only be applied to “a.” &aring; produces å and &Aring; produces Å. The cedil (cedilla) code can only be applied to “c.” &ccedil; produces ç and &Ccedil; produces Ç. Finally, slash only applies to “o.” &oslash; produces ø and &Oslash; produces Ø."}, {"tag": "p", "text": "The æ ligature follows a pattern similar to accents: &aelig; produces æ and &AElig; produces Æ."}, {"tag": "p", "text": "Note that most HTML entities are not legal in XHTML. See Greek letters in HTML, XML, TeX, and Unicode for an explanation of how to denote Unicode symbols in ASCII text in XML."}, {"tag": "h2", "text": "TeX and LaTeX"}, {"tag": "p", "text": "TeX uses a consistent pattern for adding diacritical marks to letters:"}, {"tag": "p", "text": "\\ + code + letter"}, {"tag": "p", "text": "The code is a single character indicating the kind of diacritical mark to add. Unlike HTML entities, any mark can be added to any letter. Letters may be upper-case or lower-case."}, {"tag": "p", "text": "TeX supports 14 diacritical mark codes. The ones corresponding to HTML entities listed above are ` (grave), ' (acute), ^ (circumflex), ~ (tilde), \" (umlaut), and c (cedilla)."}, {"tag": "p", "text": "Examples: jalapeño is written jalape\\~no in TeX. garçon is written gar\\ccon."}, {"tag": "p", "text": "TeX uses \\ae and \\AE for æ and Æ, \\o and \\O for ø and Ø, and \\aa and \\AA for å and Å."}, {"tag": "p", "text": "Note that the above remarks only apply to text mode. LaTeX handles accents differently in math mode."}, {"tag": "h2", "text": "Microsoft Word"}, {"tag": "p", "text": "Microsoft Word is nearly identical to HTML in the accent-letter combinations it supports, but similar to TeX in the key strokes that it uses."}, {"tag": "p", "text": "To add a grave accent to a vowel in Word, hold down the control key and press ` (sometimes call the back tick or back quote symbol, often in the top left of a US keyboard) then type the letter to accent. In short, CTRL + ` is the escape sequence."}, {"tag": "p", "text": "Use CTRL + ' for an acute accent, CTRL + ^ for circumflex, CTRL + SHIFT + ~ for tilde, CTRL + SHIFT + : for umlaut, and CTRL + , for cedilla."}, {"tag": "p", "text": "Word uses CTRL + SHIFT + & for æ and Æ, CTRL + / for ø and Ø, and CTRL + SHIFT + @ for å and Å."}, {"tag": "h2", "text": "Combined chart"}, {"tag": "p", "text": "See also Common math symbols in HTML, XML, TeX, and Unicode."}], "content": "Accented letters in HTML, TeX, and Microsoft Word\n\nThis page explains the patterns behind how HTML, TeX, and Microsoft Word represent accented letters. The page begins with explanations for HTML, LaTeX, and Word and concludes with a chart summarizing how accents are handled across languages.\n\nHTML\n\nHTML uses “entities”, escape sequences, to represent accented letters (diacritics) with only ASCII characters. For example, à is encoded as &agrave;. The general pattern for constructing an HTML entity for an accented letter is\n\n& + letter + accent code + ;\n\nThe letters that can be accented this way are a, c, e, i, n, o, u, and y, in both their lower-case and upper-case forms.\n\nThe possible accent codes are grave, acute, circ (circumflex), tilde, and uml (umlaut).\n\nNot all combinations of letters and accents are possible. For example, the entity &agrave; places a grave accent on the letter “a”, but there is no entity &ngrave; to put such an accent on top of a letter “n.”\n\nThere are three additional accent codes that can only be applied to a single letter. The code ring can only be applied to “a.” &aring; produces å and &Aring; produces Å. The cedil (cedilla) code can only be applied to “c.” &ccedil; produces ç and &Ccedil; produces Ç. Finally, slash only applies to “o.” &oslash; produces ø and &Oslash; produces Ø.\n\nThe æ ligature follows a pattern similar to accents: &aelig; produces æ and &AElig; produces Æ.\n\nNote that most HTML entities are not legal in XHTML. See Greek letters in HTML, XML, TeX, and Unicode for an explanation of how to denote Unicode symbols in ASCII text in XML.\n\nTeX and LaTeX\n\nTeX uses a consistent pattern for adding diacritical marks to letters:\n\n\\ + code + letter\n\nThe code is a single character indicating the kind of diacritical mark to add. Unlike HTML entities, any mark can be added to any letter. Letters may be upper-case or lower-case.\n\nTeX supports 14 diacritical mark codes. The ones corresponding to HTML entities listed above are ` (grave), ' (acute), ^ (circumflex), ~ (tilde), \" (umlaut), and c (cedilla).\n\nExamples: jalapeño is written jalape\\~no in TeX. garçon is written gar\\ccon.\n\nTeX uses \\ae and \\AE for æ and Æ, \\o and \\O for ø and Ø, and \\aa and \\AA for å and Å.\n\nNote that the above remarks only apply to text mode. LaTeX handles accents differently in math mode.\n\nMicrosoft Word\n\nMicrosoft Word is nearly identical to HTML in the accent-letter combinations it supports, but similar to TeX in the key strokes that it uses.\n\nTo add a grave accent to a vowel in Word, hold down the control key and press ` (sometimes call the back tick or back quote symbol, often in the top left of a US keyboard) then type the letter to accent. In short, CTRL + ` is the escape sequence.\n\nUse CTRL + ' for an acute accent, CTRL + ^ for circumflex, CTRL + SHIFT + ~ for tilde, CTRL + SHIFT + : for umlaut, and CTRL + , for cedilla.\n\nWord uses CTRL + SHIFT + & for æ and Æ, CTRL + / for ø and Ø, and CTRL + SHIFT + @ for å and Å.\n\nCombined chart\n\nSee also Common math symbols in HTML, XML, TeX, and Unicode."}
{"slug": "articles", "canonical_url": "https://www.johndcook.com/blog/articles/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/articles.html", "title": "Journal articles and technical reports", "heading": "Journal articles and technical reports", "description": "Journal articles and technical reports by John Cook related to clinical trials, applied math, and computing.", "summary": "This page lists academic articles. See also informal articles on software development and math and statistics.", "word_count": 4514, "blocks": [{"tag": "h1", "text": "Journal articles and technical reports"}, {"tag": "p", "text": "This page lists academic articles. See also informal articles on software development and math and statistics."}, {"tag": "p", "text": "Contact info"}, {"tag": "p", "text": "John D. Cook, Robert Primmer, Ab de Kwant. Comparing cost and performance of replication in erasure coding. Hitachi Review, vol 63 (July 2014)."}, {"tag": "p", "text": "Abstract. Data storage systems are more reliable than their individual components. In order to build highly reliable systems out of less reliable parts, systems introduce redundancy. In replicated systems, objects are simply copied several times with each copy residing on a different physical device. While such an approach is simple and direct, more elaborate approaches such as erasure coding can achieve equivalent levels of data protection while using less redundancy. This report examines the trade-offs in cost and performance between replicated and erasure encoded storage systems."}, {"tag": "p", "text": "John D. Cook Approximating random inequalities with Edgeworth expansions (2012). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 78."}, {"tag": "p", "text": "Abstract. Random inequalities of the form Prob (X > Y + δ) often appear as part of Bayesian clinical trial methods. Simulating trial designs could require calculating millions of random inequalities. When these inequalities require numerical integration, or worse random sampling, the inequality calculations account for the large majority of the simulation time. In this paper we show how to approximate random inequalities using Edgeworth expansions. The calculations required to use these expansions can be done in closed form, as we will see below. Although the calculations are elementary, they are also somewhat tedious, and so we include Python code to illustrate how to use the approximations in practice. We make no distributional assumptions on the random variables X and Y other than requiring that the necessary moments exist. The accuracy of the approximation will depend on how well the densities of these random variables are approximated by the Edgeworth expansions."}, {"tag": "p", "text": "John D. Cook Fast approximation of gamma inequalities (2012). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 77."}, {"tag": "p", "text": "Abstract. Approximation for computing P(X > Y + δ) for independent gamma random variables X and Y."}, {"tag": "p", "text": "John D. Cook Fast approximation of beta inequalities (2012). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 76."}, {"tag": "p", "text": "Abstract. Approximation for computing P(X > Y + δ) for independent beta random variables X and Y."}, {"tag": "p", "text": "John D. Cook CRM: Prior means and medians (2012). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 73."}, {"tag": "p", "text": "Abstract. Resolving the confusion between two ways of specifying a CRM dose-finding trial design."}, {"tag": "p", "text": "John D. Cook Random inequalities between survival and uniform distributions (2011). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 71."}, {"tag": "p", "text": "Abstract. This note will look at ways of computing P(X>Y) where X is a distribution modeling survival (gamma, inverse gamma, Weibull, log-normal) and Y has a uniform distribution. Each of these can be computer in closed form in terms of common statistical functions. We begin with analytical calculations and then include software implementations in R to make some of the details more explicit. Finally, we give a suggestion for using simulation to compute random inequalities that cannot be computed in closed form."}, {"tag": "p", "text": "John D. Cook Basic properties of the soft maximum (2011). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 70."}, {"tag": "p", "text": "Abstract. This note presents the basic properties of the soft maximum, a smooth approximation to the maximum of two real variables. It concludes by looking at potential numerical difficulties with the soft maximum and how to avoid these difficulties."}, {"tag": "p", "text": "John D. Cook, Jairo Fúquene, Luis Pericchi. Skeptical and optimistic robust priors for clinical trials, Revista Columbiana de Estadistica, (2011) 34 no. 2, pp. 333–345."}, {"tag": "p", "text": "Abstract. A useful technique from the subjective Bayesian viewpoint, suggested by Spiegelhalter et al. (1994), is to ask the subject matter researchers and other parties involved, such as pharmaceutical companies and regulatory bodies, for reasonable optimistic and pessimistic priors regarding the effectiveness of a new treatment. Up to now, the proposed skeptical and optimistic priors have been limited to conjugate priors, though there is no need for this limitation. The same reasonably adversarial points of view can be taken with robust priors. A recent reference with robust priors usefully applied to clinical trials is in Fuquene, Cook, and Pericchi (2009). Our proposal in this paper is to use Cauchy and intrinsic robust priors for both skeptical and optimistic priors leading to results more closely related with the sampling data when prior and data are in conflict. In other words, the use of robust priors removes the dogmatism implicit in conjugate priors."}, {"tag": "p", "text": "John D. Cook. Block Adaptive Randomization (2011). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 63."}, {"tag": "p", "text": "Abstract. This note proposes a block-adaptive randomization method to limit the length of runs in an outcome-adaptive randomized trial."}, {"tag": "p", "text": "John D. Cook Upper bounds on non-central chi-squared tails and truncated normal moments (2010). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 62."}, {"tag": "p", "text": "Abstract. We show that moments of the truncated normal distribution provide upper bounds on the tails of the non-central chi-squared distribution, then develop upper bounds for the former."}, {"tag": "p", "text": "John D. Cook Asymptotic results for Normal-Cauchy model (2010). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 61."}, {"tag": "p", "text": "Abstract. This report proves asymptotic results for the posterior mean when sampling from a normal distribution with a Cauchy prior on the location parameter."}, {"tag": "p", "text": "John D. Cook Determining distribution parameters from quantiles"}, {"tag": "p", "text": "Abstract. Bayesian statistics often requires eliciting prior probabilities from subject matter experts who are unfamiliar with statistics. While most people an intuitive understanding of the mean of a probability distribution, fewer people understand variance as well, particularly in the context of asymmetric distributions. Prior beliefs may be more accurately captured by asking experts for quantiles rather than for means and variances. This note will explain how to solve for parameters so that common distributions satisfy two quantile conditions. We present algorithms for computing these parameters and point to corresponding software. The distributions discussed are normal, log normal, Cauchy, Weibull, gamma, and inverse gamma. The method given for the normal and Cauchy distributions applies more generally to any location-scale family."}, {"tag": "p", "text": "John D. Cook. Exact calculation of inequality probabilities (2009). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 54."}, {"tag": "p", "text": "Abstract. This note surveys results for computing the inequality probability P(X > Y) in closed form where X and Y are independent continuous random variables. Distribution families discussed include normal, Cauchy, gamma, inverse gamma, Levy, folded normal, and beta. Mixture distributions are also discussed."}, {"tag": "p", "text": "Jairo A. Fúquene P., John D. Cook, Luis Raúl Pericchi. A Case for Robust Bayesian priors with Applications to Binary Clinical Trials. Bayesian Analysis (2009) 4, Number 4, pp. 817–846."}, {"tag": "p", "text": "Abstract. Bayesian analysis is frequently limited to conjugate Bayesian analysis, particularly in the case in the analysis of clinical trial data. Even though conjugate analysis may be simpler computationally, the price to be paid is high: such analysis is not robust with respect to the prior, i.e., changing the prior may affect the conclusions without bound. Furthermore conjugate Bayesian analysis is blind with respect to the potential conflict between the prior and the data. On the other hand, robust priors have bounded influence. The prior is discounted automatically when there are conflicts between prior information and data. The original proposal of robust priors was made by de-Finetti in the 1960's. However, the practice has not taken hold in important areas such as in clinical trials where conjugate priors are ubiquitous."}, {"tag": "p", "text": "We show here how the Bayesian analysis for simple binary binomial data, after expressing in its exponentially family form, is improved by employing Cauchy priors. Moreover, we also introduce in the analysis of clinical trials a robust prior originally developed by J.O. Berger that gives closed-form results when coupled with a normal log-odds likelihood. Berger's prior yields the superior robust analysis with no added computational complication compared to the conjugate analysis. We illustrate the results with famous textbook examples and a with data set and a prior from a previous trial."}, {"tag": "p", "text": "On the formal side, we give here a theorem that we call the “Polynomial Tails Comparison Theorem.” This theorem establishes the analytical behavior of any likelihood function with tails bounded by a polynomial when used with priors with polynomial-order tails, such as Cauchy or Student's t. The likelihood does not have to be a location family nor exponential family distribution and the conditions of the theorem are easily verifiable. For Berger's prior robustness can be established directly since the exact expressions for posterior moments are known."}, {"tag": "p", "text": "John D. Cook. Inequality Probabilities for Folded Normal Random Variables (2009). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 52."}, {"tag": "p", "text": "Abstract. This note explains how to calculate the probability Pr(|X| > |Y|) for normal random variables X and Y. (A random variable formed by taking the absolute value of a normal random variable is known as a folded normal random variable.) When X and Y have equal variance, a simple expression is obtained. Otherwise the problem is reduced to a well-known problem."}, {"tag": "p", "text": "Valen E. Johnson, John D. Cook. Bayesian Design of Single-Arm Phase II Clinical Trials with Continuous Monitoring. Clinical Trials 2009; 6(3):217–26. (preprint)"}, {"tag": "p", "text": "Abstract. Many “Bayesian” clinical trial designs use posterior credible intervals as tools to define stopping boundaries for inferiority, futility, or superiority. However, the thresholds on posterior credible intervals that trigger termination of a trial are determined by frequentist operating characteristics. This practice can result in substantial overlap between the credible intervals associated with, say, stopping a trial for superiority and stopping a trial for inferiority, which severely limits the interpretation of posterior probability statements. In this article, we use formal Bayesian hypothesis tests to design single-arm phase II clinical trials. By using non-local prior densities to define null and alternative models, we obtain exponential convergence of Bayes factors under both null and alternative models. When compared to other commonly used Bayesian and frequentist designs, we show that our method provides better operating characteristics, uses fewer patients per correct decision, and provides more directly interpretable results. We also demonstrate that designs based on Bayesian hypothesis tests eliminates a potential source of bias often associated with Bayesian trial designs."}, {"tag": "p", "text": "John D. Cook, Luis Raúl Pericchi. Information and Cross-Entropic Approaches, In: Lauretto MS; Pereira CAB; Stern JM (Org.), Bayesian Methods and Maximum Entropy Methods in Science and Engineering 28, Melville: AIP — American Institute of Physics, 2008, v 28, pp 278–285."}, {"tag": "p", "text": "Abstract. In a recent working paper, Fúquene, Cook, and Pericchi make a comprehensive proposal putting forward robust, heavy-tailed priors over conjugate, light-tailed priors in Bayesian analysis. The paper focuses particularly on clinical trials, where information from previous trials should be used in an non-dogmatic fashion, suggesting the use of robust priors. Robust priors have bounded influence in the posterior distribution and their influence is inversely related to the conflict between the data in previous and the current trial. Clearly, the likelihood has to be taken into consideration and not only the prior. We explore here a novel proposal based on a cross-entropy measure of comparison between different models, on which the expectations of the log ratio of the evidences of the contender models are taken with respect to each of the different models and then compared. We also compute the expected increase of information within each model. Both criteria seems to justify the use of robust priors."}, {"tag": "p", "text": "John D. Cook. Exact operating characteristics for single-arm Phase II trials (2008). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 45."}, {"tag": "p", "text": "Abstract. Simulation is so widely used in studying the operating characteristics of clinical trials that we may forget that simulation is not always necessary. This note gives an algorithm for computing the operating characteristics of a stopping rule for a single-arm phase II clinical trial exactly."}, {"tag": "p", "text": "Yisheng Li, Benjamin Bekele, Yuan Ji, and John D. Cook. Dose-Schedule Finding in Phase I/II Clinical Trials Using Bayesian Isotonic Transformation (2008). Statistics in Medicine 27:4895–4913"}, {"tag": "p", "text": "Abstract. The intent of most phase I oncology trials is to determine the maximum-tolerated dose (MTD) of an experimental treatment. One of the main considerations apart from determining the MTD is determining an appropriate schedule for administration of the treatment. Historically, schedules have been fixed prior to the start of dose finding. Recently, an increasing number of trials have been designed to determine the MTDs during a phase I component and subsequently determine a schedule during a phase II component. In this paper, we propose a Bayesian design for dose-schedule finding by jointly modeling binary toxicity and efficacy outcomes. Assuming the probability of toxicity follows an order constraint between schedules, we apply a Bayesian isotonic transformation approach to estimating the constrained parameters. We select a dose-schedule combination based on the joint posterior distribution of toxicity and efficacy. Using simulation studies for a hypothetical and a practical cancer clinical trial, we demonstrate that the proposed design performs well under different clinical scenarios."}, {"tag": "p", "text": "John D. Cook. The Effect of Population Drift on Adaptively Randomized Trials (2007). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 39."}, {"tag": "p", "text": "Abstract. Adaptively randomized trials aim to treat patients in clinical trials more effectively by increasing the probability of assigning treatments that appear to have a higher probability of response. Studies of adaptive randomization to date have assumed constant probabilities of response on each treatment. This paper examines the effect of response probabilities that change over time due to population drift."}, {"tag": "p", "text": "Harry T. Whelan et al, Practical model-based dose-finding in early phase clinical trials: Optimizing tPA dose for treatment of ischemic stroke in children. Stroke. 39 (2008) 2627-2636."}, {"tag": "p", "text": "Abstract. A safe and effective tissue plasminogen activator (tPA) dose for childhood stroke has not been established. This paper describes a Bayesian outcome-adaptive method for determining the best dose of an experimental agent, and explains how this method was used to design a dose-finding trial for tPA in childhood acute ischemic stroke (AIS)."}, {"tag": "p", "text": "J. Kyle Wathen, Peter F. Thall, John D. Cook, Elihu H. Estey, Accounting for Patient Heterogeneity in Phase II Clinical Trials. Statistics in Medicine. 27 (2008) 2802–2815."}, {"tag": "p", "text": "Abstract. Phase II clinical trials typically are single-arm studies conducted to decide whether an experimental treatment is sufficiently promising, relative to standard treatment, to warrant further investigation. Many methods exist for conducting phase II trials under the assumption that patients are homogeneous. In the presence of patient heterogeneity, however, these designs are likely to draw incorrect conclusions. We propose a class of model-based Bayesian designs for single-arm phase II trials with a binary or time-to-event outcome and two or more prognostic subgroups. The designs' early stopping rules are subgroup specific and allow the possibility of terminating some subgroups while continuing others, thus providing superior results when compared with designs that ignore treatment-subgroup interactions. Because our formulation requires informative priors on standard treatment parameters and subgroup main effects, and non-informative priors on experimental treatment parameters and treatment-subgroup interactions, we provide an algorithm for computing prior hyperparameter values. A simulation study is presented and the method is illustrated by a chemotherapy trial in acute leukemia."}, {"tag": "p", "text": "Marcos de Lima et al, Phase I/II study of gemtuzumab ozogamicin added to fludarabine, melphalan and allogeneic hematopoietic stem cell transplantation for high-risk CD33 positive myeloid leukemias and myelodysplastic syndrome. Leukemia 22 (2008) pp 258–264."}, {"tag": "p", "text": "Abstract. We investigated the hypothesis that gemtuzumab ozogamicin (GO), an anti-CD33 immunotoxin would improve the efficacy of fludarabine/melphalan as a preparative regimen for allogeneic hematopoietic stem cell transplantation (HSCT) in a phase I/II trial. Toxicity was defined as grades III–IV organ damage, engraftment failure or death within 30 days. 'Response' was engraftment and remission (CR) on day +30. We sought to determine the GO dose (2, 4 or 6 mg m-2) giving the best trade-off between toxicity and response. All patients were not candidates for myeloablative regimens. Treatment plan: GO (day -12), fludarabine 30 mg m-2 (days -5 to -2), melphalan 140 mg m-2 (day -2) and HSCT (day 0). GVHD prophylaxis was tacrolimus and mini-methotrexate. Diagnoses were AML (n=47), MDS (n=4) or CML (n=1). Median age was 53 years (range, 13–72). All but three patients were not in CR. Donors were related (n=33) or unrelated (n=19). Toxicity and response rates at 4 mg m-2 were 50% (n=4) and 50% (n=4). GO dose was de-escalated to 2 mg m-2: 18% had toxicity (n=8) and 82% responded (n=36). 100-day TRM was 15%; one patient had reversible hepatic VOD. Median follow-up was 37 months. Median event-free and overall survival was 6 and 11 months. GO 2 mg m-2 can be safely added to fludarabine/melphalan, and this regimen merits further evaluation."}, {"tag": "p", "text": "John D. Cook, Comparing Methods of Tuning Adaptively Randomized Trials (2007). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 32."}, {"tag": "p", "text": "Abstract. The simplest Bayesian adaptive randomization scheme is to randomize patients to a treatment with probability equal to the probability p that the treatment is better. We examine three variations on adaptive randomization which are used to compromise between this scheme and equal randomization. The first variation is to apply a power transformation to p to obtain randomization probabilities. The second is to clip p to live within specified lower and upper bounds. The third is to begin the trial with a burn-in period of equal randomization. We illustrate how each approach effects statistical power and the number of patients assigned to each treatment. We conclude with recommendations for designing adaptively randomized clinical trials."}, {"tag": "p", "text": "John D. Cook, Understanding the Exponential Tuning Parameter in Adaptively Randomized Trials (2006). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 27."}, {"tag": "p", "text": "Abstract. We examine the effect of a parameter λ used to calibrate how responsive randomization probabilities are to observed data in an adaptively randomized clinical trial. We define and motivate the parameter λ and demonstrate how varying this parameter effects the operating characteristics of example clinical trial designs."}, {"tag": "p", "text": "John D. Cook and Saralees Nadarajah. Stochastic Inequality Probabilities for Adaptively Randomized Clinical Trials. Biometrical Journal. 48 (2006) pp 256–365."}, {"tag": "p", "text": "Abstract. We examine stochastic inequality probabilities of the form P(X > Y) and P(X > \\max(Y, Z)) where X, Y, and Z are random variables with beta, gamma, or inverse gamma distributions. We discuss the applications of such inequality probabilities to adaptively randomized clinical trials as well as methods for calculating their values."}, {"tag": "p", "text": "Peter F. Thall and John D. Cook. Using both efficacy and toxicity for dose-finding. In S. Chevret (ed), Statistical Methods for Dose Finding Experiments. New York: John Wiley & Sons, June 2006."}, {"tag": "p", "text": "Peter F. Thall and John D. Cook. Adaptive dose-finding based on efficacy-toxicity trade-offs. Encyclopedia of Biopharmaceutical Statistics, 2nd edition, 2006, Chein-Chung Chow editor."}, {"tag": "p", "text": "Peter F. Thall, John D. Cook, and Elihu H. Estey. Adaptive dose selection using efficacy-toxicity trade-offs: illustrations and practical considerations. J Biopharmaceutical Stat. 16: 623–638 (2006)"}, {"tag": "p", "text": "Abstract. The purpose of this paper is to describe and illustrate an outcome-adaptive Bayesian procedure, proposed by Thall and Cook (2004), for assigning doses of an experimental treatment to successive cohorts of patients. The method uses elicited (efficacy, toxicity) probability pairs to construct a family of trade-off contours that are used to quantify the desirability of each dose. This provides a basis for determining a best dose for each cohort. The method combines the goals of conventional Phase I and Phase II trials, and thus may be called a “Phase I-II” design. We first give a general review of the probability model and dose-finding algorithm. We next describe an application to a trial of a biologic agent for treatment of acute myelogenous leukemia, including a computer simulation study to assess the design's average behavior. To illustrate how the method may work in practice, we present a cohort-by-cohort example of a particular trial. We close with a discussion of some practical issues that may arise during implementation."}, {"tag": "p", "text": "John D. Cook. Efficacy-toxicity trade-offs based on Lp norms (2006). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 29."}, {"tag": "p", "text": "Abstract. This report examines in detail a family of efficacy-toxicity trade-off functions simpler and more general than those originally proposed in [1]. The new trade-off functions are based on distance in Lp norm to the ideal point and were first presented in [2]. We define and illustrate these functions and demonstrate how to compute their parameters based on elicited values."}, {"tag": "p", "text": "J. Kyle Wathen and John D. Cook. Power and bias in adaptively randomized clinical trials (2006). Technical Report UTMDABTR-002-06."}, {"tag": "p", "text": "Abstract. This report examines the operating characteristics of adaptively randomized trials relative to equally randomized trials in regard to power and bias. We also examine the number of patients in the trial assigned to the superior treatment. The effects of prior selection, sample size, and patient prognostic factors are investigated for both binary and time-to-event outcomes."}, {"tag": "p", "text": "John D. Cook. Numerical evaluation of gamma inequalities (2006). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 30."}, {"tag": "p", "text": "Abstract. This paper addresses the problem of numerically evaluating the probabilities P(X > Y), P(X > max(Y,Z)), and P(X < min(Y,Z)) where X, Y, and Z are independent gamma or inverse gamma random variables."}, {"tag": "p", "text": "John D. Cook. Continuous safety monitoring in single-arm, time-to-event trials without software (2005). Technical Report UTMDABTR-006-05."}, {"tag": "p", "text": "Abstract. This note concerns trial conduct for one-arm trials that monitor safety by comparing time-to-event outcomes of the experimental treatment to an historical treatment. To date, such trials have been conducted using software which evaluates the stopping rule as the trial progresses. We show that is it possible to pre-calculate the stopping conditions, simplifying trial conduct and opening up new possibilities."}, {"tag": "p", "text": "John D. Cook. Exact calculation of beta inequalities (2005). Technical Report UTMDABTR-005-05."}, {"tag": "p", "text": "Abstract. This paper addresses the problem of evaluating P(X > Y) where X and Y are independent beta random variables. We cast the problem in terms of a hypergeometric function and use hypergeometric identities to calculate the probability in closed form for certain values of the distribution parameters."}, {"tag": "p", "text": "Peter F. Thall and John D. Cook. Dose-finding based on efficacy-toxicity trade-offs (2004) Biometrics, 60:684–693."}, {"tag": "p", "text": "Abstract. We present an adaptive Bayesian method for dose-finding in phase I/II clinical trials based on trade-offs between the probabilities of treatment efficacy and toxicity. The method accommodates either trinary or bivariate binary outcomes, as well as efficacy probabilities that are potentially non-monotone in dose. Doses are selected for successive patient cohorts based on a set of efficacy-toxicity trade-off contours that partition the two-dimensional outcome probability domain. Priors are established by solving for hyperparameters which optimize the fit of the model to elicited mean outcome probabilities. For trinary outcomes, the new algorithm is compared to the method of Thall and Russell by application to a trial of rapid treatment for ischemic stroke. The bivariate binary outcome case is illustrated by a trial of graft-versus-host disease prophylaxis in allogeneic bone marrow transplantation. Computer simulations show that, under a wide rage of dose-outcome scenarios, the new method has high probabilities of making correct decisions and treats most patients at doses with desirable efficacy-toxicity trade-offs."}, {"tag": "p", "text": "John D. Cook. Simulation results for phase II clinical trial durations (2004). Technical Report UTMDABTR-014-04"}, {"tag": "p", "text": "Abstract. This paper investigates the effect of cohort size on phase II clinical trial duration by doing a simulation study of a monitoring method of Thall and Simon. We challenge the assumptions that larger cohort sizes lead to shorter trials and that continuous monitoring is impractical."}, {"tag": "p", "text": "John D. Cook. Numerical computation of stochastic inequality probabilities (2003). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 46."}, {"tag": "p", "text": "Abstract. This paper addresses the problem of numerically evaluating P(X > Y) for independent continuous random variables X and Y. This calculation arises in the design of clinical trials and as such appears in the inner loop of simulations of these trials. An early example of this is given in (Thompson 1933). More recent examples are given in (Giles et al 2003), (Berry 2003a, 2003b). It is worthwhile to optimize the calculation of these probabilities as they may be computed millions of times in the course of simulating a single trial. Techniques such as memoization (Orwant 2002) can eliminate redundant calculations of such probabilities over a simulation but the need for a large number of evaluations remains. After considering how to compute P(X > Y) in general, we present optimizations for important special cases in which X and Y both belong to one of the following families of classical distributions: exponential, gamma, inverse gamma, normal, Cauchy, beta, and Weibull."}, {"tag": "p", "text": "John D. Cook and Ralph E. Showalter. Microstructure Diffusion Models with Secondary Flux (1995) Journal of Mathematical Analysis 1995, pp 731–756"}, {"tag": "p", "text": "Abstract. Totally fissured media in which the cells are isolated by the fissure system are effectively described by double porosity models with microstructure. These models contain the geometry of the individual cells or pores in the medium and the flux across their interface with the fissures which surround them. We extend these models to include the case of partially fissured media in which a secondary flux effect arises from cell-to-cell diffusion paths. These quasi-linear problems are formulated in appropriate spaced for which the cells respond to the local linearization of the fissure pressure. It is shown that they are well-posed and the solutions depend continuously on parameters that determine the models."}, {"tag": "p", "text": "John D. Cook and Ralph E. Showalter. Distributed Systems of PDE in Hilbert Space (1993) Differential and Integral Equations, Vol 6 No 5, Sept 1993, pp. 981–994"}, {"tag": "p", "text": "Abstract. We present a system of two nonlinear evolution equations and a corresponding approximating system which provide a common framework for studying distributed microstructure models and a variety of other models for transport and diffusion in heterogeneous media. Existence and uniqueness are demonstrated using semigroup methods, and solutions to the approximating system are shown to converge strongly to the solution of the limiting system. In the microstructure case, new results are obtained, and additional PDE examples are provided to show that in general, certain hypotheses cannot be removed."}, {"tag": "p", "text": "John D. Cook. A Stefan Problem on a Region and its Boundary (1993) Applicable Analysis, Vol 57 No 3–4 (95) pp 367–381"}, {"tag": "p", "text": "Abstract. This paper considers a system of equations consisting of a nonlinear evolution equation on an open set Ω in Rn coupled to another nonlinear evolution equation on the boundary ∂Ω. Rather general assumptions are made concerning the operators involved and the coupling between the two problems. Existence and uniqueness are demonstrated via a semigroup of nonlinear operators on L1(Ω) x L1(∂ Ω)."}, {"tag": "p", "text": "John D. Cook. Separation of convex sets in linear topological spaces (1988)"}, {"tag": "p", "text": "Abstract. This paper discusses under what conditions two disjoint convex subsets of a linear topological space can be separated by a continuous linear functional. The equivalence of several forms of the Hahn-Banach theorem is proven. The separation problem is considered in linear topological spaces, locally convex linear topological spaces, Banach spaces, and finally finite dimensional Banach spaces. A number of examples are included to show the necessity of the hypotheses of various theorems."}, {"tag": "li", "text": "Home"}, {"tag": "p", "text": "jdc"}], "content": "Journal articles and technical reports\n\nThis page lists academic articles. See also informal articles on software development and math and statistics.\n\nContact info\n\nJohn D. Cook, Robert Primmer, Ab de Kwant. Comparing cost and performance of replication in erasure coding. Hitachi Review, vol 63 (July 2014).\n\nAbstract. Data storage systems are more reliable than their individual components. In order to build highly reliable systems out of less reliable parts, systems introduce redundancy. In replicated systems, objects are simply copied several times with each copy residing on a different physical device. While such an approach is simple and direct, more elaborate approaches such as erasure coding can achieve equivalent levels of data protection while using less redundancy. This report examines the trade-offs in cost and performance between replicated and erasure encoded storage systems.\n\nJohn D. Cook Approximating random inequalities with Edgeworth expansions (2012). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 78.\n\nAbstract. Random inequalities of the form Prob (X > Y + δ) often appear as part of Bayesian clinical trial methods. Simulating trial designs could require calculating millions of random inequalities. When these inequalities require numerical integration, or worse random sampling, the inequality calculations account for the large majority of the simulation time. In this paper we show how to approximate random inequalities using Edgeworth expansions. The calculations required to use these expansions can be done in closed form, as we will see below. Although the calculations are elementary, they are also somewhat tedious, and so we include Python code to illustrate how to use the approximations in practice. We make no distributional assumptions on the random variables X and Y other than requiring that the necessary moments exist. The accuracy of the approximation will depend on how well the densities of these random variables are approximated by the Edgeworth expansions.\n\nJohn D. Cook Fast approximation of gamma inequalities (2012). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 77.\n\nAbstract. Approximation for computing P(X > Y + δ) for independent gamma random variables X and Y.\n\nJohn D. Cook Fast approximation of beta inequalities (2012). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 76.\n\nAbstract. Approximation for computing P(X > Y + δ) for independent beta random variables X and Y.\n\nJohn D. Cook CRM: Prior means and medians (2012). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 73.\n\nAbstract. Resolving the confusion between two ways of specifying a CRM dose-finding trial design.\n\nJohn D. Cook Random inequalities between survival and uniform distributions (2011). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 71.\n\nAbstract. This note will look at ways of computing P(X>Y) where X is a distribution modeling survival (gamma, inverse gamma, Weibull, log-normal) and Y has a uniform distribution. Each of these can be computer in closed form in terms of common statistical functions. We begin with analytical calculations and then include software implementations in R to make some of the details more explicit. Finally, we give a suggestion for using simulation to compute random inequalities that cannot be computed in closed form.\n\nJohn D. Cook Basic properties of the soft maximum (2011). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 70.\n\nAbstract. This note presents the basic properties of the soft maximum, a smooth approximation to the maximum of two real variables. It concludes by looking at potential numerical difficulties with the soft maximum and how to avoid these difficulties.\n\nJohn D. Cook, Jairo Fúquene, Luis Pericchi. Skeptical and optimistic robust priors for clinical trials, Revista Columbiana de Estadistica, (2011) 34 no. 2, pp. 333–345.\n\nAbstract. A useful technique from the subjective Bayesian viewpoint, suggested by Spiegelhalter et al. (1994), is to ask the subject matter researchers and other parties involved, such as pharmaceutical companies and regulatory bodies, for reasonable optimistic and pessimistic priors regarding the effectiveness of a new treatment. Up to now, the proposed skeptical and optimistic priors have been limited to conjugate priors, though there is no need for this limitation. The same reasonably adversarial points of view can be taken with robust priors. A recent reference with robust priors usefully applied to clinical trials is in Fuquene, Cook, and Pericchi (2009). Our proposal in this paper is to use Cauchy and intrinsic robust priors for both skeptical and optimistic priors leading to results more closely related with the sampling data when prior and data are in conflict. In other words, the use of robust priors removes the dogmatism implicit in conjugate priors.\n\nJohn D. Cook. Block Adaptive Randomization (2011). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 63.\n\nAbstract. This note proposes a block-adaptive randomization method to limit the length of runs in an outcome-adaptive randomized trial.\n\nJohn D. Cook Upper bounds on non-central chi-squared tails and truncated normal moments (2010). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 62.\n\nAbstract. We show that moments of the truncated normal distribution provide upper bounds on the tails of the non-central chi-squared distribution, then develop upper bounds for the former.\n\nJohn D. Cook Asymptotic results for Normal-Cauchy model (2010). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 61.\n\nAbstract. This report proves asymptotic results for the posterior mean when sampling from a normal distribution with a Cauchy prior on the location parameter.\n\nJohn D. Cook Determining distribution parameters from quantiles\n\nAbstract. Bayesian statistics often requires eliciting prior probabilities from subject matter experts who are unfamiliar with statistics. While most people an intuitive understanding of the mean of a probability distribution, fewer people understand variance as well, particularly in the context of asymmetric distributions. Prior beliefs may be more accurately captured by asking experts for quantiles rather than for means and variances. This note will explain how to solve for parameters so that common distributions satisfy two quantile conditions. We present algorithms for computing these parameters and point to corresponding software. The distributions discussed are normal, log normal, Cauchy, Weibull, gamma, and inverse gamma. The method given for the normal and Cauchy distributions applies more generally to any location-scale family.\n\nJohn D. Cook. Exact calculation of inequality probabilities (2009). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 54.\n\nAbstract. This note surveys results for computing the inequality probability P(X > Y) in closed form where X and Y are independent continuous random variables. Distribution families discussed include normal, Cauchy, gamma, inverse gamma, Levy, folded normal, and beta. Mixture distributions are also discussed.\n\nJairo A. Fúquene P., John D. Cook, Luis Raúl Pericchi. A Case for Robust Bayesian priors with Applications to Binary Clinical Trials. Bayesian Analysis (2009) 4, Number 4, pp. 817–846.\n\nAbstract. Bayesian analysis is frequently limited to conjugate Bayesian analysis, particularly in the case in the analysis of clinical trial data. Even though conjugate analysis may be simpler computationally, the price to be paid is high: such analysis is not robust with respect to the prior, i.e., changing the prior may affect the conclusions without bound. Furthermore conjugate Bayesian analysis is blind with respect to the potential conflict between the prior and the data. On the other hand, robust priors have bounded influence. The prior is discounted automatically when there are conflicts between prior information and data. The original proposal of robust priors was made by de-Finetti in the 1960's. However, the practice has not taken hold in important areas such as in clinical trials where conjugate priors are ubiquitous.\n\nWe show here how the Bayesian analysis for simple binary binomial data, after expressing in its exponentially family form, is improved by employing Cauchy priors. Moreover, we also introduce in the analysis of clinical trials a robust prior originally developed by J.O. Berger that gives closed-form results when coupled with a normal log-odds likelihood. Berger's prior yields the superior robust analysis with no added computational complication compared to the conjugate analysis. We illustrate the results with famous textbook examples and a with data set and a prior from a previous trial.\n\nOn the formal side, we give here a theorem that we call the “Polynomial Tails Comparison Theorem.” This theorem establishes the analytical behavior of any likelihood function with tails bounded by a polynomial when used with priors with polynomial-order tails, such as Cauchy or Student's t. The likelihood does not have to be a location family nor exponential family distribution and the conditions of the theorem are easily verifiable. For Berger's prior robustness can be established directly since the exact expressions for posterior moments are known.\n\nJohn D. Cook. Inequality Probabilities for Folded Normal Random Variables (2009). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 52.\n\nAbstract. This note explains how to calculate the probability Pr(|X| > |Y|) for normal random variables X and Y. (A random variable formed by taking the absolute value of a normal random variable is known as a folded normal random variable.) When X and Y have equal variance, a simple expression is obtained. Otherwise the problem is reduced to a well-known problem.\n\nValen E. Johnson, John D. Cook. Bayesian Design of Single-Arm Phase II Clinical Trials with Continuous Monitoring. Clinical Trials 2009; 6(3):217–26. (preprint)\n\nAbstract. Many “Bayesian” clinical trial designs use posterior credible intervals as tools to define stopping boundaries for inferiority, futility, or superiority. However, the thresholds on posterior credible intervals that trigger termination of a trial are determined by frequentist operating characteristics. This practice can result in substantial overlap between the credible intervals associated with, say, stopping a trial for superiority and stopping a trial for inferiority, which severely limits the interpretation of posterior probability statements. In this article, we use formal Bayesian hypothesis tests to design single-arm phase II clinical trials. By using non-local prior densities to define null and alternative models, we obtain exponential convergence of Bayes factors under both null and alternative models. When compared to other commonly used Bayesian and frequentist designs, we show that our method provides better operating characteristics, uses fewer patients per correct decision, and provides more directly interpretable results. We also demonstrate that designs based on Bayesian hypothesis tests eliminates a potential source of bias often associated with Bayesian trial designs.\n\nJohn D. Cook, Luis Raúl Pericchi. Information and Cross-Entropic Approaches, In: Lauretto MS; Pereira CAB; Stern JM (Org.), Bayesian Methods and Maximum Entropy Methods in Science and Engineering 28, Melville: AIP — American Institute of Physics, 2008, v 28, pp 278–285.\n\nAbstract. In a recent working paper, Fúquene, Cook, and Pericchi make a comprehensive proposal putting forward robust, heavy-tailed priors over conjugate, light-tailed priors in Bayesian analysis. The paper focuses particularly on clinical trials, where information from previous trials should be used in an non-dogmatic fashion, suggesting the use of robust priors. Robust priors have bounded influence in the posterior distribution and their influence is inversely related to the conflict between the data in previous and the current trial. Clearly, the likelihood has to be taken into consideration and not only the prior. We explore here a novel proposal based on a cross-entropy measure of comparison between different models, on which the expectations of the log ratio of the evidences of the contender models are taken with respect to each of the different models and then compared. We also compute the expected increase of information within each model. Both criteria seems to justify the use of robust priors.\n\nJohn D. Cook. Exact operating characteristics for single-arm Phase II trials (2008). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 45.\n\nAbstract. Simulation is so widely used in studying the operating characteristics of clinical trials that we may forget that simulation is not always necessary. This note gives an algorithm for computing the operating characteristics of a stopping rule for a single-arm phase II clinical trial exactly.\n\nYisheng Li, Benjamin Bekele, Yuan Ji, and John D. Cook. Dose-Schedule Finding in Phase I/II Clinical Trials Using Bayesian Isotonic Transformation (2008). Statistics in Medicine 27:4895–4913\n\nAbstract. The intent of most phase I oncology trials is to determine the maximum-tolerated dose (MTD) of an experimental treatment. One of the main considerations apart from determining the MTD is determining an appropriate schedule for administration of the treatment. Historically, schedules have been fixed prior to the start of dose finding. Recently, an increasing number of trials have been designed to determine the MTDs during a phase I component and subsequently determine a schedule during a phase II component. In this paper, we propose a Bayesian design for dose-schedule finding by jointly modeling binary toxicity and efficacy outcomes. Assuming the probability of toxicity follows an order constraint between schedules, we apply a Bayesian isotonic transformation approach to estimating the constrained parameters. We select a dose-schedule combination based on the joint posterior distribution of toxicity and efficacy. Using simulation studies for a hypothetical and a practical cancer clinical trial, we demonstrate that the proposed design performs well under different clinical scenarios.\n\nJohn D. Cook. The Effect of Population Drift on Adaptively Randomized Trials (2007). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 39.\n\nAbstract. Adaptively randomized trials aim to treat patients in clinical trials more effectively by increasing the probability of assigning treatments that appear to have a higher probability of response. Studies of adaptive randomization to date have assumed constant probabilities of response on each treatment. This paper examines the effect of response probabilities that change over time due to population drift.\n\nHarry T. Whelan et al, Practical model-based dose-finding in early phase clinical trials: Optimizing tPA dose for treatment of ischemic stroke in children. Stroke. 39 (2008) 2627-2636.\n\nAbstract. A safe and effective tissue plasminogen activator (tPA) dose for childhood stroke has not been established. This paper describes a Bayesian outcome-adaptive method for determining the best dose of an experimental agent, and explains how this method was used to design a dose-finding trial for tPA in childhood acute ischemic stroke (AIS).\n\nJ. Kyle Wathen, Peter F. Thall, John D. Cook, Elihu H. Estey, Accounting for Patient Heterogeneity in Phase II Clinical Trials. Statistics in Medicine. 27 (2008) 2802–2815.\n\nAbstract. Phase II clinical trials typically are single-arm studies conducted to decide whether an experimental treatment is sufficiently promising, relative to standard treatment, to warrant further investigation. Many methods exist for conducting phase II trials under the assumption that patients are homogeneous. In the presence of patient heterogeneity, however, these designs are likely to draw incorrect conclusions. We propose a class of model-based Bayesian designs for single-arm phase II trials with a binary or time-to-event outcome and two or more prognostic subgroups. The designs' early stopping rules are subgroup specific and allow the possibility of terminating some subgroups while continuing others, thus providing superior results when compared with designs that ignore treatment-subgroup interactions. Because our formulation requires informative priors on standard treatment parameters and subgroup main effects, and non-informative priors on experimental treatment parameters and treatment-subgroup interactions, we provide an algorithm for computing prior hyperparameter values. A simulation study is presented and the method is illustrated by a chemotherapy trial in acute leukemia.\n\nMarcos de Lima et al, Phase I/II study of gemtuzumab ozogamicin added to fludarabine, melphalan and allogeneic hematopoietic stem cell transplantation for high-risk CD33 positive myeloid leukemias and myelodysplastic syndrome. Leukemia 22 (2008) pp 258–264.\n\nAbstract. We investigated the hypothesis that gemtuzumab ozogamicin (GO), an anti-CD33 immunotoxin would improve the efficacy of fludarabine/melphalan as a preparative regimen for allogeneic hematopoietic stem cell transplantation (HSCT) in a phase I/II trial. Toxicity was defined as grades III–IV organ damage, engraftment failure or death within 30 days. 'Response' was engraftment and remission (CR) on day +30. We sought to determine the GO dose (2, 4 or 6 mg m-2) giving the best trade-off between toxicity and response. All patients were not candidates for myeloablative regimens. Treatment plan: GO (day -12), fludarabine 30 mg m-2 (days -5 to -2), melphalan 140 mg m-2 (day -2) and HSCT (day 0). GVHD prophylaxis was tacrolimus and mini-methotrexate. Diagnoses were AML (n=47), MDS (n=4) or CML (n=1). Median age was 53 years (range, 13–72). All but three patients were not in CR. Donors were related (n=33) or unrelated (n=19). Toxicity and response rates at 4 mg m-2 were 50% (n=4) and 50% (n=4). GO dose was de-escalated to 2 mg m-2: 18% had toxicity (n=8) and 82% responded (n=36). 100-day TRM was 15%; one patient had reversible hepatic VOD. Median follow-up was 37 months. Median event-free and overall survival was 6 and 11 months. GO 2 mg m-2 can be safely added to fludarabine/melphalan, and this regimen merits further evaluation.\n\nJohn D. Cook, Comparing Methods of Tuning Adaptively Randomized Trials (2007). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 32.\n\nAbstract. The simplest Bayesian adaptive randomization scheme is to randomize patients to a treatment with probability equal to the probability p that the treatment is better. We examine three variations on adaptive randomization which are used to compromise between this scheme and equal randomization. The first variation is to apply a power transformation to p to obtain randomization probabilities. The second is to clip p to live within specified lower and upper bounds. The third is to begin the trial with a burn-in period of equal randomization. We illustrate how each approach effects statistical power and the number of patients assigned to each treatment. We conclude with recommendations for designing adaptively randomized clinical trials.\n\nJohn D. Cook, Understanding the Exponential Tuning Parameter in Adaptively Randomized Trials (2006). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 27.\n\nAbstract. We examine the effect of a parameter λ used to calibrate how responsive randomization probabilities are to observed data in an adaptively randomized clinical trial. We define and motivate the parameter λ and demonstrate how varying this parameter effects the operating characteristics of example clinical trial designs.\n\nJohn D. Cook and Saralees Nadarajah. Stochastic Inequality Probabilities for Adaptively Randomized Clinical Trials. Biometrical Journal. 48 (2006) pp 256–365.\n\nAbstract. We examine stochastic inequality probabilities of the form P(X > Y) and P(X > \\max(Y, Z)) where X, Y, and Z are random variables with beta, gamma, or inverse gamma distributions. We discuss the applications of such inequality probabilities to adaptively randomized clinical trials as well as methods for calculating their values.\n\nPeter F. Thall and John D. Cook. Using both efficacy and toxicity for dose-finding. In S. Chevret (ed), Statistical Methods for Dose Finding Experiments. New York: John Wiley & Sons, June 2006.\n\nPeter F. Thall and John D. Cook. Adaptive dose-finding based on efficacy-toxicity trade-offs. Encyclopedia of Biopharmaceutical Statistics, 2nd edition, 2006, Chein-Chung Chow editor.\n\nPeter F. Thall, John D. Cook, and Elihu H. Estey. Adaptive dose selection using efficacy-toxicity trade-offs: illustrations and practical considerations. J Biopharmaceutical Stat. 16: 623–638 (2006)\n\nAbstract. The purpose of this paper is to describe and illustrate an outcome-adaptive Bayesian procedure, proposed by Thall and Cook (2004), for assigning doses of an experimental treatment to successive cohorts of patients. The method uses elicited (efficacy, toxicity) probability pairs to construct a family of trade-off contours that are used to quantify the desirability of each dose. This provides a basis for determining a best dose for each cohort. The method combines the goals of conventional Phase I and Phase II trials, and thus may be called a “Phase I-II” design. We first give a general review of the probability model and dose-finding algorithm. We next describe an application to a trial of a biologic agent for treatment of acute myelogenous leukemia, including a computer simulation study to assess the design's average behavior. To illustrate how the method may work in practice, we present a cohort-by-cohort example of a particular trial. We close with a discussion of some practical issues that may arise during implementation.\n\nJohn D. Cook. Efficacy-toxicity trade-offs based on Lp norms (2006). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 29.\n\nAbstract. This report examines in detail a family of efficacy-toxicity trade-off functions simpler and more general than those originally proposed in [1]. The new trade-off functions are based on distance in Lp norm to the ideal point and were first presented in [2]. We define and illustrate these functions and demonstrate how to compute their parameters based on elicited values.\n\nJ. Kyle Wathen and John D. Cook. Power and bias in adaptively randomized clinical trials (2006). Technical Report UTMDABTR-002-06.\n\nAbstract. This report examines the operating characteristics of adaptively randomized trials relative to equally randomized trials in regard to power and bias. We also examine the number of patients in the trial assigned to the superior treatment. The effects of prior selection, sample size, and patient prognostic factors are investigated for both binary and time-to-event outcomes.\n\nJohn D. Cook. Numerical evaluation of gamma inequalities (2006). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 30.\n\nAbstract. This paper addresses the problem of numerically evaluating the probabilities P(X > Y), P(X > max(Y,Z)), and P(X < min(Y,Z)) where X, Y, and Z are independent gamma or inverse gamma random variables.\n\nJohn D. Cook. Continuous safety monitoring in single-arm, time-to-event trials without software (2005). Technical Report UTMDABTR-006-05.\n\nAbstract. This note concerns trial conduct for one-arm trials that monitor safety by comparing time-to-event outcomes of the experimental treatment to an historical treatment. To date, such trials have been conducted using software which evaluates the stopping rule as the trial progresses. We show that is it possible to pre-calculate the stopping conditions, simplifying trial conduct and opening up new possibilities.\n\nJohn D. Cook. Exact calculation of beta inequalities (2005). Technical Report UTMDABTR-005-05.\n\nAbstract. This paper addresses the problem of evaluating P(X > Y) where X and Y are independent beta random variables. We cast the problem in terms of a hypergeometric function and use hypergeometric identities to calculate the probability in closed form for certain values of the distribution parameters.\n\nPeter F. Thall and John D. Cook. Dose-finding based on efficacy-toxicity trade-offs (2004) Biometrics, 60:684–693.\n\nAbstract. We present an adaptive Bayesian method for dose-finding in phase I/II clinical trials based on trade-offs between the probabilities of treatment efficacy and toxicity. The method accommodates either trinary or bivariate binary outcomes, as well as efficacy probabilities that are potentially non-monotone in dose. Doses are selected for successive patient cohorts based on a set of efficacy-toxicity trade-off contours that partition the two-dimensional outcome probability domain. Priors are established by solving for hyperparameters which optimize the fit of the model to elicited mean outcome probabilities. For trinary outcomes, the new algorithm is compared to the method of Thall and Russell by application to a trial of rapid treatment for ischemic stroke. The bivariate binary outcome case is illustrated by a trial of graft-versus-host disease prophylaxis in allogeneic bone marrow transplantation. Computer simulations show that, under a wide rage of dose-outcome scenarios, the new method has high probabilities of making correct decisions and treats most patients at doses with desirable efficacy-toxicity trade-offs.\n\nJohn D. Cook. Simulation results for phase II clinical trial durations (2004). Technical Report UTMDABTR-014-04\n\nAbstract. This paper investigates the effect of cohort size on phase II clinical trial duration by doing a simulation study of a monitoring method of Thall and Simon. We challenge the assumptions that larger cohort sizes lead to shorter trials and that continuous monitoring is impractical.\n\nJohn D. Cook. Numerical computation of stochastic inequality probabilities (2003). UT MD Anderson Cancer Center Department of Biostatistics Working Paper Series. Working Paper 46.\n\nAbstract. This paper addresses the problem of numerically evaluating P(X > Y) for independent continuous random variables X and Y. This calculation arises in the design of clinical trials and as such appears in the inner loop of simulations of these trials. An early example of this is given in (Thompson 1933). More recent examples are given in (Giles et al 2003), (Berry 2003a, 2003b). It is worthwhile to optimize the calculation of these probabilities as they may be computed millions of times in the course of simulating a single trial. Techniques such as memoization (Orwant 2002) can eliminate redundant calculations of such probabilities over a simulation but the need for a large number of evaluations remains. After considering how to compute P(X > Y) in general, we present optimizations for important special cases in which X and Y both belong to one of the following families of classical distributions: exponential, gamma, inverse gamma, normal, Cauchy, beta, and Weibull.\n\nJohn D. Cook and Ralph E. Showalter. Microstructure Diffusion Models with Secondary Flux (1995) Journal of Mathematical Analysis 1995, pp 731–756\n\nAbstract. Totally fissured media in which the cells are isolated by the fissure system are effectively described by double porosity models with microstructure. These models contain the geometry of the individual cells or pores in the medium and the flux across their interface with the fissures which surround them. We extend these models to include the case of partially fissured media in which a secondary flux effect arises from cell-to-cell diffusion paths. These quasi-linear problems are formulated in appropriate spaced for which the cells respond to the local linearization of the fissure pressure. It is shown that they are well-posed and the solutions depend continuously on parameters that determine the models.\n\nJohn D. Cook and Ralph E. Showalter. Distributed Systems of PDE in Hilbert Space (1993) Differential and Integral Equations, Vol 6 No 5, Sept 1993, pp. 981–994\n\nAbstract. We present a system of two nonlinear evolution equations and a corresponding approximating system which provide a common framework for studying distributed microstructure models and a variety of other models for transport and diffusion in heterogeneous media. Existence and uniqueness are demonstrated using semigroup methods, and solutions to the approximating system are shown to converge strongly to the solution of the limiting system. In the microstructure case, new results are obtained, and additional PDE examples are provided to show that in general, certain hypotheses cannot be removed.\n\nJohn D. Cook. A Stefan Problem on a Region and its Boundary (1993) Applicable Analysis, Vol 57 No 3–4 (95) pp 367–381\n\nAbstract. This paper considers a system of equations consisting of a nonlinear evolution equation on an open set Ω in Rn coupled to another nonlinear evolution equation on the boundary ∂Ω. Rather general assumptions are made concerning the operators involved and the coupling between the two problems. Existence and uniqueness are demonstrated via a semigroup of nonlinear operators on L1(Ω) x L1(∂ Ω).\n\nJohn D. Cook. Separation of convex sets in linear topological spaces (1988)\n\nAbstract. This paper discusses under what conditions two disjoint convex subsets of a linear topological space can be separated by a continuous linear functional. The equivalence of several forms of the Hahn-Banach theorem is proven. The separation problem is considered in linear topological spaces, locally convex linear topological spaces, Banach spaces, and finally finite dimensional Banach spaces. A number of examples are included to show the necessity of the hypotheses of various theorems.\n\nHome\n\njdc"}
{"slug": "asymptotic_notation", "canonical_url": "https://www.johndcook.com/blog/asymptotic_notation/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/asymptotic_notation.html", "title": "Big-O and related notation for asymptotic order", "heading": "Big-O and related notation", "description": "Summary of asymptotic notations: big-O, little-O, theta, omega, omicron", "summary": "This page summarizes \"Big-O\" notation and related notations introduced by Paul Bachmann and Edmund Landau.", "word_count": 515, "blocks": [{"tag": "h1", "text": "Big-O and related notation"}, {"tag": "h2", "text": "Definitions"}, {"tag": "p", "text": "This page summarizes \"Big-O\" notation and related notations introduced by Paul Bachmann and Edmund Landau."}, {"tag": "h3", "text": "Infinite limits"}, {"tag": "p", "text": "The notation f(x) = O( g(x) ) means the function f(x) is eventually bounded by a multiple of g(x). Here \"eventually\" depends on the direction in which one is taking a limit. We focus first on limits going to infinity since that is the most common case."}, {"tag": "p", "text": "More precisely, we say f(x) = O( g(x) ) as x → ∞ if there exist positive constants M and C such that f(x) ≤ C g(x) for all x > M."}, {"tag": "p", "text": "While \"Big O\" notation means \"eventually bounded above by,\" Ω notation means \"eventually bounded below by.\" More precisely, we say f(x) = Ω( g(x) ) as x → ∞ if there exist positive constants M and C such that f(x) ≥ C g(x) for all x > M."}, {"tag": "p", "text": "The notation f(x) = Θ( g(x) ) means that f(x) = O( g(x) ) and f(x) = Ω( g(x) )."}, {"tag": "p", "text": "There are two remaining notations: ο (Greek omicron) and ω (lower-case Greek omega). The omicron notation is also called \"little-o\" notation."}, {"tag": "p", "text": "We say f(x) = ο( g(x) ) if the limit as x → ∞ of f(x)/g(x) is 0."}, {"tag": "p", "text": "We say f(x) = ω( g(x) ) if the same limit is ∞."}, {"tag": "h3", "text": "Finite limits"}, {"tag": "p", "text": "The ideas above remain essentially the same for finite limits, though the technical details of the definitions differ."}, {"tag": "p", "text": "f(x) = O( g(x) ) as x → k if there exist positive constants δ and C such that f(x) ≤ C g(x) for all x such that |x - k| < δ."}, {"tag": "p", "text": "f(x) = Ω( g(x) ) as x → k if there exist positive constants δ and C such that f(x) ≥ C g(x) for all x such that |x - k| < δ."}, {"tag": "p", "text": "f(x) = ο( g(x) ) as x → k if the limit as x → k of f(x)/g(x) is 0."}, {"tag": "p", "text": "f(x) = ω( g(x) ) as x → k if the limit as x → k of f(x)/g(x) is ∞."}, {"tag": "p", "text": "Often you will see statements such as f(x) = O( g(x) ) without reference to an explicit limit and will have to determine from context what limit is implied."}, {"tag": "h2", "text": "Usage"}, {"tag": "p", "text": "Big-O notation is common in computer science and mathematics. However, some of the other notations are common in one field but not in the other."}, {"tag": "p", "text": "In computer science, the emphasis is nearly always on the behavior of an algorithm as the problem size n grows, and so limits are implicitly taken as n goes to infinity. The Ω and Θ notations are more commonly used in computer science than in mathematics. Little-o (omicron) notation is rarely used."}, {"tag": "p", "text": "In mathematics, big-O notation is common with both infinite and finite limits. Little-o notation is the next most popular. The Ω and Θ notations are uncommon."}, {"tag": "p", "text": "The notation f = ω( g(x) ) is not common in computer science or mathematics."}, {"tag": "h2", "text": "Reference"}, {"tag": "p", "text": "For further information, see Introduction to Algorithms (computer science) or Asymptotic Methods in Analysis (mathematics)."}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Big-O and related notation\n\nDefinitions\n\nThis page summarizes \"Big-O\" notation and related notations introduced by Paul Bachmann and Edmund Landau.\n\nInfinite limits\n\nThe notation f(x) = O( g(x) ) means the function f(x) is eventually bounded by a multiple of g(x). Here \"eventually\" depends on the direction in which one is taking a limit. We focus first on limits going to infinity since that is the most common case.\n\nMore precisely, we say f(x) = O( g(x) ) as x → ∞ if there exist positive constants M and C such that f(x) ≤ C g(x) for all x > M.\n\nWhile \"Big O\" notation means \"eventually bounded above by,\" Ω notation means \"eventually bounded below by.\" More precisely, we say f(x) = Ω( g(x) ) as x → ∞ if there exist positive constants M and C such that f(x) ≥ C g(x) for all x > M.\n\nThe notation f(x) = Θ( g(x) ) means that f(x) = O( g(x) ) and f(x) = Ω( g(x) ).\n\nThere are two remaining notations: ο (Greek omicron) and ω (lower-case Greek omega). The omicron notation is also called \"little-o\" notation.\n\nWe say f(x) = ο( g(x) ) if the limit as x → ∞ of f(x)/g(x) is 0.\n\nWe say f(x) = ω( g(x) ) if the same limit is ∞.\n\nFinite limits\n\nThe ideas above remain essentially the same for finite limits, though the technical details of the definitions differ.\n\nf(x) = O( g(x) ) as x → k if there exist positive constants δ and C such that f(x) ≤ C g(x) for all x such that |x - k| < δ.\n\nf(x) = Ω( g(x) ) as x → k if there exist positive constants δ and C such that f(x) ≥ C g(x) for all x such that |x - k| < δ.\n\nf(x) = ο( g(x) ) as x → k if the limit as x → k of f(x)/g(x) is 0.\n\nf(x) = ω( g(x) ) as x → k if the limit as x → k of f(x)/g(x) is ∞.\n\nOften you will see statements such as f(x) = O( g(x) ) without reference to an explicit limit and will have to determine from context what limit is implied.\n\nUsage\n\nBig-O notation is common in computer science and mathematics. However, some of the other notations are common in one field but not in the other.\n\nIn computer science, the emphasis is nearly always on the behavior of an algorithm as the problem size n grows, and so limits are implicitly taken as n goes to infinity. The Ω and Θ notations are more commonly used in computer science than in mathematics. Little-o (omicron) notation is rarely used.\n\nIn mathematics, big-O notation is common with both infinite and finite limits. Little-o notation is the next most popular. The Ω and Θ notations are uncommon.\n\nThe notation f = ω( g(x) ) is not common in computer science or mathematics.\n\nReference\n\nFor further information, see Introduction to Algorithms (computer science) or Asymptotic Methods in Analysis (mathematics).\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "bayesian-consulting", "canonical_url": "https://www.johndcook.com/blog/bayesian-consulting/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/bayesian_statistics.html", "title": "Bayesian statistics", "heading": "Bayesian statistics", "description": "Bayesian statistics: John D. Cook experience.", "summary": "Bayesian statistics draws inferences from data in a way that makes sense to ordinary humans. It directly addresses the questions people naturally ask, rather than inverting the questions for technical convenience.", "word_count": 212, "blocks": [{"tag": "h1", "text": "Bayesian statistics"}, {"tag": "p", "text": "Bayesian statistics draws inferences from data in a way that makes sense to ordinary humans. It directly addresses the questions people naturally ask, rather than inverting the questions for technical convenience."}, {"tag": "p", "text": "Bayesian inference allows you to bring together all sources of information, subjective and objective. You can combine expert opinion or intuition with data, weighing each in the proportion appropriate for your situation. The weight given different kinds of information automatically adjusts according to the quantity and quality of each."}, {"tag": "p", "text": "Because of this ability to adjust to new information, Bayesian methods naturally lend themselves to adaptive decision making, updating the representation of your knowledge of the world as new data become available."}, {"tag": "p", "text": "For over a decade I applied Bayesian methods to cancer treatment, using these methods to design, simulate, and conduct adaptive clinical trials. Now I am applying these methods to business opportunities. I have extensive experience in Bayesian analysis and especially in the computational techniques necessary to make Bayesian methods practical."}, {"tag": "p", "text": "You can see some of the journal articles and technical reports I have written in Bayesian statistics here and some of the software I have written here."}, {"tag": "p", "text": "If you would like for me to help your company with Bayesian statistics, please let me know."}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Bayesian statistics\n\nBayesian statistics draws inferences from data in a way that makes sense to ordinary humans. It directly addresses the questions people naturally ask, rather than inverting the questions for technical convenience.\n\nBayesian inference allows you to bring together all sources of information, subjective and objective. You can combine expert opinion or intuition with data, weighing each in the proportion appropriate for your situation. The weight given different kinds of information automatically adjusts according to the quantity and quality of each.\n\nBecause of this ability to adjust to new information, Bayesian methods naturally lend themselves to adaptive decision making, updating the representation of your knowledge of the world as new data become available.\n\nFor over a decade I applied Bayesian methods to cancer treatment, using these methods to design, simulate, and conduct adaptive clinical trials. Now I am applying these methods to business opportunities. I have extensive experience in Bayesian analysis and especially in the computational techniques necessary to make Bayesian methods practical.\n\nYou can see some of the journal articles and technical reports I have written in Bayesian statistics here and some of the software I have written here.\n\nIf you would like for me to help your company with Bayesian statistics, please let me know.\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "berry_esseen_poisson", "canonical_url": "https://www.johndcook.com/blog/berry_esseen_poisson/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/berry_esseen_poisson.html", "title": "Error in the normal approximation to the Poisson distribution", "heading": "Details for error bound on normal approximation to the Poisson distribution", "description": "Details on using the Berry-Esseen theorem to bound the error in the normal approximation to the Poisson distribution.", "summary": "This page is an appendix to the page Error in the normal approximation to the Poisson distribution.", "word_count": 225, "blocks": [{"tag": "h1", "text": "Details for error bound on normal approximation to the Poisson distribution"}, {"tag": "p", "text": "This page is an appendix to the page Error in the normal approximation to the Poisson distribution."}, {"tag": "p", "text": "The previous page noted that a Poisson random variable with mean λ has the same distribution as the sum of N independent Poisson random variables Xi each with mean λ/N. The Berry-Esséen theorem says that the error in approximating the CDF of ∑Xi with the CDF of its normal approximation is uniformly bounded by C ρ/σ3√N where C is a constant we discuss below, ρ = E(|Xi - λ/N|3) and σ, the standard deviation of Xi, equals (λ/N)1/2."}, {"tag": "p", "text": "Since the number N is arbitrary, we pick it as large as we like. This paper says the constant in the Berry-Esséen theorem can be made less than 0.7164 if N ≥ 65. This is no problem here because we will be taking the limit as N goes to infinity."}, {"tag": "p", "text": "Next we calculate ρ, the third absolute moment of Xi. Let ω = λ/N. We can get an upper bound on ρ as follows."}, {"tag": "p", "text": "From this we can conclude that ρ/σ3√N equals λ plus term involving 1/N. As we let N to to infinity, the latter terms drop out and so we have the error bound on the normal approximation to the Poisson of C/√λ where C < 0.7164."}], "content": "Details for error bound on normal approximation to the Poisson distribution\n\nThis page is an appendix to the page Error in the normal approximation to the Poisson distribution.\n\nThe previous page noted that a Poisson random variable with mean λ has the same distribution as the sum of N independent Poisson random variables Xi each with mean λ/N. The Berry-Esséen theorem says that the error in approximating the CDF of ∑Xi with the CDF of its normal approximation is uniformly bounded by C ρ/σ3√N where C is a constant we discuss below, ρ = E(|Xi - λ/N|3) and σ, the standard deviation of Xi, equals (λ/N)1/2.\n\nSince the number N is arbitrary, we pick it as large as we like. This paper says the constant in the Berry-Esséen theorem can be made less than 0.7164 if N ≥ 65. This is no problem here because we will be taking the limit as N goes to infinity.\n\nNext we calculate ρ, the third absolute moment of Xi. Let ω = λ/N. We can get an upper bound on ρ as follows.\n\nFrom this we can conclude that ρ/σ3√N equals λ plus term involving 1/N. As we let N to to infinity, the latter terms drop out and so we have the error bound on the normal approximation to the Poisson of C/√λ where C < 0.7164."}
{"slug": "bias_consistency", "canonical_url": "https://www.johndcook.com/blog/bias_consistency/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/bias_consistency.html", "title": "The difference between unbiased estimators and consistent estimators", "heading": "The difference between an unbiased estimator and a consistent estimator", "description": "Explaining and illustrating the difference between an unbiased estimator and a consistent estimator", "summary": "Sometimes code is easier to understand than prose. Here I presented a Python script that illustrates the difference between an unbiased estimator and a consistent estimator.", "word_count": 612, "blocks": [{"tag": "h1", "text": "The difference between an unbiased estimator and a consistent estimator"}, {"tag": "p", "text": "Sometimes code is easier to understand than prose. Here I presented a Python script that illustrates the difference between an unbiased estimator and a consistent estimator."}, {"tag": "p", "text": "Here are a couple ways to estimate the variance of a sample. The maximum likelihood estimate (MLE) is"}, {"tag": "p", "text": "where x with a bar on top is the average of the x's. The unbiased estimate is"}, {"tag": "p", "text": "Our code will generate samples from a normal distribution with mean 3 and variance 49. Both of the estimators above are consistent in the sense that as n, the number of samples, gets large, the estimated values get close to 49 with high probability. This is illustrated in the following graph. The horizontal line is at the expected value, 49."}, {"tag": "p", "text": "The code below takes samples of size n=10 and estimates the variance both ways. It does this N times and average the estimates. If an estimator is unbiased, these averages should be close to 49 for large values of N. Think of N going to infinity while n is small and fixed. Note that the sample size is not increasing: each estimate is based on only 10 samples. However, we are averaging a lot of such estimates."}, {"tag": "p", "text": "The average of the unbiased estimates is good. But how good are the individual estimates?"}, {"tag": "p", "text": "Just because the value of the estimates averages to the correct value, that does not mean that individual estimates are good. It is possible for an unbiased estimator to give a sequence ridiculous estimates that nevertheless converge on average to an accurate value. (For an example, see this article.) So we also look at the efficiency, how the variance estimates bounce around 49, measured by mean squared error (MSE)."}, {"tag": "p", "text": "The MSE for the unbiased estimator appears to be around 528 and the MSE for the biased estimator appears to be around 457. In this particular example, the MSEs can be calculated analytically. The MSE for the unbiased estimator is 533.55 and the MSE for the biased estimator is 456.19."}, {"tag": "pre", "text": "import random \n\nmu = 3\nsigma = 7\n\nrandom.seed(1)\n\ndef mean(sample):\n    sum = 0.0\n    for x in sample:\n        sum += x\n    return sum/len(sample)\n\ndef square(x):\n    return x*x    \n\ndef sum_sq_diff(sample):\n    avg = mean(sample)\n    sum = 0.0\n    for x in sample:\n        sum += square(x - avg)\n    return sum\n    \ndef average_estimate(samples_per_estimate, num_estimates, biased):\n    average = 0.0\n    for i in xrange(num_estimates):\n        sample = [random.normalvariate(mu, sigma) for j in xrange(samples_per_estimate)]\n        estimate = sum_sq_diff(sample)\n        if biased:\n            estimate /= samples_per_estimate\n        else:\n            estimate /= samples_per_estimate - 1\n        average += estimate\n    average /= num_estimates\n    return average\n\ndef mean_squared_error(samples_per_estimate, num_estimates, biased):\n    average = 0.0\n    for i in xrange(num_estimates):\n        sample = [random.normalvariate(mu, sigma) for j in xrange(samples_per_estimate)]\n        estimate = sum_sq_diff(sample)\n        if biased:\n            estimate /= samples_per_estimate\n        else:\n            estimate /= samples_per_estimate - 1\n        average += square( estimate - sigma*sigma )\n    average /= num_estimates\n    return average\n\n# Show consistency\nsample_sizes = [2, 10, 100, 1000, 10000, 100000]\nfor size in sample_sizes:\n    print \"variance estimate biased:   %f, sample size: %d\" % \\\n        (average_estimate(size, 1, True), size)\n\nprint\n\nfor size in sample_sizes:\n    print \"variance estimate unbiased: %f, sample size: %d\" % \\\n        (average_estimate(size, 1, False), size)\n    \nprint\n    \n# Show bias\nnum_estimates_to_average = [1, 10, 100, 1000, 10000, 100000]\nfixed_sample_size = 10\nfor estimates in num_estimates_to_average:\n    print \"average biased estimate: %f, num estimates: %d\" % \\\n        (average_estimate(fixed_sample_size, estimates, True), estimates)\n\nprint\n\nfor estimates in num_estimates_to_average:\n    print \"average unbiased estimate: %f, num estimates: %d\" % \\\n        (average_estimate(fixed_sample_size, estimates, False), estimates)\n    \nprint     \n    \n# Show efficiency\nfor estimates in num_estimates_to_average:\n    print \"MSE biased:   %f, sample size: %d\" % \n        (mean_squared_error(fixed_sample_size, estimates, True), size)\n    \nprint \n\nfor estimates in num_estimates_to_average:\n    print \"MSE unbiased: %f, sample size: %d\" % \n        (mean_squared_error(fixed_sample_size, estimates, False), size)\n    \nprint"}], "content": "The difference between an unbiased estimator and a consistent estimator\n\nSometimes code is easier to understand than prose. Here I presented a Python script that illustrates the difference between an unbiased estimator and a consistent estimator.\n\nHere are a couple ways to estimate the variance of a sample. The maximum likelihood estimate (MLE) is\n\nwhere x with a bar on top is the average of the x's. The unbiased estimate is\n\nOur code will generate samples from a normal distribution with mean 3 and variance 49. Both of the estimators above are consistent in the sense that as n, the number of samples, gets large, the estimated values get close to 49 with high probability. This is illustrated in the following graph. The horizontal line is at the expected value, 49.\n\nThe code below takes samples of size n=10 and estimates the variance both ways. It does this N times and average the estimates. If an estimator is unbiased, these averages should be close to 49 for large values of N. Think of N going to infinity while n is small and fixed. Note that the sample size is not increasing: each estimate is based on only 10 samples. However, we are averaging a lot of such estimates.\n\nThe average of the unbiased estimates is good. But how good are the individual estimates?\n\nJust because the value of the estimates averages to the correct value, that does not mean that individual estimates are good. It is possible for an unbiased estimator to give a sequence ridiculous estimates that nevertheless converge on average to an accurate value. (For an example, see this article.) So we also look at the efficiency, how the variance estimates bounce around 49, measured by mean squared error (MSE).\n\nThe MSE for the unbiased estimator appears to be around 528 and the MSE for the biased estimator appears to be around 457. In this particular example, the MSEs can be calculated analytically. The MSE for the unbiased estimator is 533.55 and the MSE for the biased estimator is 456.19.\n\nimport random \n\nmu = 3\nsigma = 7\n\nrandom.seed(1)\n\ndef mean(sample):\n    sum = 0.0\n    for x in sample:\n        sum += x\n    return sum/len(sample)\n\ndef square(x):\n    return x*x    \n\ndef sum_sq_diff(sample):\n    avg = mean(sample)\n    sum = 0.0\n    for x in sample:\n        sum += square(x - avg)\n    return sum\n    \ndef average_estimate(samples_per_estimate, num_estimates, biased):\n    average = 0.0\n    for i in xrange(num_estimates):\n        sample = [random.normalvariate(mu, sigma) for j in xrange(samples_per_estimate)]\n        estimate = sum_sq_diff(sample)\n        if biased:\n            estimate /= samples_per_estimate\n        else:\n            estimate /= samples_per_estimate - 1\n        average += estimate\n    average /= num_estimates\n    return average\n\ndef mean_squared_error(samples_per_estimate, num_estimates, biased):\n    average = 0.0\n    for i in xrange(num_estimates):\n        sample = [random.normalvariate(mu, sigma) for j in xrange(samples_per_estimate)]\n        estimate = sum_sq_diff(sample)\n        if biased:\n            estimate /= samples_per_estimate\n        else:\n            estimate /= samples_per_estimate - 1\n        average += square( estimate - sigma*sigma )\n    average /= num_estimates\n    return average\n\n# Show consistency\nsample_sizes = [2, 10, 100, 1000, 10000, 100000]\nfor size in sample_sizes:\n    print \"variance estimate biased:   %f, sample size: %d\" % \\\n        (average_estimate(size, 1, True), size)\n\nprint\n\nfor size in sample_sizes:\n    print \"variance estimate unbiased: %f, sample size: %d\" % \\\n        (average_estimate(size, 1, False), size)\n    \nprint\n    \n# Show bias\nnum_estimates_to_average = [1, 10, 100, 1000, 10000, 100000]\nfixed_sample_size = 10\nfor estimates in num_estimates_to_average:\n    print \"average biased estimate: %f, num estimates: %d\" % \\\n        (average_estimate(fixed_sample_size, estimates, True), estimates)\n\nprint\n\nfor estimates in num_estimates_to_average:\n    print \"average unbiased estimate: %f, num estimates: %d\" % \\\n        (average_estimate(fixed_sample_size, estimates, False), estimates)\n    \nprint     \n    \n# Show efficiency\nfor estimates in num_estimates_to_average:\n    print \"MSE biased:   %f, sample size: %d\" % \n        (mean_squared_error(fixed_sample_size, estimates, True), size)\n    \nprint \n\nfor estimates in num_estimates_to_average:\n    print \"MSE unbiased: %f, sample size: %d\" % \n        (mean_squared_error(fixed_sample_size, estimates, False), size)\n    \nprint"}
{"slug": "binomial_coefficients", "canonical_url": "https://www.johndcook.com/blog/binomial_coefficients/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/binomial_coefficients.html", "title": "Binomial coefficients", "heading": "Binomial coefficients", "description": "This page carefully presents more general definitions of binomial coefficients and explains why these generalizations are useful.", "summary": "This page gives three definitions of binomial coefficients in order of increasing generality.", "word_count": 687, "blocks": [{"tag": "h1", "text": "Binomial coefficients"}, {"tag": "p", "text": "This page gives three definitions of binomial coefficients in order of increasing generality."}, {"tag": "h2", "text": "Definition 1: most basic"}, {"tag": "p", "text": "The most common definition of binomial coefficients is not the most useful or the most general. Most sources define the binomial coefficient (n, k) as"}, {"tag": "p", "text": "for where n is a positive integer and 0 ≤ k ≤ n. Call this Definition 1. It turns out to be useful to replace the above definition with one that allows n to be any real number and allows k to be any integer."}, {"tag": "h2", "text": "Definition 2: more general"}, {"tag": "p", "text": "The book Concrete Mathematics defines binomial coefficients as follows. First, define the kth falling power of r as"}, {"tag": "p", "text": "The kth falling power of r multiplies starts with r and multiplies together k numbers, each being one less than its predecessor. Given this notation, define the binomial coefficients as"}, {"tag": "p", "text": "Call this Definition 2. Note the use of \"r\" rather than \"n\" to remind us that the top number can be any real number, not necessarily an integer. When Definition 1 and Definition 2 both hold, they give the same result. When only Definition 2 holds, the value can no longer be interpreted combinatorially. That is, it doesn't make sense to think of \"r things taken k at a time\" or \"choosing k things from a set of r items\" when r is not an integer or when k is negative."}, {"tag": "p", "text": "So why the new definition? Here are four reasons."}, {"tag": "li", "text": "Many theorems can be stated more simply in terms of Definition 2. There is less need to include qualifications in theorem statements. For example, the cases k ≤ n and k > n can often be handled uniformly."}, {"tag": "li", "text": "Definition 2 turns out to be the correct generalization to allow many theorems to extend to more general arguments. For example, the binomial theorem extends to general exponents when binomial coefficients are defined this way."}, {"tag": "li", "text": "Sums can often be expressed as summations over all integers; the terms that you'd otherwise need to explicitly exclude turn out to be zero. This simplifies formulas and reduces errors."}, {"tag": "li", "text": "The binomial coefficient (r, k) is a kth degree polynomial in r, which turns out to be very handy in proofs. It's often possible to show that two binomial coefficients are equal by showing that the corresponding polynomials agree at k+1 points and hence must be equal everywhere."}, {"tag": "p", "text": "The choose(r, k) function in R implements this definition."}, {"tag": "h2", "text": "Definition 3: most general"}, {"tag": "p", "text": "Definition 2 is adequate for many applications, but there is a still more general definition. For any complex numbers z and w, the binomial coefficient (z, w) can be defined as follows."}, {"tag": "p", "text": "Call this Definition 3. Note that the order of the limits is important. More on that below."}, {"tag": "p", "text": "If we define z! for a complex number to be Γ(z+1) then Definition 3 looks like Definition 1, except for the limits. What's up with the limits? They are necessary to properly handle the cases where z or w are singularities of the Γ function. It is not obvious that Definition 3 gives the same values as Definition 2 when both definitions apply, but we will show that this is the case."}, {"tag": "p", "text": "The singularities of the Γ function occur at 0, -1, -2, etc. and so the limit in Definition 3 is non-trivial when z or w are negative integers. If w is a negative integer, the limit is zero because the denominator goes to ∞. We do not need to be concerned about whether z is also a negative integer because the limit in v is evaluated before the limit in u."}, {"tag": "p", "text": "If w is not an integer but z is a negative integer, the binomial coefficient is infinite because the denominator is bounded but the numerator goes to ∞. This does not contradict Definition 2 because that definition only allows integer values of w."}, {"tag": "p", "text": "Suppose w is a non-negative integer. We show that Definition 3 gives the same value as Definition 2. First we apply the reflection identity"}, {"tag": "p", "text": "to Γ(u+1) and Γ(u-v+1). Then"}, {"tag": "p", "text": "The Binomial[r, k] function in Mathematica implements this definition."}, {"tag": "p", "text": "See also Computing binomial coefficients."}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Binomial coefficients\n\nThis page gives three definitions of binomial coefficients in order of increasing generality.\n\nDefinition 1: most basic\n\nThe most common definition of binomial coefficients is not the most useful or the most general. Most sources define the binomial coefficient (n, k) as\n\nfor where n is a positive integer and 0 ≤ k ≤ n. Call this Definition 1. It turns out to be useful to replace the above definition with one that allows n to be any real number and allows k to be any integer.\n\nDefinition 2: more general\n\nThe book Concrete Mathematics defines binomial coefficients as follows. First, define the kth falling power of r as\n\nThe kth falling power of r multiplies starts with r and multiplies together k numbers, each being one less than its predecessor. Given this notation, define the binomial coefficients as\n\nCall this Definition 2. Note the use of \"r\" rather than \"n\" to remind us that the top number can be any real number, not necessarily an integer. When Definition 1 and Definition 2 both hold, they give the same result. When only Definition 2 holds, the value can no longer be interpreted combinatorially. That is, it doesn't make sense to think of \"r things taken k at a time\" or \"choosing k things from a set of r items\" when r is not an integer or when k is negative.\n\nSo why the new definition? Here are four reasons.\n\nMany theorems can be stated more simply in terms of Definition 2. There is less need to include qualifications in theorem statements. For example, the cases k ≤ n and k > n can often be handled uniformly.\n\nDefinition 2 turns out to be the correct generalization to allow many theorems to extend to more general arguments. For example, the binomial theorem extends to general exponents when binomial coefficients are defined this way.\n\nSums can often be expressed as summations over all integers; the terms that you'd otherwise need to explicitly exclude turn out to be zero. This simplifies formulas and reduces errors.\n\nThe binomial coefficient (r, k) is a kth degree polynomial in r, which turns out to be very handy in proofs. It's often possible to show that two binomial coefficients are equal by showing that the corresponding polynomials agree at k+1 points and hence must be equal everywhere.\n\nThe choose(r, k) function in R implements this definition.\n\nDefinition 3: most general\n\nDefinition 2 is adequate for many applications, but there is a still more general definition. For any complex numbers z and w, the binomial coefficient (z, w) can be defined as follows.\n\nCall this Definition 3. Note that the order of the limits is important. More on that below.\n\nIf we define z! for a complex number to be Γ(z+1) then Definition 3 looks like Definition 1, except for the limits. What's up with the limits? They are necessary to properly handle the cases where z or w are singularities of the Γ function. It is not obvious that Definition 3 gives the same values as Definition 2 when both definitions apply, but we will show that this is the case.\n\nThe singularities of the Γ function occur at 0, -1, -2, etc. and so the limit in Definition 3 is non-trivial when z or w are negative integers. If w is a negative integer, the limit is zero because the denominator goes to ∞. We do not need to be concerned about whether z is also a negative integer because the limit in v is evaluated before the limit in u.\n\nIf w is not an integer but z is a negative integer, the binomial coefficient is infinite because the denominator is bounded but the numerator goes to ∞. This does not contradict Definition 2 because that definition only allows integer values of w.\n\nSuppose w is a non-negative integer. We show that Definition 3 gives the same value as Definition 2. First we apply the reflection identity\n\nto Γ(u+1) and Γ(u-v+1). Then\n\nThe Binomial[r, k] function in Mathematica implements this definition.\n\nSee also Computing binomial coefficients.\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "book_reviews", "canonical_url": "https://www.johndcook.com/blog/book_reviews/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/book_reviews.html", "title": "Book reviews", "heading": "Book reviews", "description": "MAA online book reviews.", "summary": "This page lists book I have reviewed for MAA Reviews.", "word_count": 232, "blocks": [{"tag": "h1", "text": "Book reviews"}, {"tag": "p", "text": "This page lists book I have reviewed for MAA Reviews."}, {"tag": "p", "text": "Variational Methods: Applications to Nonlinear Partial Differential Equations and Hamiltonian Systems by Michael Struwe"}, {"tag": "p", "text": "Introduction to Applied Optimization by Urmila Diwekar"}, {"tag": "p", "text": "An Atlas of Functions by Keith Oldham, Jan Myland, and Jerome Spanier"}, {"tag": "p", "text": "A Course in Approximation Theory by Ward Cheney and Will Light"}, {"tag": "p", "text": "Complexity: A Guided Tour by Melanie Mitchell"}, {"tag": "p", "text": "Nine Introductions in Complex Analysis by Sanford Segal"}, {"tag": "p", "text": "Closer and Closer: Introducing Real Analysis by Carol Schumacher"}, {"tag": "p", "text": "Combinatorics of Genome Rearrangements by Guillaume Fertin et al."}, {"tag": "p", "text": "Linear Algebra: Theory and Applications by Ward Cheney and David Kincaid"}, {"tag": "p", "text": "The Mathematical Mechanic by Mark Levi"}, {"tag": "p", "text": "Explorations in Monte Carlo Methods by Ronald Shonkwiler and Franklin Mendivil"}, {"tag": "p", "text": "Not Always Buried Deep: A Second Course in Elementary Number Theory by Paul Pollack"}, {"tag": "p", "text": "Introduction of Probability with Mathematica by Kevin Hastings"}, {"tag": "p", "text": "The Life and Times of the Central Limit Theorem by William J. Adams"}, {"tag": "p", "text": "Linear Systems Theory by João Hespanha"}, {"tag": "p", "text": "Scientific Computation by Gaston Gonnet and Ralf Scholl"}, {"tag": "p", "text": "The Pleasures of Statistics: The Autobiography of Frederick Mosteller"}, {"tag": "p", "text": "Convex Functions by Jonathan M. Borwein and Jon D. Vanderwerff"}, {"tag": "p", "text": "Partial Differential Equations by Lawrence C. Evans"}, {"tag": "p", "text": "Remarkable Engineers by Ioan James"}, {"tag": "p", "text": "Second Order Differential Equations: Special Functions and their Classification by Gerhard Kristensson"}, {"tag": "p", "text": "I also maintain a page listing books I've mentioned on my blog along with references to the posts where the books were mentioned."}], "content": "Book reviews\n\nThis page lists book I have reviewed for MAA Reviews.\n\nVariational Methods: Applications to Nonlinear Partial Differential Equations and Hamiltonian Systems by Michael Struwe\n\nIntroduction to Applied Optimization by Urmila Diwekar\n\nAn Atlas of Functions by Keith Oldham, Jan Myland, and Jerome Spanier\n\nA Course in Approximation Theory by Ward Cheney and Will Light\n\nComplexity: A Guided Tour by Melanie Mitchell\n\nNine Introductions in Complex Analysis by Sanford Segal\n\nCloser and Closer: Introducing Real Analysis by Carol Schumacher\n\nCombinatorics of Genome Rearrangements by Guillaume Fertin et al.\n\nLinear Algebra: Theory and Applications by Ward Cheney and David Kincaid\n\nThe Mathematical Mechanic by Mark Levi\n\nExplorations in Monte Carlo Methods by Ronald Shonkwiler and Franklin Mendivil\n\nNot Always Buried Deep: A Second Course in Elementary Number Theory by Paul Pollack\n\nIntroduction of Probability with Mathematica by Kevin Hastings\n\nThe Life and Times of the Central Limit Theorem by William J. Adams\n\nLinear Systems Theory by João Hespanha\n\nScientific Computation by Gaston Gonnet and Ralf Scholl\n\nThe Pleasures of Statistics: The Autobiography of Frederick Mosteller\n\nConvex Functions by Jonathan M. Borwein and Jon D. Vanderwerff\n\nPartial Differential Equations by Lawrence C. Evans\n\nRemarkable Engineers by Ioan James\n\nSecond Order Differential Equations: Special Functions and their Classification by Gerhard Kristensson\n\nI also maintain a page listing books I've mentioned on my blog along with references to the posts where the books were mentioned."}
{"slug": "books_page", "canonical_url": "https://www.johndcook.com/blog/books_page/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/books.html", "title": "Books", "heading": "Books", "description": "Books mentioned on this site.", "summary": "This page lists the books mentioned on my blog. Each book is followed by links to the posts that mention it.", "word_count": 774, "blocks": [{"tag": "h1", "text": "Books"}, {"tag": "p", "text": "This page lists the books mentioned on my blog. Each book is followed by links to the posts that mention it."}, {"tag": "p", "text": "See also the books tag on the blog."}, {"tag": "p", "text": "Entrepreneurship Computing Math Statistics Eclectic"}, {"tag": "h2", "text": "Entrepreneurship"}, {"tag": "p", "text": "Blog posts: Plane crashes, software crashes, and business crashes"}, {"tag": "p", "text": "Blog posts: Gerald Weinberg's law of twins"}, {"tag": "p", "text": "Blog posts: Innovation V"}, {"tag": "p", "text": "Blog posts: Peter Drucker and abandoning projects Inside Steve Job's brain"}, {"tag": "p", "text": "Blog posts: Innovation II Innovation III"}, {"tag": "p", "text": "Blog posts: Obscuring complexity"}, {"tag": "p", "text": "Blog posts: Peter Drucker and abandoning projects"}, {"tag": "p", "text": "Blog posts: Index of tail weights in statistics"}, {"tag": "p", "text": "Blog posts: Three quotes on originality"}, {"tag": "p", "text": "Blog posts: Being busy"}, {"tag": "p", "text": "Blog posts: Top three iPod troubleshooting tips"}, {"tag": "p", "text": "Blog posts: How to avoid being outsourced or open sourced"}, {"tag": "h2", "text": "Computing"}, {"tag": "p", "text": "Blog posts: Interview with Dan Bricklin Would you rather have a chauffeur or a Ferrari? Two kinds of software challenges"}, {"tag": "p", "text": "Blog posts: The holy grail of CSS Side benefits of accessibility Migrating from HTML to XHTML"}, {"tag": "p", "text": "Blog posts: Beautiful Testing"}, {"tag": "p", "text": "Blog posts: Writes large correct programs Experienced programmers and lines of code"}, {"tag": "p", "text": "Blog posts: Upper and lower bounds on binomial coefficients The cost of breaking things apart and putting them back together"}, {"tag": "p", "text": "Blog posts: Michael Feathers on refactoring"}, {"tag": "p", "text": "Blog posts: In praise of tedious proofs The excitement of not knowing what you're doing"}, {"tag": "p", "text": "Blog posts: Rotating programmers Automated software builds"}, {"tag": "p", "text": "Blog posts: Book review: Expert Python Programming"}, {"tag": "p", "text": "Blog posts: Book review: Expert Python Programming"}, {"tag": "p", "text": "Blog posts: Book review: Expert Python Programming"}, {"tag": "p", "text": "Blog posts: Conceptual integrity"}, {"tag": "p", "text": "Blog posts: PowerShell output redirection: Unicode or ASCII?"}, {"tag": "p", "text": "Blog posts: Programming language subsets"}, {"tag": "p", "text": "Blog posts: Literate programming and statistics Complementary validation"}, {"tag": "p", "text": "Blog posts: Sample code sites"}, {"tag": "p", "text": "Blog posts: Architects versus engineers"}, {"tag": "p", "text": "Blog posts: Programmers aren't reading programming books Free C# book"}, {"tag": "p", "text": "Blog posts: Tips for learning regular expressions"}, {"tag": "p", "text": "Blog posts: Interview with Green Baret debugger"}, {"tag": "p", "text": "Blog posts: Customizing the PowerShell command prompt II"}, {"tag": "p", "text": "Blog posts: Regular expressions in C++ TR1"}, {"tag": "p", "text": "Blog posts: Tricky code"}, {"tag": "p", "text": "Blog posts: Why Unicode is subtle"}, {"tag": "p", "text": "Blog posts: Feasibility studies"}, {"tag": "p", "text": "Blog posts: MIT replaces Scheme with Python"}, {"tag": "h2", "text": "Math"}, {"tag": "p", "text": "Blog posts: Fast exponentiation Comparing three methods of computing standard deviation"}, {"tag": "p", "text": "Blog posts: Four uncommon but handy math notations Finite differences"}, {"tag": "p", "text": "Blog posts: Three books on inequalities"}, {"tag": "p", "text": "Blog posts: Three books on inequalities"}, {"tag": "p", "text": "Blog posts: Sums of uniform random values Old math books"}, {"tag": "p", "text": "Mentioned: Academic articles"}, {"tag": "p", "text": "Mentioned: Academic articles"}, {"tag": "p", "text": "Blog posts: Networks and power laws"}, {"tag": "p", "text": "Blog posts: Random inequalities III: numerical results"}, {"tag": "p", "text": "Blog posts: Simple legacy"}, {"tag": "p", "text": "Blog posts: Computing the inverse of the normal CDF Stand-alone error function erf(x) Optical illusion, mathematical illusion"}, {"tag": "p", "text": "Blog posts: Constructive proof of the Chinese Remainder Theorem"}, {"tag": "p", "text": "Blog posts: Random inequalities IV: Cauchy distributions Thick tails"}, {"tag": "p", "text": "Blog posts: Flaw in Riemann hypothesis proof"}, {"tag": "p", "text": "Blog posts: Diagramming modes of convergence"}, {"tag": "p", "text": "Blog posts: Fibonacci methods at work"}, {"tag": "p", "text": "Blog posts: Old math books Means and inequalities Three books on inequalities"}, {"tag": "p", "text": "Blog posts: Old math books"}, {"tag": "p", "text": "Blog posts: Fractional derivatives"}, {"tag": "h2", "text": "Statistics"}, {"tag": "p", "text": "Blog posts: Origin of \"statistically significant\" What is a confidence interval?"}, {"tag": "p", "text": "Blog posts: The cult of significance testing Five criticisms of significance testing"}, {"tag": "p", "text": "Blog posts: What is a confidence interval? Rolling dice for normal samples"}, {"tag": "p", "text": "Blog posts: Probability distribution relationships Finding the shortest interval with given mass"}, {"tag": "p", "text": "Blog posts: Plausible reasoning How loud is the evidence?"}, {"tag": "p", "text": "Blog posts: Index of tail weights in statistics"}, {"tag": "p", "text": "Blog posts: Good opening lines"}, {"tag": "p", "text": "Blog posts: Four pillars of Bayesian statistics"}, {"tag": "p", "text": "Blog posts: Quantifying the error in the central limit theorem"}, {"tag": "p", "text": "Blog posts: Random inequalities IV: Cauchy distributions Black swan talk Why heights are not normally distributed"}, {"tag": "p", "text": "Blog posts: Four characterizations of the normal distribution"}, {"tag": "p", "text": "Blog posts: Tukey tallying How to linearize data for regression"}, {"tag": "p", "text": "The Likelihood Principle Blog posts: Musicians, drunks, and Oliver Cromwell"}, {"tag": "p", "text": "Blog posts: The law of small numbers"}, {"tag": "h2", "text": "Eclectic"}, {"tag": "p", "text": "Blog posts: Don't forget the salt"}, {"tag": "p", "text": "Blog posts: Originality Stephen Covey and Pope Leo X"}, {"tag": "p", "text": "Blog posts: Small advantages show up in the extremes Twitter is not micro-blogging"}, {"tag": "p", "text": "Blog posts: Wine and politics"}, {"tag": "p", "text": "Blog posts: Was Einstein an atheist?"}, {"tag": "p", "text": "Blog posts: Xylophones and zebras part II: learning Spanish"}, {"tag": "p", "text": "Blog posts: Was Einstein an atheist?"}, {"tag": "p", "text": "Blog posts: Wendell Berry on publish-or-perish"}, {"tag": "p", "text": "Blog posts: Team moon"}, {"tag": "p", "text": "Blog posts: Robert's rules of order and Galveston flooding"}, {"tag": "p", "text": "Blog posts: Houston deco"}, {"tag": "p", "text": "Blog posts: C-state and F-state"}, {"tag": "p", "text": "Blog posts: What to make flexible"}, {"tag": "p", "text": "Blog posts: How to avoid being outsourced or open sourced"}, {"tag": "p", "text": "Blog posts: Selective use of technology"}, {"tag": "p", "text": "Blog posts: Aging with grace"}, {"tag": "p", "text": "Blog posts: Battling sharks"}, {"tag": "p", "text": "Blog posts: Things that work best when you don't notice them"}, {"tag": "p", "text": "Blog posts: Broken windows theory and programming"}, {"tag": "p", "text": "Blog posts: C. S. Lewis on reading old books"}, {"tag": "p", "text": "Blog posts: Three reasons expert predictions are often wrong"}, {"tag": "p", "text": "Blog posts: The Medici Effect"}, {"tag": "p", "text": "Blog posts: Privacy"}], "content": "Books\n\nThis page lists the books mentioned on my blog. Each book is followed by links to the posts that mention it.\n\nSee also the books tag on the blog.\n\nEntrepreneurship Computing Math Statistics Eclectic\n\nEntrepreneurship\n\nBlog posts: Plane crashes, software crashes, and business crashes\n\nBlog posts: Gerald Weinberg's law of twins\n\nBlog posts: Innovation V\n\nBlog posts: Peter Drucker and abandoning projects Inside Steve Job's brain\n\nBlog posts: Innovation II Innovation III\n\nBlog posts: Obscuring complexity\n\nBlog posts: Peter Drucker and abandoning projects\n\nBlog posts: Index of tail weights in statistics\n\nBlog posts: Three quotes on originality\n\nBlog posts: Being busy\n\nBlog posts: Top three iPod troubleshooting tips\n\nBlog posts: How to avoid being outsourced or open sourced\n\nComputing\n\nBlog posts: Interview with Dan Bricklin Would you rather have a chauffeur or a Ferrari? Two kinds of software challenges\n\nBlog posts: The holy grail of CSS Side benefits of accessibility Migrating from HTML to XHTML\n\nBlog posts: Beautiful Testing\n\nBlog posts: Writes large correct programs Experienced programmers and lines of code\n\nBlog posts: Upper and lower bounds on binomial coefficients The cost of breaking things apart and putting them back together\n\nBlog posts: Michael Feathers on refactoring\n\nBlog posts: In praise of tedious proofs The excitement of not knowing what you're doing\n\nBlog posts: Rotating programmers Automated software builds\n\nBlog posts: Book review: Expert Python Programming\n\nBlog posts: Book review: Expert Python Programming\n\nBlog posts: Book review: Expert Python Programming\n\nBlog posts: Conceptual integrity\n\nBlog posts: PowerShell output redirection: Unicode or ASCII?\n\nBlog posts: Programming language subsets\n\nBlog posts: Literate programming and statistics Complementary validation\n\nBlog posts: Sample code sites\n\nBlog posts: Architects versus engineers\n\nBlog posts: Programmers aren't reading programming books Free C# book\n\nBlog posts: Tips for learning regular expressions\n\nBlog posts: Interview with Green Baret debugger\n\nBlog posts: Customizing the PowerShell command prompt II\n\nBlog posts: Regular expressions in C++ TR1\n\nBlog posts: Tricky code\n\nBlog posts: Why Unicode is subtle\n\nBlog posts: Feasibility studies\n\nBlog posts: MIT replaces Scheme with Python\n\nMath\n\nBlog posts: Fast exponentiation Comparing three methods of computing standard deviation\n\nBlog posts: Four uncommon but handy math notations Finite differences\n\nBlog posts: Three books on inequalities\n\nBlog posts: Three books on inequalities\n\nBlog posts: Sums of uniform random values Old math books\n\nMentioned: Academic articles\n\nMentioned: Academic articles\n\nBlog posts: Networks and power laws\n\nBlog posts: Random inequalities III: numerical results\n\nBlog posts: Simple legacy\n\nBlog posts: Computing the inverse of the normal CDF Stand-alone error function erf(x) Optical illusion, mathematical illusion\n\nBlog posts: Constructive proof of the Chinese Remainder Theorem\n\nBlog posts: Random inequalities IV: Cauchy distributions Thick tails\n\nBlog posts: Flaw in Riemann hypothesis proof\n\nBlog posts: Diagramming modes of convergence\n\nBlog posts: Fibonacci methods at work\n\nBlog posts: Old math books Means and inequalities Three books on inequalities\n\nBlog posts: Old math books\n\nBlog posts: Fractional derivatives\n\nStatistics\n\nBlog posts: Origin of \"statistically significant\" What is a confidence interval?\n\nBlog posts: The cult of significance testing Five criticisms of significance testing\n\nBlog posts: What is a confidence interval? Rolling dice for normal samples\n\nBlog posts: Probability distribution relationships Finding the shortest interval with given mass\n\nBlog posts: Plausible reasoning How loud is the evidence?\n\nBlog posts: Index of tail weights in statistics\n\nBlog posts: Good opening lines\n\nBlog posts: Four pillars of Bayesian statistics\n\nBlog posts: Quantifying the error in the central limit theorem\n\nBlog posts: Random inequalities IV: Cauchy distributions Black swan talk Why heights are not normally distributed\n\nBlog posts: Four characterizations of the normal distribution\n\nBlog posts: Tukey tallying How to linearize data for regression\n\nThe Likelihood Principle Blog posts: Musicians, drunks, and Oliver Cromwell\n\nBlog posts: The law of small numbers\n\nEclectic\n\nBlog posts: Don't forget the salt\n\nBlog posts: Originality Stephen Covey and Pope Leo X\n\nBlog posts: Small advantages show up in the extremes Twitter is not micro-blogging\n\nBlog posts: Wine and politics\n\nBlog posts: Was Einstein an atheist?\n\nBlog posts: Xylophones and zebras part II: learning Spanish\n\nBlog posts: Was Einstein an atheist?\n\nBlog posts: Wendell Berry on publish-or-perish\n\nBlog posts: Team moon\n\nBlog posts: Robert's rules of order and Galveston flooding\n\nBlog posts: Houston deco\n\nBlog posts: C-state and F-state\n\nBlog posts: What to make flexible\n\nBlog posts: How to avoid being outsourced or open sourced\n\nBlog posts: Selective use of technology\n\nBlog posts: Aging with grace\n\nBlog posts: Battling sharks\n\nBlog posts: Things that work best when you don't notice them\n\nBlog posts: Broken windows theory and programming\n\nBlog posts: C. S. Lewis on reading old books\n\nBlog posts: Three reasons expert predictions are often wrong\n\nBlog posts: The Medici Effect\n\nBlog posts: Privacy"}
{"slug": "camp_paulson", "canonical_url": "https://www.johndcook.com/blog/camp_paulson/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/camp_paulson.html", "title": "Camp-Paulson normal approximation to the Binomial distribution", "heading": "Camp-Paulson normal approximation to the binomial distribution", "description": "Notes on the Camp-Paulson normal approximation to the binomial distribution.", "summary": "If X ~ Binomial(n, p) the Central Limit Theorem provides an approximation to the CDF FX. This approximation is given by", "word_count": 201, "blocks": [{"tag": "h1", "text": "Camp-Paulson normal approximation to the binomial distribution"}, {"tag": "p", "text": "If X ~ Binomial(n, p) the Central Limit Theorem provides an approximation to the CDF FX. This approximation is given by"}, {"tag": "p", "text": "FX(k) ≈ Φ((k + 0.5 - np)/√ (npq))."}, {"tag": "p", "text": "Here Φ is the CDF of a standard normal (Gaussian) random variable and q = 1-p. The central limit theorem approximation is studied in these notes. The Camp-Paulson approximation improves on the classical approximation by using a non-linear transformation of the argument k. This approximation uses"}, {"tag": "p", "text": "FX(k) ≈ Φ((c - μ)/σ)"}, {"tag": "p", "text": "where"}, {"tag": "p", "text": "c = (1-b)r1/3, μ = 1 - a, σ = √(br2/3 + a), a = 1/(9n-9k), b = 1/(9k+9), and r = (k+1)(1-p)/(np-kp)."}, {"tag": "p", "text": "Johnson and Kotz prove that the error in the Camp-Paulson approximation is never more than 0.007/√ (npq). The Camp-Paulson approximation is often one or two orders of magnitude more accurate than the classical approximation arising directly from the Central Limit Theorem."}, {"tag": "p", "text": "For more information, see \"Some Suggestions for Teaching About Normal Approximations to Poisson and Binomial Distribution Functions\" by Scott M. Lesch and Daniel R. Jeske, The American Statistician, August 2009, Vol 63, No 3."}, {"tag": "p", "text": "See also notes on the normal approximation to the beta, binomial, gamma, and student-t distributions."}], "content": "Camp-Paulson normal approximation to the binomial distribution\n\nIf X ~ Binomial(n, p) the Central Limit Theorem provides an approximation to the CDF FX. This approximation is given by\n\nFX(k) ≈ Φ((k + 0.5 - np)/√ (npq)).\n\nHere Φ is the CDF of a standard normal (Gaussian) random variable and q = 1-p. The central limit theorem approximation is studied in these notes. The Camp-Paulson approximation improves on the classical approximation by using a non-linear transformation of the argument k. This approximation uses\n\nFX(k) ≈ Φ((c - μ)/σ)\n\nwhere\n\nc = (1-b)r1/3, μ = 1 - a, σ = √(br2/3 + a), a = 1/(9n-9k), b = 1/(9k+9), and r = (k+1)(1-p)/(np-kp).\n\nJohnson and Kotz prove that the error in the Camp-Paulson approximation is never more than 0.007/√ (npq). The Camp-Paulson approximation is often one or two orders of magnitude more accurate than the classical approximation arising directly from the Central Limit Theorem.\n\nFor more information, see \"Some Suggestions for Teaching About Normal Approximations to Poisson and Binomial Distribution Functions\" by Scott M. Lesch and Daniel R. Jeske, The American Statistician, August 2009, Vol 63, No 3.\n\nSee also notes on the normal approximation to the beta, binomial, gamma, and student-t distributions."}
{"slug": "catchblocks", "canonical_url": "https://www.johndcook.com/blog/catchblocks/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/catchblocks.html", "title": "PowerShell script to print catch blocks", "heading": "PowerShell script to report catch blocks", "description": "PowerShell script to report catch blocks", "summary": "This script will look for catch statements and report a specified number of lines following each catch. It's a little crude — it prints a fixed number of lines rather than parsing the code well enough to pull out the entire catch block — but it is good enough for code reviews. If there's a problem, a few lines is ...", "word_count": 243, "blocks": [{"tag": "h1", "text": "PowerShell script to report catch blocks"}, {"tag": "p", "text": "This script will look for catch statements and report a specified number of lines following each catch. It's a little crude — it prints a fixed number of lines rather than parsing the code well enough to pull out the entire catch block — but it is good enough for code reviews. If there's a problem, a few lines is usually plenty to spot it. In fact, having less code to look at may help spot problems."}, {"tag": "p", "text": "See this blog post: Reviewing catch blocks."}, {"tag": "pre", "text": "# If path is not provided as an argument,\n# the script searches the current directory.\nparam($srcRoot=(pwd))\n\n# Make an array of file names in the directory\n# containing C# or C++ code\n$ext = \"\\.(cs|cpp|h)$\"\n$files = (dir -recurse $srcRoot | %{$_.FullName}) -match $ext\n\n# number of lines to print after a catch block\n$contextLines = 5\n\n# $contextLines is a guard.\n# Initialize $context to any value great than $contextLines\n$context = $contextLines + 1 \n\n# Process each file\nforeach ($file in $files)\n{\n    $allLines = (get-content $file)\n    $linesToKeep = @()\n    $lineNumber = 0\n\n    # Process each line in a file\n    foreach ($line in $allLines)\n    {\n        $lineNumber++\n\n        if ($line -match \"catch\\s*\\(\")\n        {\n            $linesToKeep += \"`nLine $lineNumber\"\n            $context = 0\n        } \n\n        if ($context -le $contextLines)\n        {\n            $linesToKeep += $line\n            $context++\n        }\n    }\n\n    # Print report of catch blocks\n    if ($linesToKeep.Length -gt 0)\n    {\n        $file\n        \" \"\n        $linesToKeep\n        \"___________________________________________\"\n        \" \"\n    }\n}"}], "content": "PowerShell script to report catch blocks\n\nThis script will look for catch statements and report a specified number of lines following each catch. It's a little crude — it prints a fixed number of lines rather than parsing the code well enough to pull out the entire catch block — but it is good enough for code reviews. If there's a problem, a few lines is usually plenty to spot it. In fact, having less code to look at may help spot problems.\n\nSee this blog post: Reviewing catch blocks.\n\n# If path is not provided as an argument,\n# the script searches the current directory.\nparam($srcRoot=(pwd))\n\n# Make an array of file names in the directory\n# containing C# or C++ code\n$ext = \"\\.(cs|cpp|h)$\"\n$files = (dir -recurse $srcRoot | %{$_.FullName}) -match $ext\n\n# number of lines to print after a catch block\n$contextLines = 5\n\n# $contextLines is a guard.\n# Initialize $context to any value great than $contextLines\n$context = $contextLines + 1 \n\n# Process each file\nforeach ($file in $files)\n{\n    $allLines = (get-content $file)\n    $linesToKeep = @()\n    $lineNumber = 0\n\n    # Process each line in a file\n    foreach ($line in $allLines)\n    {\n        $lineNumber++\n\n        if ($line -match \"catch\\s*\\(\")\n        {\n            $linesToKeep += \"`nLine $lineNumber\"\n            $context = 0\n        } \n\n        if ($context -le $contextLines)\n        {\n            $linesToKeep += $line\n            $context++\n        }\n    }\n\n    # Print report of catch blocks\n    if ($linesToKeep.Length -gt 0)\n    {\n        $file\n        \" \"\n        $linesToKeep\n        \"___________________________________________\"\n        \" \"\n    }\n}"}
{"slug": "category_theory", "canonical_url": "https://www.johndcook.com/blog/category_theory/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/category_theory.html", "title": "Category theory definition dependencies", "heading": null, "description": "Diagram showing the dependencies between category theory definitions", "summary": null, "word_count": 0, "blocks": [], "content": ""}
{"slug": "central_limit_theorems", "canonical_url": "https://www.johndcook.com/blog/central_limit_theorems/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/central_limit_theorems.html", "title": "Central Limit Theorems", "heading": "Central Limit Theorems", "description": "Various forms of the central limit theorem: the classical theorem, extensions by Feller, Lindeberg, Liapounov, and Levy.", "summary": "These notes summarize several extensions of the Central Limit Theorem (CLT) and related results.", "word_count": 1141, "blocks": [{"tag": "h1", "text": "Central Limit Theorems"}, {"tag": "p", "text": "These notes summarize several extensions of the Central Limit Theorem (CLT) and related results."}, {"tag": "p", "text": "Outline:"}, {"tag": "p", "text": "Classical CLT Rate of convergence Directions for generalization Non-identically distributed random variables Liapounov's theorem Lindeberg-Feller theorem Generalized CLT for infinite variance"}, {"tag": "h2", "text": "Classical Central Limit Theorem"}, {"tag": "p", "text": "Let Xn be a sequence of independent, identically distributed (i.i.d.) random variables. Assume each X has finite mean, E(X) = μ, and finite variance, Var(X) = σ2. Let Zn be the normalized average of the first n random variables, i.e."}, {"tag": "p", "text": "Zn = (X1 + X2 + ... + Xn - nμ)/ σ √ n."}, {"tag": "p", "text": "The classical Central Limit Theorem says that Zn converges in distribution to a standard normal distribution. This means that the CDF of Zn converges pointwise to Φ, the CDF of a standard normal (Gaussian) random variable. (See notes on modes of convergence.)"}, {"tag": "p", "text": "A special case of the CLT in which the Xn are assumed to be binomial goes back to Abraham de Moivre in 1733."}, {"tag": "h3", "text": "Rate of convergence"}, {"tag": "p", "text": "It is natural to ask about the rate of convergence in the CLT. If Fn is the CDF of Zn, once we know that Fn(x) converges to Φ(x) as n → ∞, we might want to know how quickly this convergence takes place. Said another way, for a given n, we might want to know how well Φ approximates Fn. This question is settled by the Berry-Esséen theorem. See Quantifying the error in the central limit theorem. For examples of normal approximations for specific distributions, see the following links: binomial, beta, gamma, Poisson, Student-t."}, {"tag": "h3", "text": "Directions for generalization"}, {"tag": "p", "text": "The classical CLT has three requirements:"}, {"tag": "li", "text": "independence,"}, {"tag": "li", "text": "identical distribution, and"}, {"tag": "li", "text": "finite variance."}, {"tag": "p", "text": "Each of these conditions can be weakened to create variations on the central limit theorem. We will keep the assumption of independence in these notes. For CLT results for dependent random variables, see Chow and Teicher. Below we consider non-identically distributed random variables and random variables with infinite variance."}, {"tag": "h2", "text": "Non-identically distributed random variables"}, {"tag": "p", "text": "In this section we allow the possibility that the Xn variables are not identically distributed. The main results in this are the Lindeberg-Feller theorem and its corollary Liapounov's theorem."}, {"tag": "p", "text": "First we introduce notation and assumptions common to both theorems. Let Xn be a sequence of independent random variables, at least one of which has a non-degenerate distribution. Assume each Xn has mean 0 and variance σn2. Define the partial sum"}, {"tag": "p", "text": "Sn = X1 + X2 + ... + Xn"}, {"tag": "p", "text": "and its variance"}, {"tag": "p", "text": "sn2 = σ12 + σ22 + ... + σn2."}, {"tag": "p", "text": "Both theorems concern under what circumstances the normalized partial sums Sn / sn converge in distribution to a standard normal random variable. We start with Liapounov's theorem because it is simpler."}, {"tag": "h3", "text": "Liapounov's theorem"}, {"tag": "p", "text": "Liapounov's theorem weakens the requirement of identical distribution but strengthens the requirement of finite variance. Where the classical CLT requires finite moments of order 2, Liapounov's CLT requires finite moments of order 2 + δ for some δ > 0."}, {"tag": "p", "text": "Assume E(|Xn|2+δ) is bounded for some δ > 0 and for all n. If"}, {"tag": "p", "text": "sn-1 - δ/2 ∑1 ≤ k ≤ n E(|Xn|2 + δ) → 0"}, {"tag": "p", "text": "as n → ∞ then Sn / sn converges in distribution to a standard normal random variable."}, {"tag": "h3", "text": "Lindeberg-Feller theorem"}, {"tag": "p", "text": "The Lindeberg-Feller theorem is more general than Liapounov's theorem. It gives necessary and sufficient conditions for Sn / sn to converge to a standard normal."}, {"tag": "p", "text": "Lindeberg: Under the assumptions above (each X has zero mean and finite variance, and at least one X has a non-degenerate distribution) then if the Lindeberg condition holds, Sn / sn converges in distribution to a standard normal random variable."}, {"tag": "p", "text": "Feller: Conversely, if Sn / sn converges in distribution to a standard normal and σn/sn → 0 and sn → ∞ then the Lindeberg condition holds."}, {"tag": "p", "text": "So what is this Lindeberg condition? Let Fn be the CDF of Xn, i.e. Fn(x) = P(Xn < x). The Lindeberg condition requires"}, {"tag": "p", "text": "for all ε > 0."}, {"tag": "h2", "text": "Generalized CLT for random variables with infinite variance"}, {"tag": "p", "text": "For this section, we require the random variables Xn to be independent and identically distributed. However, we do not require that they have finite variance."}, {"tag": "p", "text": "First we look at some restrictions for what a generalized CLT would look like for random variables Xn without finite variance. We would need sequences of constants an and bn such that (X1 + X2 + ... + Xn - bn)/an converges in distribution to something. It turns out that the something that the sequence converges to must have a stable distribution."}, {"tag": "p", "text": "Let X0, X1, and X2 be independent, identically distributed (iid) random variables. The distribution of these random variables is called stable if for every pair of positive real numbers a and b, there exists a positive c and a real d such that cX0 + d has the same distribution as aX1 + bX2."}, {"tag": "p", "text": "Stable distributions can be specified by four parameters. One of the four parameters is the exponent parameter 0 < α ≤ 2. This parameter is controls the thickness of the distribution tails. The distributions with α = 2 are the normal (Gaussian) distributions. For α < 2, the PDF is asymptotically proportional to |x|-α-1 and the CDF is asymptotically proportional to |x|-α as x → ±∞. And so except for the normal distribution, all stable distributions have thick tails; the variance does not exist."}, {"tag": "p", "text": "The characteristic functions for stable distributions can be written in closed form in terms of the four parameters mentioned above. In general, however, the density functions for stable distributions cannot be written down in closed form. There are three exceptions: the normal distributions, the Cauchy distributions, and the Lévy distributions."}, {"tag": "p", "text": "Let F(x) be the CDF for the random variables Xi. The following conditions on F are necessary and sufficient for the aggregation of the X’s to converge to a stable distribution with exponent α < 2."}, {"tag": "li", "text": "F(x) = (c1 + o(1)) |x|-α h(|x|) as x → -∞, and"}, {"tag": "li", "text": "1 - F(x) = (c2 + o(1)) x-α h(x) as x → ∞"}, {"tag": "p", "text": "where h(x) is a slowly varying function. Here o(1) denotes a function tending to 0. (See notes on asymptotic notation.) A slowly varying function h(x) is one such that the ratio h(cx) / h(x) → 1 as x → ∞ for all c > 0. Roughly speaking, this means F(x) has to look something like |x|-α in both the left and right tails, and so the X’s must be distributed something like the limiting distribution. For more information, see Petrov's book below."}, {"tag": "h2", "text": "References"}, {"tag": "p", "text": "Limit Theorems of Probability Theory: Sequences of Independent Random Variables by Valentin Petrov."}, {"tag": "p", "text": "Probability Theory: Independence, Interchangeability, Martingales by Yuan Shih Chow and Henry Teicher."}, {"tag": "p", "text": "Power laws and the generalized CLT blog post"}, {"tag": "p", "text": "An introduction to stable distributions by John P. Nolan"}, {"tag": "p", "text": "The Life and Times of the Central Limit Theorem by William Adams"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Central Limit Theorems\n\nThese notes summarize several extensions of the Central Limit Theorem (CLT) and related results.\n\nOutline:\n\nClassical CLT Rate of convergence Directions for generalization Non-identically distributed random variables Liapounov's theorem Lindeberg-Feller theorem Generalized CLT for infinite variance\n\nClassical Central Limit Theorem\n\nLet Xn be a sequence of independent, identically distributed (i.i.d.) random variables. Assume each X has finite mean, E(X) = μ, and finite variance, Var(X) = σ2. Let Zn be the normalized average of the first n random variables, i.e.\n\nZn = (X1 + X2 + ... + Xn - nμ)/ σ √ n.\n\nThe classical Central Limit Theorem says that Zn converges in distribution to a standard normal distribution. This means that the CDF of Zn converges pointwise to Φ, the CDF of a standard normal (Gaussian) random variable. (See notes on modes of convergence.)\n\nA special case of the CLT in which the Xn are assumed to be binomial goes back to Abraham de Moivre in 1733.\n\nRate of convergence\n\nIt is natural to ask about the rate of convergence in the CLT. If Fn is the CDF of Zn, once we know that Fn(x) converges to Φ(x) as n → ∞, we might want to know how quickly this convergence takes place. Said another way, for a given n, we might want to know how well Φ approximates Fn. This question is settled by the Berry-Esséen theorem. See Quantifying the error in the central limit theorem. For examples of normal approximations for specific distributions, see the following links: binomial, beta, gamma, Poisson, Student-t.\n\nDirections for generalization\n\nThe classical CLT has three requirements:\n\nindependence,\n\nidentical distribution, and\n\nfinite variance.\n\nEach of these conditions can be weakened to create variations on the central limit theorem. We will keep the assumption of independence in these notes. For CLT results for dependent random variables, see Chow and Teicher. Below we consider non-identically distributed random variables and random variables with infinite variance.\n\nNon-identically distributed random variables\n\nIn this section we allow the possibility that the Xn variables are not identically distributed. The main results in this are the Lindeberg-Feller theorem and its corollary Liapounov's theorem.\n\nFirst we introduce notation and assumptions common to both theorems. Let Xn be a sequence of independent random variables, at least one of which has a non-degenerate distribution. Assume each Xn has mean 0 and variance σn2. Define the partial sum\n\nSn = X1 + X2 + ... + Xn\n\nand its variance\n\nsn2 = σ12 + σ22 + ... + σn2.\n\nBoth theorems concern under what circumstances the normalized partial sums Sn / sn converge in distribution to a standard normal random variable. We start with Liapounov's theorem because it is simpler.\n\nLiapounov's theorem\n\nLiapounov's theorem weakens the requirement of identical distribution but strengthens the requirement of finite variance. Where the classical CLT requires finite moments of order 2, Liapounov's CLT requires finite moments of order 2 + δ for some δ > 0.\n\nAssume E(|Xn|2+δ) is bounded for some δ > 0 and for all n. If\n\nsn-1 - δ/2 ∑1 ≤ k ≤ n E(|Xn|2 + δ) → 0\n\nas n → ∞ then Sn / sn converges in distribution to a standard normal random variable.\n\nLindeberg-Feller theorem\n\nThe Lindeberg-Feller theorem is more general than Liapounov's theorem. It gives necessary and sufficient conditions for Sn / sn to converge to a standard normal.\n\nLindeberg: Under the assumptions above (each X has zero mean and finite variance, and at least one X has a non-degenerate distribution) then if the Lindeberg condition holds, Sn / sn converges in distribution to a standard normal random variable.\n\nFeller: Conversely, if Sn / sn converges in distribution to a standard normal and σn/sn → 0 and sn → ∞ then the Lindeberg condition holds.\n\nSo what is this Lindeberg condition? Let Fn be the CDF of Xn, i.e. Fn(x) = P(Xn < x). The Lindeberg condition requires\n\nfor all ε > 0.\n\nGeneralized CLT for random variables with infinite variance\n\nFor this section, we require the random variables Xn to be independent and identically distributed. However, we do not require that they have finite variance.\n\nFirst we look at some restrictions for what a generalized CLT would look like for random variables Xn without finite variance. We would need sequences of constants an and bn such that (X1 + X2 + ... + Xn - bn)/an converges in distribution to something. It turns out that the something that the sequence converges to must have a stable distribution.\n\nLet X0, X1, and X2 be independent, identically distributed (iid) random variables. The distribution of these random variables is called stable if for every pair of positive real numbers a and b, there exists a positive c and a real d such that cX0 + d has the same distribution as aX1 + bX2.\n\nStable distributions can be specified by four parameters. One of the four parameters is the exponent parameter 0 < α ≤ 2. This parameter is controls the thickness of the distribution tails. The distributions with α = 2 are the normal (Gaussian) distributions. For α < 2, the PDF is asymptotically proportional to |x|-α-1 and the CDF is asymptotically proportional to |x|-α as x → ±∞. And so except for the normal distribution, all stable distributions have thick tails; the variance does not exist.\n\nThe characteristic functions for stable distributions can be written in closed form in terms of the four parameters mentioned above. In general, however, the density functions for stable distributions cannot be written down in closed form. There are three exceptions: the normal distributions, the Cauchy distributions, and the Lévy distributions.\n\nLet F(x) be the CDF for the random variables Xi. The following conditions on F are necessary and sufficient for the aggregation of the X’s to converge to a stable distribution with exponent α < 2.\n\nF(x) = (c1 + o(1)) |x|-α h(|x|) as x → -∞, and\n\n1 - F(x) = (c2 + o(1)) x-α h(x) as x → ∞\n\nwhere h(x) is a slowly varying function. Here o(1) denotes a function tending to 0. (See notes on asymptotic notation.) A slowly varying function h(x) is one such that the ratio h(cx) / h(x) → 1 as x → ∞ for all c > 0. Roughly speaking, this means F(x) has to look something like |x|-α in both the left and right tails, and so the X’s must be distributed something like the limiting distribution. For more information, see Petrov's book below.\n\nReferences\n\nLimit Theorems of Probability Theory: Sequences of Independent Random Variables by Valentin Petrov.\n\nProbability Theory: Independence, Interchangeability, Martingales by Yuan Shih Chow and Henry Teicher.\n\nPower laws and the generalized CLT blog post\n\nAn introduction to stable distributions by John P. Nolan\n\nThe Life and Times of the Central Limit Theorem by William Adams\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "clinical-trial-design", "canonical_url": "https://www.johndcook.com/blog/clinical-trial-design/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/clinical_trials.html", "title": "Clinical trials and software", "heading": "Adaptive clinical trial software", "description": "Bayesian adaptive clinical trials: methods and software", "summary": "In May 2000 I joined the Department of Biostatistics at MD Anderson Cancer Center to build and lead its software development group, starting with three developers and growing the team to around 20 at its height. We developed the infrastructure for designing and conducting adaptive clinical trials based on Bayesian methods.", "word_count": 212, "blocks": [{"tag": "h1", "text": "Adaptive clinical trial software"}, {"tag": "p", "text": "In May 2000 I joined the Department of Biostatistics at MD Anderson Cancer Center to build and lead its software development group, starting with three developers and growing the team to around 20 at its height. We developed the infrastructure for designing and conducting adaptive clinical trials based on Bayesian methods."}, {"tag": "p", "text": "In addition to leading the software development group, I developed the C++ numerical library at the heart of our software and developed algorithms for several of the most commonly used applications."}, {"tag": "p", "text": "Working at MD Anderson, the world’s largest cancer center, gave me the opportunity to be involved with many clinical trials in roles ranging from initial design and simulation to conduct, support, and analysis."}, {"tag": "p", "text": "In addition to my work at MD Anderson, I have consulted for pharmaceutical companies regarding adaptive clinical trial design and conduct."}, {"tag": "p", "text": "I have taught short courses in Bayesian clinical trial software at L’Institut Bergonié in Bordeaux and at University of Puerto Rico in San Juan. I have also given shorter presentations on clinical trial software at Vanderbilt University and University of Houston."}, {"tag": "p", "text": "Some of the clinical trial software I have contributed to is available here."}, {"tag": "p", "text": "My publications include papers on Bayesian analysis and numerical algorithm design for Bayesian computation."}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Adaptive clinical trial software\n\nIn May 2000 I joined the Department of Biostatistics at MD Anderson Cancer Center to build and lead its software development group, starting with three developers and growing the team to around 20 at its height. We developed the infrastructure for designing and conducting adaptive clinical trials based on Bayesian methods.\n\nIn addition to leading the software development group, I developed the C++ numerical library at the heart of our software and developed algorithms for several of the most commonly used applications.\n\nWorking at MD Anderson, the world’s largest cancer center, gave me the opportunity to be involved with many clinical trials in roles ranging from initial design and simulation to conduct, support, and analysis.\n\nIn addition to my work at MD Anderson, I have consulted for pharmaceutical companies regarding adaptive clinical trial design and conduct.\n\nI have taught short courses in Bayesian clinical trial software at L’Institut Bergonié in Bordeaux and at University of Puerto Rico in San Juan. I have also given shorter presentations on clinical trial software at Vanderbilt University and University of Houston.\n\nSome of the clinical trial software I have contributed to is available here.\n\nMy publications include papers on Bayesian analysis and numerical algorithm design for Bayesian computation.\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "computing_binomial_coefficients", "canonical_url": "https://www.johndcook.com/blog/computing_binomial_coefficients/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/computing_binomial_coefficients.html", "title": "How to calculate general binomial coefficients", "heading": "How to calculate binomial coefficients", "description": "How to calculate general binomial coefficients with arbitrary real-valued arguments.", "summary": "This article explains how to calculate binomial coefficients for general arguments. See the article binomial coefficients for definitions.", "word_count": 135, "blocks": [{"tag": "h1", "text": "How to calculate binomial coefficients"}, {"tag": "p", "text": "This article explains how to calculate binomial coefficients for general arguments. See the article binomial coefficients for definitions."}, {"tag": "p", "text": "The flow chart below summarizes how to express the binomial coefficient (z, w) in terms of more familiar functions."}, {"tag": "p", "text": "Note that the expressions at the terminal nodes of the decision tree are only involve taking factorials of positive integers or evaluating the gamma function where it is non-singular."}, {"tag": "p", "text": "The Mathematica function Binomial[z, w] implements binomial coefficients as explained here. The R function choose(z, w) requires z to be real and w to be an integer."}, {"tag": "p", "text": "The fractions should be evaluated by subtracting logarithms rather than computing the numerators and denominators directly. Otherwise, the numerators or denominators will overflow for large arguments. See How to calculate binomial probabilities for details."}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "How to calculate binomial coefficients\n\nThis article explains how to calculate binomial coefficients for general arguments. See the article binomial coefficients for definitions.\n\nThe flow chart below summarizes how to express the binomial coefficient (z, w) in terms of more familiar functions.\n\nNote that the expressions at the terminal nodes of the decision tree are only involve taking factorials of positive integers or evaluating the gamma function where it is non-singular.\n\nThe Mathematica function Binomial[z, w] implements binomial coefficients as explained here. The R function choose(z, w) requires z to be real and w to be an integer.\n\nThe fractions should be evaluated by subtracting logarithms rather than computing the numerators and denominators directly. Otherwise, the numerators or denominators will overflow for large arguments. See How to calculate binomial probabilities for details.\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "conjugate_prior_diagram", "canonical_url": "https://www.johndcook.com/blog/conjugate_prior_diagram/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/conjugate_prior_diagram.html", "title": "Diagram of conjugate prior relationships", "heading": "Conjugate prior relationships", "description": "Graphical representation of common conjugate prior relationships along with footnotes.", "summary": "The following diagram summarizes conjugate prior relationships for a number of common sampling distributions. Arrows point from a sampling distribution to its conjugate prior distribution. The symbol near the arrow indicates which parameter the prior is unknown.", "word_count": 637, "blocks": [{"tag": "h1", "text": "Conjugate prior relationships"}, {"tag": "p", "text": "The following diagram summarizes conjugate prior relationships for a number of common sampling distributions. Arrows point from a sampling distribution to its conjugate prior distribution. The symbol near the arrow indicates which parameter the prior is unknown."}, {"tag": "p", "text": "These relationships depends critically on choice of parameterization, some of which are uncommon. This page uses the parameterizations that make the relationships simplest to state, not necessarily the most common parameterizations. See footnotes below."}, {"tag": "p", "text": "More mathematical diagrams"}, {"tag": "p", "text": "Click on a distribution to see its parameterization. Click on an arrow to see posterior parameters."}, {"tag": "p", "text": "See this page for more diagrams on this site including diagrams for probability and statistics, analysis, topology, and category theory."}, {"tag": "h2", "text": "Parameterizations"}, {"tag": "p", "text": "Let C(n, k) denote the binomial coefficient(n, k)."}, {"tag": "p", "text": "The geometric distribution has only one parameter, p, and has PMF f(x) = p (1-p)x."}, {"tag": "p", "text": "The binomial distribution with parameters n and p has PMF f(x) = C(n, x) px(1-p)n-x."}, {"tag": "p", "text": "The negative binomial distribution with parameters r and p has PMF f(x) = C(r + x - 1, x) pr(1-p)x."}, {"tag": "p", "text": "The Bernoulli distribution has probability of success p."}, {"tag": "p", "text": "The beta distribution has PDF f(p) = Γ(α + β) pα-1(1-p)β-1 / (Γ(α) Γ(β))."}, {"tag": "p", "text": "The exponential distribution parameterized in terms of the rate λ has PDF f(x) = λ exp(-λ x)."}, {"tag": "p", "text": "The gamma distribution parameterized in terms of the rate has PDF f(x) = βα xα-1exp(-β x) / Γ(α)."}, {"tag": "p", "text": "The Poisson distribution has one parameter λ and PMF f(x) = exp(-λ) λx/ x!."}, {"tag": "p", "text": "The normal distribution parameterized in terms of precision τ (τ = 1/σ2) has PDF f(x) = (τ/2π)1/2 exp( -τ(x - μ)2/2 )."}, {"tag": "p", "text": "The lognormal distribution parameterized in terms of precision τ has PDF f(x) = (τ/2π)1/2 exp( -τ(log(x) - μ)2/2 ) / x."}, {"tag": "h2", "text": "Posterior parameters"}, {"tag": "p", "text": "For each sampling distribution, assume we have data x1, x2, ..., xn."}, {"tag": "p", "text": "If the sampling distribution for x is binomial(m, p) with m known, and the prior distribution is beta(α, β), the posterior distribution for p is beta(α + Σxi, β + mn - Σxi). The Bernoulli is the special case of the binomial with m = 1."}, {"tag": "p", "text": "If the sampling distribution for x is negative binomial(r, p) with r known, and the prior distribution is beta(α, β), the posterior distribution for p is beta(α + nr, β + Σxi). The geometric is the special case of the negative binomial with r = 1."}, {"tag": "p", "text": "If the sampling distribution for x is gamma(α, β) with α known, and the prior distribution on β is gamma(α0, β0), the posterior distribution for β is gamma(α0 + n, β0 + Σxi). The exponential is a special case of the gamma with α = 1."}, {"tag": "p", "text": "If the sampling distribution for x is Poisson(λ), and the prior distribution on λ is gamma(α0, β0), the posterior on λ is gamma(α0 + Σxi, β0 + n)."}, {"tag": "p", "text": "If the sampling distribution for x is normal(μ, τ) with τ known, and the prior distribution on μ is normal(μ0, τ0), the posterior distribution on μ is normal((μ0 τ0 + τ Σxi)/(τ0 + nτ), τ0 + nτ)."}, {"tag": "p", "text": "If the sampling distribution for x is normal(μ, τ) with μ known, and the prior distribution on τ is gamma(α, β), the posterior distribution on τ is gamma(α + n/2, (n-1)S2) where S2 is the sample variance."}, {"tag": "p", "text": "If the sampling distribution for x is lognormal(μ, τ) with τ known, and the prior distribution on μ is normal(μ0, τ0), the posterior distribution on μ is normal((μ0 τ0 + τ Πxi)/(τ0 + nτ), τ0 + nτ)."}, {"tag": "p", "text": "If the sampling distribution for x is lognormal(μ, τ) with μ known, and the prior distribution on τ is gamma(α, β), the posterior distribution on τ is gamma(α + n/2, (n-1)S2) where S2 is the sample variance."}, {"tag": "h2", "text": "References"}, {"tag": "p", "text": "A compendium of conjugate priors by Daniel Fink."}, {"tag": "p", "text": "See also Wikipedia's article on conjugate priors."}, {"tag": "p", "text": "Please contact me to report any errors."}], "content": "Conjugate prior relationships\n\nThe following diagram summarizes conjugate prior relationships for a number of common sampling distributions. Arrows point from a sampling distribution to its conjugate prior distribution. The symbol near the arrow indicates which parameter the prior is unknown.\n\nThese relationships depends critically on choice of parameterization, some of which are uncommon. This page uses the parameterizations that make the relationships simplest to state, not necessarily the most common parameterizations. See footnotes below.\n\nMore mathematical diagrams\n\nClick on a distribution to see its parameterization. Click on an arrow to see posterior parameters.\n\nSee this page for more diagrams on this site including diagrams for probability and statistics, analysis, topology, and category theory.\n\nParameterizations\n\nLet C(n, k) denote the binomial coefficient(n, k).\n\nThe geometric distribution has only one parameter, p, and has PMF f(x) = p (1-p)x.\n\nThe binomial distribution with parameters n and p has PMF f(x) = C(n, x) px(1-p)n-x.\n\nThe negative binomial distribution with parameters r and p has PMF f(x) = C(r + x - 1, x) pr(1-p)x.\n\nThe Bernoulli distribution has probability of success p.\n\nThe beta distribution has PDF f(p) = Γ(α + β) pα-1(1-p)β-1 / (Γ(α) Γ(β)).\n\nThe exponential distribution parameterized in terms of the rate λ has PDF f(x) = λ exp(-λ x).\n\nThe gamma distribution parameterized in terms of the rate has PDF f(x) = βα xα-1exp(-β x) / Γ(α).\n\nThe Poisson distribution has one parameter λ and PMF f(x) = exp(-λ) λx/ x!.\n\nThe normal distribution parameterized in terms of precision τ (τ = 1/σ2) has PDF f(x) = (τ/2π)1/2 exp( -τ(x - μ)2/2 ).\n\nThe lognormal distribution parameterized in terms of precision τ has PDF f(x) = (τ/2π)1/2 exp( -τ(log(x) - μ)2/2 ) / x.\n\nPosterior parameters\n\nFor each sampling distribution, assume we have data x1, x2, ..., xn.\n\nIf the sampling distribution for x is binomial(m, p) with m known, and the prior distribution is beta(α, β), the posterior distribution for p is beta(α + Σxi, β + mn - Σxi). The Bernoulli is the special case of the binomial with m = 1.\n\nIf the sampling distribution for x is negative binomial(r, p) with r known, and the prior distribution is beta(α, β), the posterior distribution for p is beta(α + nr, β + Σxi). The geometric is the special case of the negative binomial with r = 1.\n\nIf the sampling distribution for x is gamma(α, β) with α known, and the prior distribution on β is gamma(α0, β0), the posterior distribution for β is gamma(α0 + n, β0 + Σxi). The exponential is a special case of the gamma with α = 1.\n\nIf the sampling distribution for x is Poisson(λ), and the prior distribution on λ is gamma(α0, β0), the posterior on λ is gamma(α0 + Σxi, β0 + n).\n\nIf the sampling distribution for x is normal(μ, τ) with τ known, and the prior distribution on μ is normal(μ0, τ0), the posterior distribution on μ is normal((μ0 τ0 + τ Σxi)/(τ0 + nτ), τ0 + nτ).\n\nIf the sampling distribution for x is normal(μ, τ) with μ known, and the prior distribution on τ is gamma(α, β), the posterior distribution on τ is gamma(α + n/2, (n-1)S2) where S2 is the sample variance.\n\nIf the sampling distribution for x is lognormal(μ, τ) with τ known, and the prior distribution on μ is normal(μ0, τ0), the posterior distribution on μ is normal((μ0 τ0 + τ Πxi)/(τ0 + nτ), τ0 + nτ).\n\nIf the sampling distribution for x is lognormal(μ, τ) with μ known, and the prior distribution on τ is gamma(α, β), the posterior distribution on τ is gamma(α + n/2, (n-1)S2) where S2 is the sample variance.\n\nReferences\n\nA compendium of conjugate priors by Daniel Fink.\n\nSee also Wikipedia's article on conjugate priors.\n\nPlease contact me to report any errors."}
{"slug": "coombes", "canonical_url": "https://www.johndcook.com/blog/coombes/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/coombes.html", "title": "Sweave: First steps toward reproducible analyses", "heading": "Sweave: First steps toward reproducible analyses", "description": "Presentation by Kevin Coombes on using Sweave to combine TeX and R for reproducible research.", "summary": "The following presentation by Kevin Coombes gives an introduction to the problem of irreproducible analyses and suggests Sweave as the first step in solution to the problem. The presentation gives an introduction to Sweave, and was itself developed as an Sweave document.", "word_count": 129, "blocks": [{"tag": "h1", "text": "Sweave: First steps toward reproducible analyses"}, {"tag": "p", "text": "The following presentation by Kevin Coombes gives an introduction to the problem of irreproducible analyses and suggests Sweave as the first step in solution to the problem. The presentation gives an introduction to Sweave, and was itself developed as an Sweave document."}, {"tag": "p", "text": "Final PDF file of the presentation: talkb.pdf Sweave source: talk.Rnw"}, {"tag": "p", "text": "To reproduce this presentation,"}, {"tag": "li", "text": "run Sweave(\"talk.Rnw\") inside R to produce talk.tex,"}, {"tag": "li", "text": "run pdflatex talk.tex to produce talk.pdf, and"}, {"tag": "li", "text": "run ppower4 talk.pdf talkb.pdf to produce the final presentation."}, {"tag": "p", "text": "Normally only the first two steps are necessary. However this presentation uses ppower4 for additional visual effects. Without running ppower4, pdflatex will produce a PDF file with white text on a white background."}, {"tag": "p", "text": "For more detailed information on processing Sweave documents, see Troubleshooting Sweave."}], "content": "Sweave: First steps toward reproducible analyses\n\nThe following presentation by Kevin Coombes gives an introduction to the problem of irreproducible analyses and suggests Sweave as the first step in solution to the problem. The presentation gives an introduction to Sweave, and was itself developed as an Sweave document.\n\nFinal PDF file of the presentation: talkb.pdf Sweave source: talk.Rnw\n\nTo reproduce this presentation,\n\nrun Sweave(\"talk.Rnw\") inside R to produce talk.tex,\n\nrun pdflatex talk.tex to produce talk.pdf, and\n\nrun ppower4 talk.pdf talkb.pdf to produce the final presentation.\n\nNormally only the first two steps are necessary. However this presentation uses ppower4 for additional visual effects. Without running ppower4, pdflatex will produce a PDF file with white text on a white background.\n\nFor more detailed information on processing Sweave documents, see Troubleshooting Sweave."}
{"slug": "cplusplus_strings", "canonical_url": "https://www.johndcook.com/blog/cplusplus_strings/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/cplusplus_strings.html", "title": "Unraveling Strings in Visual C++", "heading": "Unraveling Strings in Visual C++", "description": "An article explaining all the various types of strings used in Visual C++, what each is for, how each is used, and how to convert between types.", "summary": "This is an article I wrote sometime in the late 1990s about working with strings coming from COM, MFC, Win32, the C++ Standard Library, etc. It does not include anything about .NET since it was written before .NET came out.", "word_count": 2999, "blocks": [{"tag": "h1", "text": "Unraveling Strings in Visual C++"}, {"tag": "p", "text": "This is an article I wrote sometime in the late 1990s about working with strings coming from COM, MFC, Win32, the C++ Standard Library, etc. It does not include anything about .NET since it was written before .NET came out."}, {"tag": "h2", "text": "Outline"}, {"tag": "p", "text": "Why so many strings? When is each string appropriate? Using various strings Conversions between types Conclusion References Sample code"}, {"tag": "h2", "text": "Introduction"}, {"tag": "p", "text": "In the good old days, a string was a pointer to a null-terminated array of chars. Period. Now a string might be a char*, wchar_t*, LPCTSTR, BSTR, CString, basic_string, _bstr_t, CComBSTR, etc. Unfortunately, you cannot simply choose your favorite string representation and ignore the rest. Each representation has its own domain and it is frequently necessary to convert between types when crossing domain boundaries. Why are there so many kinds of strings? When is each one appropriate? How do you carry out common tasks with each? How do they relate to each other?"}, {"tag": "h2", "text": "Why so many strings?"}, {"tag": "p", "text": "Strings differ in three important ways: character set, memory layout, and conventions for use. The most obvious and simplest of these is character set. To keep things focused, we will limit ourselves to ANSI and Unicode. ANSI strings, the kind everybody grew up on, are arrays of single-byte characters. By far most of the world's strings are ANSI strings. So why bother with Unicode?"}, {"tag": "p", "text": "Eight bits are plenty to represent all the characters of ordinary English text. But with the slightest thought to international software, it quickly becomes apparent that eight bits are woefully inadequate. Unicode, with 16 bits per character, has enough possibilities to cover all the world's major languages with enough characters left over to even throw in a few ancient languages for good measure."}, {"tag": "p", "text": "Windows NT was built from the beginning to use Unicode strings exclusively internally, though you may write applications for NT that use either ANSI or Unicode. Windows CE only understands Unicode. OLE is built around Unicode strings. But Windows 3.x “doesn't know Unicode from a dress code, and never will [1].” The same is true of Windows 9x. The ANSI vs. Unicode strings are much like the English vs. metric measurement units: most everyone agrees the latter is the way to go, but the former has a tremendous installed base. In both situations, we will probably have to live with two standards and all the concomitant complications for a very long time."}, {"tag": "p", "text": "C++ has two built in character types: char and wchar_t. Most commonly a char is an ANSI character and a wchar_t is a Unicode character. This is not always the case, but to simplify things a bit, we will make this assumption. Wide character strings, i.e. strings of wchar_ts, are null-terminated arrays of characters, directly analogous with ordinary strings. The terminating null character in this case is a wchar_t null. Incidentally, the default settings for the Visual C++ debugger are to not display Unicode characters. There is a check box under Tools / Options / Debug labeled \"Display unicode strings\" which turns this on."}, {"tag": "p", "text": "In order to be able to use the same source code for ANSI and Unicode builds, Windows introduced the TCHAR data type. TCHAR is simply a macro that expands to char in ANSI builds (i.e. _UNICODE is not defined) and wchar_t in Unicode builds (_UNICODE is defined). There are various string types based on the TCHAR macro, such as LPCTSTR (long pointer to a constant TCHAR string)."}, {"tag": "p", "text": "Microsoft also introduced a number of macros and typedefs with \"OLE\" in the name such as OLECHAR, LPOLESTR, etc. These are vestiges of an automatic ANSI / Unicode conversion scheme that Microsoft used prior to MFC 4.0 and has since abandoned. However, the names live on for legacy support and for Macintosh development. For example, if you look for help on CLSIDFromProgID you'll find that its first argument is an LPCOLESTR. For Win32 development, \"OLE\" corresponds to Unicode. For Win16 and for the Macintosh, the symbol OLE2ANSI is defined and \"OLE\" corresponds to ANSI. For example, in Win32 development, an OLECHAR is simply a wchar_t and an LPOLESTR is a wide character string."}, {"tag": "p", "text": "Microsoft?s character and string types may be summarized as follows. A character name has the form XCHAR and string name has the form LPCXSTR where C is optional and X is either T, OLE, W, or empty. The C indicates a string type is constant, and the X has the following meanings:"}, {"tag": "p", "text": "MFC introduced the CString class as a wrapper around LPCTSTR data type which provides methods for common tasks such as memory allocation and substring searches. A CString can be used in most circumstances where you would use an LPCTSTR."}, {"tag": "p", "text": "The Standard C++ library provides a parameterized string class basic_string<T> where T is most often a char or wchar_t. The Standard library provides the typedefs string and wstring respectively for these common cases."}, {"tag": "p", "text": "The real confusion in string types comes when we introduce BSTRs. A BSTR differs from a common string in that it always uses Unicode, regardless of compiler switches. However, it also has a different layout in memory. Furthermore, there are different conventions for using BSTRs than for using simple null-terminated string, whether of the ANSI or Unicode variety, and these conventions are seldom codified."}, {"tag": "p", "text": "A BSTR is a null-terminated Unicode string, but with a byte count (not character count!) prepended. An advantage of a byte-count prefix is that BSTR can contain internal nulls, whereas an ordinary string may not. One unusual aspect of the BSTR is that the byte count is not in the 0th entry of the array the BSTR points to. Instead, the byte count is stored in the two bytes preceding the memory the pointer ostensibly points to. (MFC?s CString uses a similar trick so that passing a CString involves no more overhead than passing a pointer [2]. This causes no problems for developers, however, because the implementation is thoroughly encapsulated.)"}, {"tag": "p", "text": "OLE standardized on the BSTR partially because of OLE's desire to be language-independent. Many languages use the counted arrays rather than using a special symbol to mark the end of a string. The BSTR compromises by requiring both a count and a terminating character. (Note that in the context of string and character types, OLE refers only to character widths. In particular, an LPOLESTR is simply a wide character string and not a BSTR. Despite the name, an LPOLESTR is not OLE's favorite string!)"}, {"tag": "p", "text": "BSTRs are an unnatural imposition on C++. However, they are unavoidable because OLE is built around BSTRs and not native C++ strings. In order to make BSTR manipulation easier from C++, several wrapper classes have been created. One is ATL's CComBSTR class, which handles basic memory management and a few basic operations for strings."}, {"tag": "p", "text": "There is another BSTR wrapper which one must use in order to take advantage of the native COM support in the Visual C++ compiler. When you use the #import directive, the compiler creates wrapper functions for the methods on the imported COM interfaces. BSTR arguments and return values are wrapped as _bstr_t. (However, BSTR* arguments are left alone so the _bstr_t doesn't entirely eliminate the need to manipulate BSTRs.) The design goals of _bstr_t are different from that of CComBSTR. The former provides more convenience functions, and is implemented with reference counting to avoid unnecessary memory copying."}, {"tag": "h2", "text": "When is each string appropriate?"}, {"tag": "p", "text": "MFC class methods often take LPCTSTR arguments. The choice of a class wrapper for strings in MFC development is obviously CString especially because a CString can be used in most situations where an LPCTSTR is specified. The advantage of the CString class is that it provides many useful methods for memory management and string manipulation. One disadvantage is that CString carries with it a little bit more overhead than a raw LPCTSTR. Also, if CString is the only MFC class in a project, it still requires linking to and redistributing the MFC DLLs."}, {"tag": "p", "text": "The Standard C++ basic_string<> has the advantage of being portable to non-Windows platforms. Also, you may explicitly decide between char and wchar_t strings on an individual basis rather than deciding once and for all based on a compiler switch as with TCHAR strings. And you could use basic_string<TCHAR> to maintain the ANSI vs. Unicode flexibility of CString. Like CString, basic_string<> does define a large number of convenient string manipulation functions. A design goal of this string class was to make the class sufficiently convenient and efficient that it would seldom be necessary to use null terminated strings and the C library manipulation functions."}, {"tag": "p", "text": "In OLE interfaces, there is no choice but to use BSTR or one of its wrapper classes. Ordinarily, a C++ developer would use a BSTR only as a delivery vehicle to a COM interface; string manipulation is more easily done via library methods and wrapper classes native to C++. Because a BSTR may contain any characters, even internal nulls, it is possible to wrap arbitrary data in a BSTR to pass to another function (for example, to avoid having to write custom marshalling code for a COM interface)."}, {"tag": "p", "text": "ATL's CComBSTR is a light-weight wrapper class with adequate functionality for common tasks, and is a natural choice for ATL development. The _bstr_t class is more complicated, but cannot be avoided when using the #import directive and the wrapper functions it creates."}, {"tag": "h2", "text": "Using various strings"}, {"tag": "p", "text": "The L symbol before a character literal denotes that the character is a wide character, as in"}, {"tag": "p", "text": "wchar_t ch = L'a';"}, {"tag": "p", "text": "This designation is seldom necessary: the first 255 characters of Unicode are the same as ANSI. Had we left out the L in front of the first quote mark, the char 'a' would have been promoted to the wchar_t with the same value."}, {"tag": "p", "text": "The L symbol is also used to distinguish wchar_t strings from ordinary strings, as in"}, {"tag": "p", "text": "wchar_t wsz = L\"Unicode String\";"}, {"tag": "p", "text": "Windows provides the macros _T() and _TEXT() which do nothing unless _UNICODE is defined, in which case they each expand to L. Hence _T(\"John\") reverts to simply \"John\" in ANSI builds and expands to L\"John\" in Unicode builds. There is an analogous OLESTR macro that disappears if OLE2ANSI is defined and expands to L otherwise."}, {"tag": "p", "text": "For most of the Standard C library string routines, you can change the initial \"str\" in the name to \"wcs\" to determine the name of the corresponding routing for wide character strings. For example, wcscpy is the wide character counterpart of the venerable strcpy. Also, you may change \"str\" to \"_tsc\" to come up with the name of a corresponding TCHAR routine."}, {"tag": "p", "text": "Because a BSTR allocates memory before the location it nominally points to, a whole different API is necessary for working with BSTRs. For example, you may not initialize a BSTR by saying"}, {"tag": "p", "text": "BSTR b = L\"A String\";"}, {"tag": "p", "text": "This will correctly initialize b to a wide character array, but the byte count is uninitialized and so b is not a valid BSTR. The proper way to initialize b is"}, {"tag": "p", "text": "BSTR b = ::SysAllocString(L\"A String\");"}, {"tag": "p", "text": "Before b goes out of scope, its memory needs to be released by calling ::SysFreeString. Note that because the memory for BSTRs is allocated via a system call rather than the C++ new operator, memory leaks due to failing to call ::SysFreeString will not show up in the Visual C++ debugger. (NuMega's BoundsChecker will catch these leaks, however.)"}, {"tag": "p", "text": "Two other handy functions for working with BSTRs are ::SysAllocStringLen and ::SysStringLen. The former allocates a string to a given length and the latter is analogous to the Standard C strlen."}, {"tag": "p", "text": "The subtlest difficulty with using BSTRs is that they have conventions for their use that differ from those of other strings. For example, a NULL BSTR is treated as a valid, zero-length string unlike an ordinary string. The only place I have seen anyone attempt to codify these conventions is in Bruce McKinney's excellent article cited earlier. The reader is advised to study the section of his article entitled \"The Eight Rules of BSTR.\""}, {"tag": "p", "text": "The CComBSTR wrapper is straightforward to use. It does not have a lot of methods, but the ones it has are simple and self-explanatory. The _bstr_t class is more complex. It has more convenience functions. It reference-counts memory to avoid unnecessary copying and throws exceptions. CComBSTR does no reference counting and does not throw exceptions."}, {"tag": "h2", "text": "Conversions between types"}, {"tag": "p", "text": "Developers frequently work in the intersection of two or more cultures. You may be writing an OLE application using Standard C++, MFC and ATL. But OLE, Standard C++, MFC, and ATL represent four different cultures, each with its own preferred string type or string wrapper class. Therefore an important part of working with strings is knowing how to convert between the various manifestations."}, {"tag": "p", "text": "Because a BSTR is null-terminated and because its pointer points past the byte count, a BSTR \"is a\" (in an inheritance sort of sense) wide character string. You may pass a BSTR to a function expecting a wchar_t*. (Of course, if the BSTR being passed in contains any internal nulls, data after the first null will be lost in the interpretation as a wide character string.) However, this interchangeability with wide character strings is tricky. You cannot always look at a variable and tell whether a wchar_t* is merely a null-terminated wide character string or whether in fact it is a BSTR. The source code for _bstr_t is a good example. There is an operator _bstr_t::operator const wchar_t* which implies only that you may pass a _bstr_t to a function expecting a const wchar_t*. However, reading the implementation code, you discover that the const wchar_t* in question is actually a full-fledged BSTR. As McKinney points out, \"a BSTR is a BSTR by convention\" and not a built-in type that the compiler can check."}, {"tag": "p", "text": "The header file atlconv.h contains a whopping 28 conversion macros for converting between the various non-class string types covered in this article. These macros have the form X2Y. The source type X can be A, T, W, or OLE for ANSI, TCHAR, wchar_t or OLE respectively. The destination type Y can be any of these types or additionally BSTR. Except for BSTR, the destination types may optionally have a C in front of their type to indicate const. For example, A2CW takes an ANSI string and returns a constant wide character string. Of course, there are no macros for converting a type to itself. Note that there is no need for a BSTR source type because you may use a BSTR as a wide character string. Some of these macros require that you first call the macro USES_CONVERSION while others do not. Note that unlike most macros, USES_CONVERSION must be followed by a semicolon. Except when converting to a BSTR, these macros allocate memory on the stack; BSTRs are always allocated by a system call and must be freed using ::SysFreeString."}, {"tag": "p", "text": "CString defines a constructor and an operator= that each take an LPCTSTR argument. In particular, you can pass an LPCTSTR into a function taking a CString. CString also provides an operator LPCTSTR and so you can also pass a CString to a function expecting an LPCTSTR. CString has a method AllocSysString that produces a BSTR from its contents. Finally, CString can take a LPCWSTR (a const wchar_t*) as an argument to either a constructor or to operator=."}, {"tag": "p", "text": "The basic_string<T> class has constructor and operator= methods which take a const T* argument. However, you cannot pass a basic_string<T> to a function expecting a const T* because basic_string<> extracts to a character string via an operator called c_str() rather than via a type conversion operator."}, {"tag": "p", "text": "CComBSTR has both a constructor and an operator= which take a BSTR argument, as well as a type conversion operator for BSTR. Thus CComBSTR has roughly the same relationship with BSTR as CString has with LPCTSTR."}, {"tag": "p", "text": "The class _bstr_t has constructor and operator= overloads that take either ANSI or wide character strings. Also, it supports type conversion operators to both kinds of strings. As noted earlier, the type conversion operator for wide character strings actually returns a BSTR. Therefore you can pass or receive a _bstr_t as an ANSI string or a BSTR."}, {"tag": "h2", "text": "Conclusion"}, {"tag": "p", "text": "Developers these days have to contend with at least two character sets — ANSI and Unicode — and at least two memory representations — null terminated and count prepended. This alone makes multiple string types inevitable. Macros and wrapper classes simplify the situation in some circumstances, but they also add their own complexity."}, {"tag": "p", "text": "The Visual C++ developer stands in the intersection of a number of programming idioms — traditional C, Standard C++, MFC, COM, ATL — each with its own favorite string representation. You cannot avoid working with numerous string representations and converting from one to another. It is important to understand how each works and the implicit conventions for working with each type."}, {"tag": "h2", "text": "References"}, {"tag": "p", "text": "1. Bruce McKinney, Strings the OLE Way, available on MSDN. 2. Jim Beveridge, CString: Part of the plumbing behind MFC and a model for efficient design, Visual C++ Developers Journal, Volume 1 Number 4."}, {"tag": "h2", "text": "Sample code"}, {"tag": "pre", "text": "#include <afxpriv.h> // for USES_CONVERSION\n#include <comdef.h>  // for _bstr_t\n\nCString cs;                \nBSTR bstr;\nWCHAR wsz[81];\nCComBSTR cbstr;\nchar sz[81];\nTCHAR tsz[81];\nbasic_string<char> bs;\n_bstr_t _bstr;\n\nUSES_CONVERSION;\n\t\n// Convert CString to various types\ncs = \"String1\";\nbstr = cs.AllocSysString();     // BSTR\t\n_tcscpy(tsz, (LPCTSTR)cs);      // LPCTSTR\nstrcpy(sz, T2A(tsz));           // ANSI string\nwcscpy(wsz, bstr);              // wide string\ncbstr = bstr;                   // CComBSTR via \nbs = sz;                        // STL string\n_bstr = (LPCTSTR) cs;           // _bstr_t via either \n                                //     operator=(const char*) or\n                                //     operator=(const wchar_t*) \n                                //     if _UNICODE is defined.\n::SysFreeString(bstr);\n\n// Convert BSTR to various types\nbstr = ::SysAllocString(L\"String2\");\ncs = bstr;                      // CString via its LPCWSTR ctor\nwcscpy(wsz, bstr);              // Unicode\ncbstr = bstr;                   // CComBSTR via operator=(LPOLESTR)\nstrcpy(sz, W2A(bstr));          // ANSI string\nbs = sz;                        // STL string operator=(const T*)\n_tcscpy(tsz, W2T(bstr));        // LPTSTR \n_bstr = bstr;                   // _bstr_t via operator=(const wchar_t*)\n::SysFreeString(bstr);"}, {"tag": "p", "text": "Other C++ articles:"}, {"tag": "li", "text": "Regular expressions"}, {"tag": "li", "text": "Random number generation"}, {"tag": "li", "text": "Floating point exceptions"}, {"tag": "li", "text": "Math.h in Visual Studio, POSIX, and ISO"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}, {"tag": "p", "text": "jdc"}], "content": "Unraveling Strings in Visual C++\n\nThis is an article I wrote sometime in the late 1990s about working with strings coming from COM, MFC, Win32, the C++ Standard Library, etc. It does not include anything about .NET since it was written before .NET came out.\n\nOutline\n\nWhy so many strings? When is each string appropriate? Using various strings Conversions between types Conclusion References Sample code\n\nIntroduction\n\nIn the good old days, a string was a pointer to a null-terminated array of chars. Period. Now a string might be a char*, wchar_t*, LPCTSTR, BSTR, CString, basic_string, _bstr_t, CComBSTR, etc. Unfortunately, you cannot simply choose your favorite string representation and ignore the rest. Each representation has its own domain and it is frequently necessary to convert between types when crossing domain boundaries. Why are there so many kinds of strings? When is each one appropriate? How do you carry out common tasks with each? How do they relate to each other?\n\nWhy so many strings?\n\nStrings differ in three important ways: character set, memory layout, and conventions for use. The most obvious and simplest of these is character set. To keep things focused, we will limit ourselves to ANSI and Unicode. ANSI strings, the kind everybody grew up on, are arrays of single-byte characters. By far most of the world's strings are ANSI strings. So why bother with Unicode?\n\nEight bits are plenty to represent all the characters of ordinary English text. But with the slightest thought to international software, it quickly becomes apparent that eight bits are woefully inadequate. Unicode, with 16 bits per character, has enough possibilities to cover all the world's major languages with enough characters left over to even throw in a few ancient languages for good measure.\n\nWindows NT was built from the beginning to use Unicode strings exclusively internally, though you may write applications for NT that use either ANSI or Unicode. Windows CE only understands Unicode. OLE is built around Unicode strings. But Windows 3.x “doesn't know Unicode from a dress code, and never will [1].” The same is true of Windows 9x. The ANSI vs. Unicode strings are much like the English vs. metric measurement units: most everyone agrees the latter is the way to go, but the former has a tremendous installed base. In both situations, we will probably have to live with two standards and all the concomitant complications for a very long time.\n\nC++ has two built in character types: char and wchar_t. Most commonly a char is an ANSI character and a wchar_t is a Unicode character. This is not always the case, but to simplify things a bit, we will make this assumption. Wide character strings, i.e. strings of wchar_ts, are null-terminated arrays of characters, directly analogous with ordinary strings. The terminating null character in this case is a wchar_t null. Incidentally, the default settings for the Visual C++ debugger are to not display Unicode characters. There is a check box under Tools / Options / Debug labeled \"Display unicode strings\" which turns this on.\n\nIn order to be able to use the same source code for ANSI and Unicode builds, Windows introduced the TCHAR data type. TCHAR is simply a macro that expands to char in ANSI builds (i.e. _UNICODE is not defined) and wchar_t in Unicode builds (_UNICODE is defined). There are various string types based on the TCHAR macro, such as LPCTSTR (long pointer to a constant TCHAR string).\n\nMicrosoft also introduced a number of macros and typedefs with \"OLE\" in the name such as OLECHAR, LPOLESTR, etc. These are vestiges of an automatic ANSI / Unicode conversion scheme that Microsoft used prior to MFC 4.0 and has since abandoned. However, the names live on for legacy support and for Macintosh development. For example, if you look for help on CLSIDFromProgID you'll find that its first argument is an LPCOLESTR. For Win32 development, \"OLE\" corresponds to Unicode. For Win16 and for the Macintosh, the symbol OLE2ANSI is defined and \"OLE\" corresponds to ANSI. For example, in Win32 development, an OLECHAR is simply a wchar_t and an LPOLESTR is a wide character string.\n\nMicrosoft?s character and string types may be summarized as follows. A character name has the form XCHAR and string name has the form LPCXSTR where C is optional and X is either T, OLE, W, or empty. The C indicates a string type is constant, and the X has the following meanings:\n\nMFC introduced the CString class as a wrapper around LPCTSTR data type which provides methods for common tasks such as memory allocation and substring searches. A CString can be used in most circumstances where you would use an LPCTSTR.\n\nThe Standard C++ library provides a parameterized string class basic_string<T> where T is most often a char or wchar_t. The Standard library provides the typedefs string and wstring respectively for these common cases.\n\nThe real confusion in string types comes when we introduce BSTRs. A BSTR differs from a common string in that it always uses Unicode, regardless of compiler switches. However, it also has a different layout in memory. Furthermore, there are different conventions for using BSTRs than for using simple null-terminated string, whether of the ANSI or Unicode variety, and these conventions are seldom codified.\n\nA BSTR is a null-terminated Unicode string, but with a byte count (not character count!) prepended. An advantage of a byte-count prefix is that BSTR can contain internal nulls, whereas an ordinary string may not. One unusual aspect of the BSTR is that the byte count is not in the 0th entry of the array the BSTR points to. Instead, the byte count is stored in the two bytes preceding the memory the pointer ostensibly points to. (MFC?s CString uses a similar trick so that passing a CString involves no more overhead than passing a pointer [2]. This causes no problems for developers, however, because the implementation is thoroughly encapsulated.)\n\nOLE standardized on the BSTR partially because of OLE's desire to be language-independent. Many languages use the counted arrays rather than using a special symbol to mark the end of a string. The BSTR compromises by requiring both a count and a terminating character. (Note that in the context of string and character types, OLE refers only to character widths. In particular, an LPOLESTR is simply a wide character string and not a BSTR. Despite the name, an LPOLESTR is not OLE's favorite string!)\n\nBSTRs are an unnatural imposition on C++. However, they are unavoidable because OLE is built around BSTRs and not native C++ strings. In order to make BSTR manipulation easier from C++, several wrapper classes have been created. One is ATL's CComBSTR class, which handles basic memory management and a few basic operations for strings.\n\nThere is another BSTR wrapper which one must use in order to take advantage of the native COM support in the Visual C++ compiler. When you use the #import directive, the compiler creates wrapper functions for the methods on the imported COM interfaces. BSTR arguments and return values are wrapped as _bstr_t. (However, BSTR* arguments are left alone so the _bstr_t doesn't entirely eliminate the need to manipulate BSTRs.) The design goals of _bstr_t are different from that of CComBSTR. The former provides more convenience functions, and is implemented with reference counting to avoid unnecessary memory copying.\n\nWhen is each string appropriate?\n\nMFC class methods often take LPCTSTR arguments. The choice of a class wrapper for strings in MFC development is obviously CString especially because a CString can be used in most situations where an LPCTSTR is specified. The advantage of the CString class is that it provides many useful methods for memory management and string manipulation. One disadvantage is that CString carries with it a little bit more overhead than a raw LPCTSTR. Also, if CString is the only MFC class in a project, it still requires linking to and redistributing the MFC DLLs.\n\nThe Standard C++ basic_string<> has the advantage of being portable to non-Windows platforms. Also, you may explicitly decide between char and wchar_t strings on an individual basis rather than deciding once and for all based on a compiler switch as with TCHAR strings. And you could use basic_string<TCHAR> to maintain the ANSI vs. Unicode flexibility of CString. Like CString, basic_string<> does define a large number of convenient string manipulation functions. A design goal of this string class was to make the class sufficiently convenient and efficient that it would seldom be necessary to use null terminated strings and the C library manipulation functions.\n\nIn OLE interfaces, there is no choice but to use BSTR or one of its wrapper classes. Ordinarily, a C++ developer would use a BSTR only as a delivery vehicle to a COM interface; string manipulation is more easily done via library methods and wrapper classes native to C++. Because a BSTR may contain any characters, even internal nulls, it is possible to wrap arbitrary data in a BSTR to pass to another function (for example, to avoid having to write custom marshalling code for a COM interface).\n\nATL's CComBSTR is a light-weight wrapper class with adequate functionality for common tasks, and is a natural choice for ATL development. The _bstr_t class is more complicated, but cannot be avoided when using the #import directive and the wrapper functions it creates.\n\nUsing various strings\n\nThe L symbol before a character literal denotes that the character is a wide character, as in\n\nwchar_t ch = L'a';\n\nThis designation is seldom necessary: the first 255 characters of Unicode are the same as ANSI. Had we left out the L in front of the first quote mark, the char 'a' would have been promoted to the wchar_t with the same value.\n\nThe L symbol is also used to distinguish wchar_t strings from ordinary strings, as in\n\nwchar_t wsz = L\"Unicode String\";\n\nWindows provides the macros _T() and _TEXT() which do nothing unless _UNICODE is defined, in which case they each expand to L. Hence _T(\"John\") reverts to simply \"John\" in ANSI builds and expands to L\"John\" in Unicode builds. There is an analogous OLESTR macro that disappears if OLE2ANSI is defined and expands to L otherwise.\n\nFor most of the Standard C library string routines, you can change the initial \"str\" in the name to \"wcs\" to determine the name of the corresponding routing for wide character strings. For example, wcscpy is the wide character counterpart of the venerable strcpy. Also, you may change \"str\" to \"_tsc\" to come up with the name of a corresponding TCHAR routine.\n\nBecause a BSTR allocates memory before the location it nominally points to, a whole different API is necessary for working with BSTRs. For example, you may not initialize a BSTR by saying\n\nBSTR b = L\"A String\";\n\nThis will correctly initialize b to a wide character array, but the byte count is uninitialized and so b is not a valid BSTR. The proper way to initialize b is\n\nBSTR b = ::SysAllocString(L\"A String\");\n\nBefore b goes out of scope, its memory needs to be released by calling ::SysFreeString. Note that because the memory for BSTRs is allocated via a system call rather than the C++ new operator, memory leaks due to failing to call ::SysFreeString will not show up in the Visual C++ debugger. (NuMega's BoundsChecker will catch these leaks, however.)\n\nTwo other handy functions for working with BSTRs are ::SysAllocStringLen and ::SysStringLen. The former allocates a string to a given length and the latter is analogous to the Standard C strlen.\n\nThe subtlest difficulty with using BSTRs is that they have conventions for their use that differ from those of other strings. For example, a NULL BSTR is treated as a valid, zero-length string unlike an ordinary string. The only place I have seen anyone attempt to codify these conventions is in Bruce McKinney's excellent article cited earlier. The reader is advised to study the section of his article entitled \"The Eight Rules of BSTR.\"\n\nThe CComBSTR wrapper is straightforward to use. It does not have a lot of methods, but the ones it has are simple and self-explanatory. The _bstr_t class is more complex. It has more convenience functions. It reference-counts memory to avoid unnecessary copying and throws exceptions. CComBSTR does no reference counting and does not throw exceptions.\n\nConversions between types\n\nDevelopers frequently work in the intersection of two or more cultures. You may be writing an OLE application using Standard C++, MFC and ATL. But OLE, Standard C++, MFC, and ATL represent four different cultures, each with its own preferred string type or string wrapper class. Therefore an important part of working with strings is knowing how to convert between the various manifestations.\n\nBecause a BSTR is null-terminated and because its pointer points past the byte count, a BSTR \"is a\" (in an inheritance sort of sense) wide character string. You may pass a BSTR to a function expecting a wchar_t*. (Of course, if the BSTR being passed in contains any internal nulls, data after the first null will be lost in the interpretation as a wide character string.) However, this interchangeability with wide character strings is tricky. You cannot always look at a variable and tell whether a wchar_t* is merely a null-terminated wide character string or whether in fact it is a BSTR. The source code for _bstr_t is a good example. There is an operator _bstr_t::operator const wchar_t* which implies only that you may pass a _bstr_t to a function expecting a const wchar_t*. However, reading the implementation code, you discover that the const wchar_t* in question is actually a full-fledged BSTR. As McKinney points out, \"a BSTR is a BSTR by convention\" and not a built-in type that the compiler can check.\n\nThe header file atlconv.h contains a whopping 28 conversion macros for converting between the various non-class string types covered in this article. These macros have the form X2Y. The source type X can be A, T, W, or OLE for ANSI, TCHAR, wchar_t or OLE respectively. The destination type Y can be any of these types or additionally BSTR. Except for BSTR, the destination types may optionally have a C in front of their type to indicate const. For example, A2CW takes an ANSI string and returns a constant wide character string. Of course, there are no macros for converting a type to itself. Note that there is no need for a BSTR source type because you may use a BSTR as a wide character string. Some of these macros require that you first call the macro USES_CONVERSION while others do not. Note that unlike most macros, USES_CONVERSION must be followed by a semicolon. Except when converting to a BSTR, these macros allocate memory on the stack; BSTRs are always allocated by a system call and must be freed using ::SysFreeString.\n\nCString defines a constructor and an operator= that each take an LPCTSTR argument. In particular, you can pass an LPCTSTR into a function taking a CString. CString also provides an operator LPCTSTR and so you can also pass a CString to a function expecting an LPCTSTR. CString has a method AllocSysString that produces a BSTR from its contents. Finally, CString can take a LPCWSTR (a const wchar_t*) as an argument to either a constructor or to operator=.\n\nThe basic_string<T> class has constructor and operator= methods which take a const T* argument. However, you cannot pass a basic_string<T> to a function expecting a const T* because basic_string<> extracts to a character string via an operator called c_str() rather than via a type conversion operator.\n\nCComBSTR has both a constructor and an operator= which take a BSTR argument, as well as a type conversion operator for BSTR. Thus CComBSTR has roughly the same relationship with BSTR as CString has with LPCTSTR.\n\nThe class _bstr_t has constructor and operator= overloads that take either ANSI or wide character strings. Also, it supports type conversion operators to both kinds of strings. As noted earlier, the type conversion operator for wide character strings actually returns a BSTR. Therefore you can pass or receive a _bstr_t as an ANSI string or a BSTR.\n\nConclusion\n\nDevelopers these days have to contend with at least two character sets — ANSI and Unicode — and at least two memory representations — null terminated and count prepended. This alone makes multiple string types inevitable. Macros and wrapper classes simplify the situation in some circumstances, but they also add their own complexity.\n\nThe Visual C++ developer stands in the intersection of a number of programming idioms — traditional C, Standard C++, MFC, COM, ATL — each with its own favorite string representation. You cannot avoid working with numerous string representations and converting from one to another. It is important to understand how each works and the implicit conventions for working with each type.\n\nReferences\n\n1. Bruce McKinney, Strings the OLE Way, available on MSDN. 2. Jim Beveridge, CString: Part of the plumbing behind MFC and a model for efficient design, Visual C++ Developers Journal, Volume 1 Number 4.\n\nSample code\n\n#include <afxpriv.h> // for USES_CONVERSION\n#include <comdef.h>  // for _bstr_t\n\nCString cs;                \nBSTR bstr;\nWCHAR wsz[81];\nCComBSTR cbstr;\nchar sz[81];\nTCHAR tsz[81];\nbasic_string<char> bs;\n_bstr_t _bstr;\n\nUSES_CONVERSION;\n\t\n// Convert CString to various types\ncs = \"String1\";\nbstr = cs.AllocSysString();     // BSTR\t\n_tcscpy(tsz, (LPCTSTR)cs);      // LPCTSTR\nstrcpy(sz, T2A(tsz));           // ANSI string\nwcscpy(wsz, bstr);              // wide string\ncbstr = bstr;                   // CComBSTR via \nbs = sz;                        // STL string\n_bstr = (LPCTSTR) cs;           // _bstr_t via either \n                                //     operator=(const char*) or\n                                //     operator=(const wchar_t*) \n                                //     if _UNICODE is defined.\n::SysFreeString(bstr);\n\n// Convert BSTR to various types\nbstr = ::SysAllocString(L\"String2\");\ncs = bstr;                      // CString via its LPCWSTR ctor\nwcscpy(wsz, bstr);              // Unicode\ncbstr = bstr;                   // CComBSTR via operator=(LPOLESTR)\nstrcpy(sz, W2A(bstr));          // ANSI string\nbs = sz;                        // STL string operator=(const T*)\n_tcscpy(tsz, W2T(bstr));        // LPTSTR \n_bstr = bstr;                   // _bstr_t via operator=(const wchar_t*)\n::SysFreeString(bstr);\n\nOther C++ articles:\n\nRegular expressions\n\nRandom number generation\n\nFloating point exceptions\n\nMath.h in Visual Studio, POSIX, and ISO\n\nHome\n\nBlog\n\nConsulting\n\nContact\n\njdc"}
{"slug": "cpp_erf", "canonical_url": "https://www.johndcook.com/blog/cpp_erf/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/cpp_erf.html", "title": "Stand-alone C++ implementation of the error function", "heading": "Stand-alone C++ implementation of the error function erf(x)", "description": "Stand-alone C++ function for computing the error function erf(x).", "summary": "The following code first appeared as Python code in my blog post Stand-alone error function erf. See that post for documentation. See also Relating erf and Φ.", "word_count": 237, "blocks": [{"tag": "h1", "text": "Stand-alone C++ implementation of the error function erf(x)"}, {"tag": "p", "text": "The following code first appeared as Python code in my blog post Stand-alone error function erf. See that post for documentation. See also Relating erf and Φ."}, {"tag": "pre", "text": "#include <cmath>\n\ndouble erf(double x)\n{\n    // constants\n    double a1 =  0.254829592;\n    double a2 = -0.284496736;\n    double a3 =  1.421413741;\n    double a4 = -1.453152027;\n    double a5 =  1.061405429;\n    double p  =  0.3275911;\n\n    // Save the sign of x\n    int sign = 1;\n    if (x < 0)\n        sign = -1;\n    x = fabs(x);\n\n    // A&S formula 7.1.26\n    double t = 1.0/(1.0 + p*x);\n    double y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*exp(-x*x);\n\n    return sign*y;\n}\n\nvoid testErf()\n{\n    // Select a few input values\n    double x[] = \n    {\n        -3, \n        -1, \n        0.0, \n        0.5, \n        2.1 \n    };\n\n    // Output computed by Mathematica\n    // y = Erf[x]\n    double y[] = \n    { \n        -0.999977909503, \n        -0.842700792950, \n        0.0, \n        0.520499877813, \n        0.997020533344 \n    };\n\n    int numTests = sizeof(x)/sizeof(double);\n\n    double maxError = 0.0;\n    for (int i = 0; i < numTests; ++i)\n    {\n        double error = fabs(y[i] - erf(x[i]));\n        if (error > maxError)\n            maxError = error;\n    }\n\n    std::cout << \"Maximum error: \" << maxError << \"\\n\";\n}"}, {"tag": "p", "text": "A&S refers to Handbook of Mathematical Functions by Abramowitz and Stegun. See Stand-alone error function for details of the algorithm."}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want with it, no strings attached."}, {"tag": "p", "text": "Other versions: C#, Python"}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Stand-alone C++ implementation of the error function erf(x)\n\nThe following code first appeared as Python code in my blog post Stand-alone error function erf. See that post for documentation. See also Relating erf and Φ.\n\n#include <cmath>\n\ndouble erf(double x)\n{\n    // constants\n    double a1 =  0.254829592;\n    double a2 = -0.284496736;\n    double a3 =  1.421413741;\n    double a4 = -1.453152027;\n    double a5 =  1.061405429;\n    double p  =  0.3275911;\n\n    // Save the sign of x\n    int sign = 1;\n    if (x < 0)\n        sign = -1;\n    x = fabs(x);\n\n    // A&S formula 7.1.26\n    double t = 1.0/(1.0 + p*x);\n    double y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*exp(-x*x);\n\n    return sign*y;\n}\n\nvoid testErf()\n{\n    // Select a few input values\n    double x[] = \n    {\n        -3, \n        -1, \n        0.0, \n        0.5, \n        2.1 \n    };\n\n    // Output computed by Mathematica\n    // y = Erf[x]\n    double y[] = \n    { \n        -0.999977909503, \n        -0.842700792950, \n        0.0, \n        0.520499877813, \n        0.997020533344 \n    };\n\n    int numTests = sizeof(x)/sizeof(double);\n\n    double maxError = 0.0;\n    for (int i = 0; i < numTests; ++i)\n    {\n        double error = fabs(y[i] - erf(x[i]));\n        if (error > maxError)\n            maxError = error;\n    }\n\n    std::cout << \"Maximum error: \" << maxError << \"\\n\";\n}\n\nA&S refers to Handbook of Mathematical Functions by Abramowitz and Stegun. See Stand-alone error function for details of the algorithm.\n\nThis code is in the public domain. Do whatever you want with it, no strings attached.\n\nOther versions: C#, Python\n\nStand-alone numerical code"}
{"slug": "cpp_expm1", "canonical_url": "https://www.johndcook.com/blog/cpp_expm1/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/cpp_expm1.html", "title": "Stand-alone C++ implementation of expm1", "heading": "Stand-alone C++ code for exp(x) - 1", "description": "Stand-alone C++ code for computing exp(x) - 1, a.k.a. expm1(x), accurate even for small values of x where direct computation is not accurate.", "summary": "If x is very small, directly computing exp(x) - 1 can be inaccurate. Numerical libraries often include a function expm1 to compute this function. The need for such a function is easiest to see when x is extremely small. If x is small enough, exp(x) = 1 in machine arithmetic and so exp(x) - 1 returns 0 even though the ...", "word_count": 355, "blocks": [{"tag": "h1", "text": "Stand-alone C++ code for exp(x) - 1"}, {"tag": "p", "text": "If x is very small, directly computing exp(x) - 1 can be inaccurate. Numerical libraries often include a function expm1 to compute this function. The need for such a function is easiest to see when x is extremely small. If x is small enough, exp(x) = 1 in machine arithmetic and so exp(x) - 1 returns 0 even though the correct result is positive. All precision is lost. If x is small but not so extremely small, direct computation still loses precision, just not as much."}, {"tag": "p", "text": "We can avoid the loss of precision by using a Taylor series to evaluate exp(x):"}, {"tag": "p", "text": "exp(x) = 1 + x + x2/2 + x36 + ..."}, {"tag": "p", "text": "If |x| < 10-5, the error in approximating exp(x) - 1 by x + x2/2 is on the order of 10-15 and so the relative error is on the order of 10-10 or better. If we compute exp(10-5) - 1 directly, the absolute error is about 10-16 and so the relative error is about 10-11. So by using the two-term Taylor approximation for |x| less than 10-5 and the direct method for |x| larger than 10-5, we obtain at least 10 significant figures for all inputs."}, {"tag": "pre", "text": "#include <cmath>\n#include <iostream>\n\n// Compute exp(x) - 1 without loss of precision for small values of x.\ndouble expm1(double x)\n{\n\tif (fabs(x) < 1e-5)\n\t\treturn x + 0.5*x*x;\n\telse\n\t\treturn exp(x) - 1.0;\n}\n\nvoid testExpm1()\n{\n    // Select a few input values\n    double x[] = \n    {\n        -1, \n        0.0, \n        1e-5 - 1e-8, \n        1e-5 + 1e-8,\n\t\t0.5\n    };\n\n    // Output computed by Mathematica\n    // y = Exp[x] - 1\n    double y[] = \n    { \n       -0.632120558828558, \n        0.0, \n        0.000009990049900216168, \n        0.00001001005010021717, \n        0.6487212707001282 \n    };\n\n\tint numTests = sizeof(x)/sizeof(double);\n\n    double maxError = 0.0;\n    for (int i = 0; i < numTests; ++i)\n    {\n        double error = fabs(y[i] - expm1(x[i]));\n        if (error > maxError)\n            maxError = error;\n    }\n\n\tstd::cout << \"Maximum error: \" << maxError << \"\\n\";\n}"}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want to with it, no strings attached."}, {"tag": "p", "text": "Other versions of the same code: Python, C#"}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Stand-alone C++ code for exp(x) - 1\n\nIf x is very small, directly computing exp(x) - 1 can be inaccurate. Numerical libraries often include a function expm1 to compute this function. The need for such a function is easiest to see when x is extremely small. If x is small enough, exp(x) = 1 in machine arithmetic and so exp(x) - 1 returns 0 even though the correct result is positive. All precision is lost. If x is small but not so extremely small, direct computation still loses precision, just not as much.\n\nWe can avoid the loss of precision by using a Taylor series to evaluate exp(x):\n\nexp(x) = 1 + x + x2/2 + x36 + ...\n\nIf |x| < 10-5, the error in approximating exp(x) - 1 by x + x2/2 is on the order of 10-15 and so the relative error is on the order of 10-10 or better. If we compute exp(10-5) - 1 directly, the absolute error is about 10-16 and so the relative error is about 10-11. So by using the two-term Taylor approximation for |x| less than 10-5 and the direct method for |x| larger than 10-5, we obtain at least 10 significant figures for all inputs.\n\n#include <cmath>\n#include <iostream>\n\n// Compute exp(x) - 1 without loss of precision for small values of x.\ndouble expm1(double x)\n{\n\tif (fabs(x) < 1e-5)\n\t\treturn x + 0.5*x*x;\n\telse\n\t\treturn exp(x) - 1.0;\n}\n\nvoid testExpm1()\n{\n    // Select a few input values\n    double x[] = \n    {\n        -1, \n        0.0, \n        1e-5 - 1e-8, \n        1e-5 + 1e-8,\n\t\t0.5\n    };\n\n    // Output computed by Mathematica\n    // y = Exp[x] - 1\n    double y[] = \n    { \n       -0.632120558828558, \n        0.0, \n        0.000009990049900216168, \n        0.00001001005010021717, \n        0.6487212707001282 \n    };\n\n\tint numTests = sizeof(x)/sizeof(double);\n\n    double maxError = 0.0;\n    for (int i = 0; i < numTests; ++i)\n    {\n        double error = fabs(y[i] - expm1(x[i]));\n        if (error > maxError)\n            maxError = error;\n    }\n\n\tstd::cout << \"Maximum error: \" << maxError << \"\\n\";\n}\n\nThis code is in the public domain. Do whatever you want to with it, no strings attached.\n\nOther versions of the same code: Python, C#\n\nStand-alone numerical code"}
{"slug": "cpp_log_factorial", "canonical_url": "https://www.johndcook.com/blog/cpp_log_factorial/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/cpp_log_factorial.html", "title": "C++ log factorial code", "heading": "C++ code for computing log factorial", "description": "Stand-alone C++ code for computing log factorial.", "summary": "For an explanation of this code, see How to compute log factorial.", "word_count": 367, "blocks": [{"tag": "h1", "text": "C++ code for computing log factorial"}, {"tag": "p", "text": "For an explanation of this code, see How to compute log factorial."}, {"tag": "pre", "text": "#include <cmath>\n    #include <sstream>\n    #include <stdexcept>\n    \n    double LogFactorial(int n)\n    {\n\n        if (n < 0)\n        {\n            std::stringstream os;\n            os << \"Invalid input argument (\" << n \n            << \"); may not be negative\";\n            throw std::invalid_argument( os.str() );\n            \n        }\n        else if (n > 254)\n        {\n            const double PI = 3.141592653589793;\n            double x = n + 1;\n            return (x - 0.5)*log(x) - x + 0.5*log(2*PI) + 1.0/(12.0*x);\n        }\n        else\n        {\n            double lf[] =        \n            {\n                0.000000000000000,\n                0.000000000000000,\n                0.693147180559945,\n                1.791759469228055,\n                3.178053830347946,\n                4.787491742782046,\n                6.579251212010101,\n                8.525161361065415,\n                10.604602902745251,\n                12.801827480081469,\n                15.104412573075516,\n                17.502307845873887,\n                19.987214495661885,\n                22.552163853123421,\n                25.191221182738683,\n                27.899271383840894,\n                30.671860106080675,\n                33.505073450136891,\n                36.395445208033053,\n                39.339884187199495,\n                42.335616460753485,\n                45.380138898476908,\n                48.471181351835227,\n                51.606675567764377,\n                54.784729398112319,\n                58.003605222980518,\n                61.261701761002001,\n                64.557538627006323,\n                67.889743137181526,\n                71.257038967168000,\n                74.658236348830158,\n                78.092223553315307,\n                81.557959456115029,\n                85.054467017581516,\n                88.580827542197682,\n                92.136175603687079,\n                95.719694542143202,\n                99.330612454787428,\n                102.968198614513810,\n                106.631760260643450,\n                110.320639714757390,\n                114.034211781461690,\n                117.771881399745060,\n                121.533081515438640,\n                125.317271149356880,\n                129.123933639127240,\n                132.952575035616290,\n                136.802722637326350,\n                140.673923648234250,\n                144.565743946344900,\n                148.477766951773020,\n                152.409592584497350,\n                156.360836303078800,\n                160.331128216630930,\n                164.320112263195170,\n                168.327445448427650,\n                172.352797139162820,\n                176.395848406997370,\n                180.456291417543780,\n                184.533828861449510,\n                188.628173423671600,\n                192.739047287844900,\n                196.866181672889980,\n                201.009316399281570,\n                205.168199482641200,\n                209.342586752536820,\n                213.532241494563270,\n                217.736934113954250,\n                221.956441819130360,\n                226.190548323727570,\n                230.439043565776930,\n                234.701723442818260,\n                238.978389561834350,\n                243.268849002982730,\n                247.572914096186910,\n                251.890402209723190,\n                256.221135550009480,\n                260.564940971863220,\n                264.921649798552780,\n                269.291097651019810,\n                273.673124285693690,\n                278.067573440366120,\n                282.474292687630400,\n                286.893133295426990,\n                291.323950094270290,\n                295.766601350760600,\n                300.220948647014100,\n                304.686856765668720,\n                309.164193580146900,\n                313.652829949878990,\n                318.152639620209300,\n                322.663499126726210,\n                327.185287703775200,\n                331.717887196928470,\n                336.261181979198450,\n                340.815058870798960,\n                345.379407062266860,\n                349.954118040770250,\n                354.539085519440790,\n                359.134205369575340,\n                363.739375555563470,\n                368.354496072404690,\n                372.979468885689020,\n                377.614197873918670,\n                382.258588773060010,\n                386.912549123217560,\n                391.575988217329610,\n                396.248817051791490,\n                400.930948278915760,\n                405.622296161144900,\n                410.322776526937280,\n                415.032306728249580,\n                419.750805599544780,\n                424.478193418257090,\n                429.214391866651570,\n                433.959323995014870,\n                438.712914186121170,\n                443.475088120918940,\n                448.245772745384610,\n                453.024896238496130,\n                457.812387981278110,\n                462.608178526874890,\n                467.412199571608080,\n                472.224383926980520,\n                477.044665492585580,\n                481.872979229887900,\n                486.709261136839360,\n                491.553448223298010,\n                496.405478487217580,\n                501.265290891579240,\n                506.132825342034830,\n                511.008022665236070,\n                515.890824587822520,\n                520.781173716044240,\n                525.679013515995050,\n                530.584288294433580,\n                535.496943180169520,\n                540.416924105997740,\n                545.344177791154950,\n                550.278651724285620,\n                555.220294146894960,\n                560.169054037273100,\n                565.124881094874350,\n                570.087725725134190,\n                575.057539024710200,\n                580.034272767130800,\n                585.017879388839220,\n                590.008311975617860,\n                595.005524249382010,\n                600.009470555327430,\n                605.020105849423770,\n                610.037385686238740,\n                615.061266207084940,\n                620.091704128477430,\n                625.128656730891070,\n                630.172081847810200,\n                635.221937855059760,\n                640.278183660408100,\n                645.340778693435030,\n                650.409682895655240,\n                655.484856710889060,\n                660.566261075873510,\n                665.653857411105950,\n                670.747607611912710,\n                675.847474039736880,\n                680.953419513637530,\n                686.065407301994010,\n                691.183401114410800,\n                696.307365093814040,\n                701.437263808737160,\n                706.573062245787470,\n                711.714725802289990,\n                716.862220279103440,\n                722.015511873601330,\n                727.174567172815840,\n                732.339353146739310,\n                737.509837141777440,\n                742.685986874351220,\n                747.867770424643370,\n                753.055156230484160,\n                758.248113081374300,\n                763.446610112640200,\n                768.650616799717000,\n                773.860102952558460,\n                779.075038710167410,\n                784.295394535245690,\n                789.521141208958970,\n                794.752249825813460,\n                799.988691788643450,\n                805.230438803703120,\n                810.477462875863580,\n                815.729736303910160,\n                820.987231675937890,\n                826.249921864842800,\n                831.517780023906310,\n                836.790779582469900,\n                842.068894241700490,\n                847.352097970438420,\n                852.640365001133090,\n                857.933669825857460,\n                863.231987192405430,\n                868.535292100464630,\n                873.843559797865740,\n                879.156765776907600,\n                884.474885770751830,\n                889.797895749890240,\n                895.125771918679900,\n                900.458490711945270,\n                905.796028791646340,\n                911.138363043611210,\n                916.485470574328820,\n                921.837328707804890,\n                927.193914982476710,\n                932.555207148186240,\n                937.921183163208070,\n                943.291821191335660,\n                948.667099599019820,\n                954.046996952560450,\n                959.431492015349480,\n                964.820563745165940,\n                970.214191291518320,\n                975.612353993036210,\n                981.015031374908400,\n                986.422203146368590,\n                991.833849198223450,\n                997.249949600427840,\n                1002.670484599700300,\n                1008.095434617181700,\n                1013.524780246136200,\n                1018.958502249690200,\n                1024.396581558613400,\n                1029.838999269135500,\n                1035.285736640801600,\n                1040.736775094367400,\n                1046.192096209724900,\n                1051.651681723869200,\n                1057.115513528895000,\n                1062.583573670030100,\n                1068.055844343701400,\n                1073.532307895632800,\n                1079.012946818975000,\n                1084.497743752465600,\n                1089.986681478622400,\n                1095.479742921962700,\n                1100.976911147256000,\n                1106.478169357800900,\n                1111.983500893733000,\n                1117.492889230361000,\n                1123.006317976526100,\n                1128.523770872990800,\n                1134.045231790853000,\n                1139.570684729984800,\n                1145.100113817496100,\n                1150.633503306223700,\n                1156.170837573242400,\n            };\n            return lf[n];\n        }\n    }"}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want with it, no strings attached."}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "C++ code for computing log factorial\n\nFor an explanation of this code, see How to compute log factorial.\n\n#include <cmath>\n    #include <sstream>\n    #include <stdexcept>\n    \n    double LogFactorial(int n)\n    {\n\n        if (n < 0)\n        {\n            std::stringstream os;\n            os << \"Invalid input argument (\" << n \n            << \"); may not be negative\";\n            throw std::invalid_argument( os.str() );\n            \n        }\n        else if (n > 254)\n        {\n            const double PI = 3.141592653589793;\n            double x = n + 1;\n            return (x - 0.5)*log(x) - x + 0.5*log(2*PI) + 1.0/(12.0*x);\n        }\n        else\n        {\n            double lf[] =        \n            {\n                0.000000000000000,\n                0.000000000000000,\n                0.693147180559945,\n                1.791759469228055,\n                3.178053830347946,\n                4.787491742782046,\n                6.579251212010101,\n                8.525161361065415,\n                10.604602902745251,\n                12.801827480081469,\n                15.104412573075516,\n                17.502307845873887,\n                19.987214495661885,\n                22.552163853123421,\n                25.191221182738683,\n                27.899271383840894,\n                30.671860106080675,\n                33.505073450136891,\n                36.395445208033053,\n                39.339884187199495,\n                42.335616460753485,\n                45.380138898476908,\n                48.471181351835227,\n                51.606675567764377,\n                54.784729398112319,\n                58.003605222980518,\n                61.261701761002001,\n                64.557538627006323,\n                67.889743137181526,\n                71.257038967168000,\n                74.658236348830158,\n                78.092223553315307,\n                81.557959456115029,\n                85.054467017581516,\n                88.580827542197682,\n                92.136175603687079,\n                95.719694542143202,\n                99.330612454787428,\n                102.968198614513810,\n                106.631760260643450,\n                110.320639714757390,\n                114.034211781461690,\n                117.771881399745060,\n                121.533081515438640,\n                125.317271149356880,\n                129.123933639127240,\n                132.952575035616290,\n                136.802722637326350,\n                140.673923648234250,\n                144.565743946344900,\n                148.477766951773020,\n                152.409592584497350,\n                156.360836303078800,\n                160.331128216630930,\n                164.320112263195170,\n                168.327445448427650,\n                172.352797139162820,\n                176.395848406997370,\n                180.456291417543780,\n                184.533828861449510,\n                188.628173423671600,\n                192.739047287844900,\n                196.866181672889980,\n                201.009316399281570,\n                205.168199482641200,\n                209.342586752536820,\n                213.532241494563270,\n                217.736934113954250,\n                221.956441819130360,\n                226.190548323727570,\n                230.439043565776930,\n                234.701723442818260,\n                238.978389561834350,\n                243.268849002982730,\n                247.572914096186910,\n                251.890402209723190,\n                256.221135550009480,\n                260.564940971863220,\n                264.921649798552780,\n                269.291097651019810,\n                273.673124285693690,\n                278.067573440366120,\n                282.474292687630400,\n                286.893133295426990,\n                291.323950094270290,\n                295.766601350760600,\n                300.220948647014100,\n                304.686856765668720,\n                309.164193580146900,\n                313.652829949878990,\n                318.152639620209300,\n                322.663499126726210,\n                327.185287703775200,\n                331.717887196928470,\n                336.261181979198450,\n                340.815058870798960,\n                345.379407062266860,\n                349.954118040770250,\n                354.539085519440790,\n                359.134205369575340,\n                363.739375555563470,\n                368.354496072404690,\n                372.979468885689020,\n                377.614197873918670,\n                382.258588773060010,\n                386.912549123217560,\n                391.575988217329610,\n                396.248817051791490,\n                400.930948278915760,\n                405.622296161144900,\n                410.322776526937280,\n                415.032306728249580,\n                419.750805599544780,\n                424.478193418257090,\n                429.214391866651570,\n                433.959323995014870,\n                438.712914186121170,\n                443.475088120918940,\n                448.245772745384610,\n                453.024896238496130,\n                457.812387981278110,\n                462.608178526874890,\n                467.412199571608080,\n                472.224383926980520,\n                477.044665492585580,\n                481.872979229887900,\n                486.709261136839360,\n                491.553448223298010,\n                496.405478487217580,\n                501.265290891579240,\n                506.132825342034830,\n                511.008022665236070,\n                515.890824587822520,\n                520.781173716044240,\n                525.679013515995050,\n                530.584288294433580,\n                535.496943180169520,\n                540.416924105997740,\n                545.344177791154950,\n                550.278651724285620,\n                555.220294146894960,\n                560.169054037273100,\n                565.124881094874350,\n                570.087725725134190,\n                575.057539024710200,\n                580.034272767130800,\n                585.017879388839220,\n                590.008311975617860,\n                595.005524249382010,\n                600.009470555327430,\n                605.020105849423770,\n                610.037385686238740,\n                615.061266207084940,\n                620.091704128477430,\n                625.128656730891070,\n                630.172081847810200,\n                635.221937855059760,\n                640.278183660408100,\n                645.340778693435030,\n                650.409682895655240,\n                655.484856710889060,\n                660.566261075873510,\n                665.653857411105950,\n                670.747607611912710,\n                675.847474039736880,\n                680.953419513637530,\n                686.065407301994010,\n                691.183401114410800,\n                696.307365093814040,\n                701.437263808737160,\n                706.573062245787470,\n                711.714725802289990,\n                716.862220279103440,\n                722.015511873601330,\n                727.174567172815840,\n                732.339353146739310,\n                737.509837141777440,\n                742.685986874351220,\n                747.867770424643370,\n                753.055156230484160,\n                758.248113081374300,\n                763.446610112640200,\n                768.650616799717000,\n                773.860102952558460,\n                779.075038710167410,\n                784.295394535245690,\n                789.521141208958970,\n                794.752249825813460,\n                799.988691788643450,\n                805.230438803703120,\n                810.477462875863580,\n                815.729736303910160,\n                820.987231675937890,\n                826.249921864842800,\n                831.517780023906310,\n                836.790779582469900,\n                842.068894241700490,\n                847.352097970438420,\n                852.640365001133090,\n                857.933669825857460,\n                863.231987192405430,\n                868.535292100464630,\n                873.843559797865740,\n                879.156765776907600,\n                884.474885770751830,\n                889.797895749890240,\n                895.125771918679900,\n                900.458490711945270,\n                905.796028791646340,\n                911.138363043611210,\n                916.485470574328820,\n                921.837328707804890,\n                927.193914982476710,\n                932.555207148186240,\n                937.921183163208070,\n                943.291821191335660,\n                948.667099599019820,\n                954.046996952560450,\n                959.431492015349480,\n                964.820563745165940,\n                970.214191291518320,\n                975.612353993036210,\n                981.015031374908400,\n                986.422203146368590,\n                991.833849198223450,\n                997.249949600427840,\n                1002.670484599700300,\n                1008.095434617181700,\n                1013.524780246136200,\n                1018.958502249690200,\n                1024.396581558613400,\n                1029.838999269135500,\n                1035.285736640801600,\n                1040.736775094367400,\n                1046.192096209724900,\n                1051.651681723869200,\n                1057.115513528895000,\n                1062.583573670030100,\n                1068.055844343701400,\n                1073.532307895632800,\n                1079.012946818975000,\n                1084.497743752465600,\n                1089.986681478622400,\n                1095.479742921962700,\n                1100.976911147256000,\n                1106.478169357800900,\n                1111.983500893733000,\n                1117.492889230361000,\n                1123.006317976526100,\n                1128.523770872990800,\n                1134.045231790853000,\n                1139.570684729984800,\n                1145.100113817496100,\n                1150.633503306223700,\n                1156.170837573242400,\n            };\n            return lf[n];\n        }\n    }\n\nThis code is in the public domain. Do whatever you want with it, no strings attached.\n\nStand-alone numerical code"}
{"slug": "cpp_phi", "canonical_url": "https://www.johndcook.com/blog/cpp_phi/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/cpp_phi.html", "title": "Stand-alone C++ implementation of the Phi function (Gaussian CDF)", "heading": "Stand-alone C++ implementation of Φ(x)", "description": "Stand-alone C++ function for computing the function Phi(x) where Phi is the normal (Gaussian) distribution function.", "summary": "The function Φ(x) is the cumulative density function (CDF) of a standard normal (Gaussian) random variable. It is closely related to the error function erf(x).", "word_count": 234, "blocks": [{"tag": "h1", "text": "Stand-alone C++ implementation of Φ(x)"}, {"tag": "p", "text": "The function Φ(x) is the cumulative density function (CDF) of a standard normal (Gaussian) random variable. It is closely related to the error function erf(x)."}, {"tag": "pre", "text": "#include <cmath>\n\ndouble phi(double x)\n{\n    // constants\n    double a1 =  0.254829592;\n    double a2 = -0.284496736;\n    double a3 =  1.421413741;\n    double a4 = -1.453152027;\n    double a5 =  1.061405429;\n    double p  =  0.3275911;\n\n    // Save the sign of x\n    int sign = 1;\n    if (x < 0)\n        sign = -1;\n    x = fabs(x)/sqrt(2.0);\n\n    // A&S formula 7.1.26\n    double t = 1.0/(1.0 + p*x);\n    double y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*exp(-x*x);\n\n    return 0.5*(1.0 + sign*y);\n}\n\nvoid testPhi()\n{\n    // Select a few input values\n    double x[] = \n    {\n        -3, \n        -1, \n        0.0, \n        0.5, \n        2.1 \n    };\n\n    // Output computed by Mathematica\n    // y = Phi[x]\n    double y[] = \n    { \n        0.00134989803163, \n        0.158655253931, \n        0.5, \n        0.691462461274, \n        0.982135579437 \n    };\n\n\tint numTests = sizeof(x)/sizeof(double);\n\n    double maxError = 0.0;\n    for (int i = 0; i < numTests; ++i)\n    {\n        double error = fabs(y[i] - phi(x[i]));\n        if (error > maxError)\n            maxError = error;\n    }\n\n\tstd::cout << \"Maximum error: \" << maxError << \"\\n\";\n}"}, {"tag": "p", "text": "A&S refers to Handbook of Mathematical Functions by Abramowitz and Stegun. See Stand-alone error function for details of the algorithm."}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want with it, no strings attached."}, {"tag": "p", "text": "Other versions: C#, Python"}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Stand-alone C++ implementation of Φ(x)\n\nThe function Φ(x) is the cumulative density function (CDF) of a standard normal (Gaussian) random variable. It is closely related to the error function erf(x).\n\n#include <cmath>\n\ndouble phi(double x)\n{\n    // constants\n    double a1 =  0.254829592;\n    double a2 = -0.284496736;\n    double a3 =  1.421413741;\n    double a4 = -1.453152027;\n    double a5 =  1.061405429;\n    double p  =  0.3275911;\n\n    // Save the sign of x\n    int sign = 1;\n    if (x < 0)\n        sign = -1;\n    x = fabs(x)/sqrt(2.0);\n\n    // A&S formula 7.1.26\n    double t = 1.0/(1.0 + p*x);\n    double y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*exp(-x*x);\n\n    return 0.5*(1.0 + sign*y);\n}\n\nvoid testPhi()\n{\n    // Select a few input values\n    double x[] = \n    {\n        -3, \n        -1, \n        0.0, \n        0.5, \n        2.1 \n    };\n\n    // Output computed by Mathematica\n    // y = Phi[x]\n    double y[] = \n    { \n        0.00134989803163, \n        0.158655253931, \n        0.5, \n        0.691462461274, \n        0.982135579437 \n    };\n\n\tint numTests = sizeof(x)/sizeof(double);\n\n    double maxError = 0.0;\n    for (int i = 0; i < numTests; ++i)\n    {\n        double error = fabs(y[i] - phi(x[i]));\n        if (error > maxError)\n            maxError = error;\n    }\n\n\tstd::cout << \"Maximum error: \" << maxError << \"\\n\";\n}\n\nA&S refers to Handbook of Mathematical Functions by Abramowitz and Stegun. See Stand-alone error function for details of the algorithm.\n\nThis code is in the public domain. Do whatever you want with it, no strings attached.\n\nOther versions: C#, Python\n\nStand-alone numerical code"}
{"slug": "cpp_random_number_generation", "canonical_url": "https://www.johndcook.com/blog/cpp_random_number_generation/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/cpp_random_number_generation.html", "title": "C++ stand-alone random number generation", "heading": "Random number generation in C++", "description": "C++ code for generating random samples from various probability distributions.", "summary": "C++ TR1 has code for generating random samples from normal, exponential, gamma, and Poisson distributions directly. Random number generation using C++ TR1 explains how to use this built-in functionality and now to bootstrap the built-in functions to generate samples from Cauchy, Student-t, Snedecor-F, and Weibull distributions.", "word_count": 148, "blocks": [{"tag": "h1", "text": "Random number generation in C++"}, {"tag": "p", "text": "C++ TR1 has code for generating random samples from normal, exponential, gamma, and Poisson distributions directly. Random number generation using C++ TR1 explains how to use this built-in functionality and now to bootstrap the built-in functions to generate samples from Cauchy, Student-t, Snedecor-F, and Weibull distributions."}, {"tag": "p", "text": "However, if for some reason you cannot use TR1 and need stand-alone random number generation code in C++, you may use the class SimpleRNG. The source files are here: SimpleRNG.h, SimpleRNG.cpp."}, {"tag": "p", "text": "The C++ implementation of SimpleRNG is based on the C# class by the same name explained in the Code Project article Simple Random Number Generation."}, {"tag": "p", "text": "SimpleRNG can be used to generate random unsigned integers and double values with several statistical distributions:"}, {"tag": "li", "text": "Beta"}, {"tag": "li", "text": "Cauchy"}, {"tag": "li", "text": "Chi square"}, {"tag": "li", "text": "Exponential"}, {"tag": "li", "text": "Gamma"}, {"tag": "li", "text": "Inverse gamma"}, {"tag": "li", "text": "Laplace (double exponential)"}, {"tag": "li", "text": "Log normal"}, {"tag": "li", "text": "Normal"}, {"tag": "li", "text": "Student t"}, {"tag": "li", "text": "Uniform"}, {"tag": "li", "text": "Weibull"}, {"tag": "p", "text": "Related links:"}, {"tag": "p", "text": "Distribution chart Stand-alone numerical code"}], "content": "Random number generation in C++\n\nC++ TR1 has code for generating random samples from normal, exponential, gamma, and Poisson distributions directly. Random number generation using C++ TR1 explains how to use this built-in functionality and now to bootstrap the built-in functions to generate samples from Cauchy, Student-t, Snedecor-F, and Weibull distributions.\n\nHowever, if for some reason you cannot use TR1 and need stand-alone random number generation code in C++, you may use the class SimpleRNG. The source files are here: SimpleRNG.h, SimpleRNG.cpp.\n\nThe C++ implementation of SimpleRNG is based on the C# class by the same name explained in the Code Project article Simple Random Number Generation.\n\nSimpleRNG can be used to generate random unsigned integers and double values with several statistical distributions:\n\nBeta\n\nCauchy\n\nChi square\n\nExponential\n\nGamma\n\nInverse gamma\n\nLaplace (double exponential)\n\nLog normal\n\nNormal\n\nStudent t\n\nUniform\n\nWeibull\n\nRelated links:\n\nDistribution chart Stand-alone numerical code"}
{"slug": "cpp_regex", "canonical_url": "https://www.johndcook.com/blog/cpp_regex/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/cpp_regex.html", "title": "Getting started with regular expressions using C++ TR1 extensions", "heading": "Getting started with C++ TR1 regular expressions", "description": "Getting started with regular expressions in C++ using the TR1 extension to the Standard Library. Examples comparing the C++ regular expression support to Perl.", "summary": "Overview Header and namespace C++ regular expression flavor Matching Retrieving matches Replacing matches Escape sequences Case-sensitivity Troubleshooting", "word_count": 1053, "blocks": [{"tag": "h1", "text": "Getting started with C++ TR1 regular expressions"}, {"tag": "p", "text": "Overview Header and namespace C++ regular expression flavor Matching Retrieving matches Replacing matches Escape sequences Case-sensitivity Troubleshooting"}, {"tag": "h2", "text": "Overview"}, {"tag": "p", "text": "This article is written for the benefit of someone familiar with regular expressions but not with the use of regular expressions in C++ via the TR1 (C++ Standards Committee Technical Report 1) extensions. Comparisons will be made with Perl for those familiar with Perl, though no knowledge of Perl is required. The focus is not on the syntax of regular expressions per se but rather how to use regular expressions to search for patterns and make replacements."}, {"tag": "p", "text": "The C++ TR1 regular expression specification has an intimidating array of options. This article is intended to get you started, not to explore every nook and cranny. Getting started is the harder part since it's easier to find API details than basic examples."}, {"tag": "p", "text": "The examples below use fully qualified namespaces for clarity. You could make your code more succinct by adding a few using statements to eliminate namespace qualifiers."}, {"tag": "h2", "text": "C++ TR1 regular expression flavor"}, {"tag": "p", "text": "The C++ TR1 regular expressions can follow the syntax of several regular expression environments depending on the optional flags sent to the regular expression class constructor. The six options given in the Microsoft implementation are as follows."}, {"tag": "li", "text": "basic"}, {"tag": "li", "text": "extended"}, {"tag": "li", "text": "ECMAScript"}, {"tag": "li", "text": "awk"}, {"tag": "li", "text": "grep"}, {"tag": "li", "text": "egrep"}, {"tag": "p", "text": "The default for the Microsoft implementation is ECMAScript, matching the regular expression syntax of the ECMAScript (JavaScript) language, which is very similar to that in Perl 5."}, {"tag": "p", "text": "The choice of flavors is extensible and implementation-specific. For example, the Boost implementation adds perl as an option, which presumably follows Perl 5 syntax more closely than the ECMASCript option does."}, {"tag": "p", "text": "For someone familiar with regular expressions the difficulty in using regular expressions in C++ TR1 is not in the syntax of regular expressions themselves, but rather in using regular expressions to do work."}, {"tag": "h2", "text": "Header and namespace"}, {"tag": "p", "text": "The C++ regular expression functions are defined in the <regex> header and contained in the namespace std::tr1. Note that tr is lowercase in C++. In English prose “TR” is capitalized."}, {"tag": "h2", "text": "Matching"}, {"tag": "p", "text": "The first surprise you may run into with the C++ regular expression implementation is that regex_match does not \"match\" in the usual sense. It will return true only when the entire string matches the regular expression. The function regex_search works more like the match operator in other environments, such as the m// operator in Perl."}, {"tag": "p", "text": "To illustrate regex_match and regex_search start with a C++ string"}, {"tag": "pre", "text": "std::string str = \"Hello world\";"}, {"tag": "p", "text": "and construct a regular expression"}, {"tag": "pre", "text": "std::tr1::regex rx(\"ello\");"}, {"tag": "p", "text": "The expression"}, {"tag": "pre", "text": "regex_match(str.begin(), str.end(), rx)"}, {"tag": "p", "text": "will return false because the string str contains more character beyond the match of the regular expression rx. However"}, {"tag": "pre", "text": "regex_search(str.begin(), str.end(), rx)"}, {"tag": "p", "text": "will return true because the regular expression matches a substring of str."}, {"tag": "h2", "text": "Retrieving matches"}, {"tag": "p", "text": "After performing a match in Perl, the captured matches are stored in the variables $1, $2, etc. Similarly, after a C++ places matches in a match_result object. However, while Perl always creates $nvariables, C++ does not store matches unless you call an overloaded form of regex_search that takes a match_result object. The class match_result is a template; often people use the class cmatch defined by"}, {"tag": "pre", "text": "typedef match_results<const char*> cmatch"}, {"tag": "p", "text": "The following example shows how retrieve captured matches."}, {"tag": "pre", "text": "std::tr1::cmatch res;\n    str = \"<h2>Egg prices</h2>\";\n    std::tr1::regex rx(\"<h(.)>([^<]+)\");\n    std::tr1::regex_search(str.c_str(), res, rx);\n    std::cout << res[1] << \". \" << res[2] << \"\\n\";"}, {"tag": "p", "text": "The code above will output"}, {"tag": "pre", "text": "2. Egg prices"}, {"tag": "p", "text": "Note that res[n] corresponds to Perl's $n."}, {"tag": "h2", "text": "Replacing matches"}, {"tag": "p", "text": "The following code will replace “world” in the string “Hello world” with “planet”. The string str2 will contain “Hello planet” and the string str will remain unchanged."}, {"tag": "pre", "text": "std::string str = \"Hello world\";\n    std::tr1::regex rx(\"world\");\n    std::string replacement = \"planet\";\n    std::string str2 = std::tr1::regex_replace(str, rx, replacement);"}, {"tag": "p", "text": "Note that regex_replace does not change its arguments, unlike the Perl command s/world/planet/."}, {"tag": "p", "text": "Note also that the third argument to regex_replace must be a string class and not a string literal. You could, however, eliminate the temporary variable replacement by changing the call to regex_replace with a string literal cast to a string."}, {"tag": "pre", "text": "regex_replace(str, rx, std::string(\"planet\"))"}, {"tag": "p", "text": "By default, all instances of the pattern that match the regular expression are replaced. In the example above, if str had been \"Hello world world\" the result would have been \"Hello planet planet\". To replace only the first instance (to produce \"Hello planet world\" you would need to add the flag"}, {"tag": "pre", "text": "std::tr1::regex_constants::format_first_only"}, {"tag": "p", "text": "as the fourth argument to regex_replace."}, {"tag": "p", "text": "Because the default behavior of regex_replaceis a global replace, the function is analogous to the s///g operator in Perl. With the format_first_only flag the function is analogous to the unmodified s/// Perl operator."}, {"tag": "h2", "text": "Escape sequences"}, {"tag": "p", "text": "Regular expression processing is not as convenient in C++ as it is in languages such as Perl that have built-in regular expression support. One reason is escape sequences. To send a backslash \\ to the regular expression engine, you have to type \\\\ in the source code. For example, consider these definitions."}, {"tag": "pre", "text": "std::string str = \"Hello\\tworld\";\n    std::tr1::regex rx(\"o\\\\tw\");"}, {"tag": "p", "text": "The string str contains a tab character between the o and the w. The regular expression rx does not contain a tab character; it contains \\t, the regular expression syntax for matching a tab character."}, {"tag": "h2", "text": "Case-sensitivity"}, {"tag": "p", "text": "C++ regular expressions are case-sensitive by default, as in Perl and many other environments. To specify that a regular expression is case-insensitive, add the flag std::tr1::regex_constants::icase as a second argument to the regex constructor. (The constructor flags can be combined with a bit-wise. So if you're specifying a flag for the regular expression flavor, you can follow it with | icase to combine the two.)"}, {"tag": "p", "text": "Support for case-sensitivity highlights the differences between C++ and scripting languages. C++ allows more control over regular expressions but also requires more input. For example, Perl makes the m// (match) and s/// (replace) operators case-insensitive by simply appending an i. While the regular expression syntax in C++ is more cluttered than that of scripting languages, people who use C++ are doing so because they value control over succinct syntax."}, {"tag": "h2", "text": "Troubleshooting"}, {"tag": "p", "text": "If you have trouble linking with the regex library in Visual Studio 2008, this post may help."}, {"tag": "h2", "text": "Further resources"}, {"tag": "p", "text": "Other C++ articles:"}, {"tag": "li", "text": "Random number generation"}, {"tag": "li", "text": "Strings"}, {"tag": "li", "text": "Floating point exceptions"}, {"tag": "li", "text": "Math.h: POSIX, ISO, and Microsoft"}, {"tag": "p", "text": "Using regular expressions in other languages:"}, {"tag": "li", "text": "PowerShell"}, {"tag": "li", "text": "Python"}, {"tag": "li", "text": "R"}, {"tag": "li", "text": "Mathematica"}, {"tag": "p", "text": "Daily tips on regular expressions"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Consulting"}, {"tag": "p", "text": "jdc"}], "content": "Getting started with C++ TR1 regular expressions\n\nOverview Header and namespace C++ regular expression flavor Matching Retrieving matches Replacing matches Escape sequences Case-sensitivity Troubleshooting\n\nOverview\n\nThis article is written for the benefit of someone familiar with regular expressions but not with the use of regular expressions in C++ via the TR1 (C++ Standards Committee Technical Report 1) extensions. Comparisons will be made with Perl for those familiar with Perl, though no knowledge of Perl is required. The focus is not on the syntax of regular expressions per se but rather how to use regular expressions to search for patterns and make replacements.\n\nThe C++ TR1 regular expression specification has an intimidating array of options. This article is intended to get you started, not to explore every nook and cranny. Getting started is the harder part since it's easier to find API details than basic examples.\n\nThe examples below use fully qualified namespaces for clarity. You could make your code more succinct by adding a few using statements to eliminate namespace qualifiers.\n\nC++ TR1 regular expression flavor\n\nThe C++ TR1 regular expressions can follow the syntax of several regular expression environments depending on the optional flags sent to the regular expression class constructor. The six options given in the Microsoft implementation are as follows.\n\nbasic\n\nextended\n\nECMAScript\n\nawk\n\ngrep\n\negrep\n\nThe default for the Microsoft implementation is ECMAScript, matching the regular expression syntax of the ECMAScript (JavaScript) language, which is very similar to that in Perl 5.\n\nThe choice of flavors is extensible and implementation-specific. For example, the Boost implementation adds perl as an option, which presumably follows Perl 5 syntax more closely than the ECMASCript option does.\n\nFor someone familiar with regular expressions the difficulty in using regular expressions in C++ TR1 is not in the syntax of regular expressions themselves, but rather in using regular expressions to do work.\n\nHeader and namespace\n\nThe C++ regular expression functions are defined in the <regex> header and contained in the namespace std::tr1. Note that tr is lowercase in C++. In English prose “TR” is capitalized.\n\nMatching\n\nThe first surprise you may run into with the C++ regular expression implementation is that regex_match does not \"match\" in the usual sense. It will return true only when the entire string matches the regular expression. The function regex_search works more like the match operator in other environments, such as the m// operator in Perl.\n\nTo illustrate regex_match and regex_search start with a C++ string\n\nstd::string str = \"Hello world\";\n\nand construct a regular expression\n\nstd::tr1::regex rx(\"ello\");\n\nThe expression\n\nregex_match(str.begin(), str.end(), rx)\n\nwill return false because the string str contains more character beyond the match of the regular expression rx. However\n\nregex_search(str.begin(), str.end(), rx)\n\nwill return true because the regular expression matches a substring of str.\n\nRetrieving matches\n\nAfter performing a match in Perl, the captured matches are stored in the variables $1, $2, etc. Similarly, after a C++ places matches in a match_result object. However, while Perl always creates $nvariables, C++ does not store matches unless you call an overloaded form of regex_search that takes a match_result object. The class match_result is a template; often people use the class cmatch defined by\n\ntypedef match_results<const char*> cmatch\n\nThe following example shows how retrieve captured matches.\n\nstd::tr1::cmatch res;\n    str = \"<h2>Egg prices</h2>\";\n    std::tr1::regex rx(\"<h(.)>([^<]+)\");\n    std::tr1::regex_search(str.c_str(), res, rx);\n    std::cout << res[1] << \". \" << res[2] << \"\\n\";\n\nThe code above will output\n\n2. Egg prices\n\nNote that res[n] corresponds to Perl's $n.\n\nReplacing matches\n\nThe following code will replace “world” in the string “Hello world” with “planet”. The string str2 will contain “Hello planet” and the string str will remain unchanged.\n\nstd::string str = \"Hello world\";\n    std::tr1::regex rx(\"world\");\n    std::string replacement = \"planet\";\n    std::string str2 = std::tr1::regex_replace(str, rx, replacement);\n\nNote that regex_replace does not change its arguments, unlike the Perl command s/world/planet/.\n\nNote also that the third argument to regex_replace must be a string class and not a string literal. You could, however, eliminate the temporary variable replacement by changing the call to regex_replace with a string literal cast to a string.\n\nregex_replace(str, rx, std::string(\"planet\"))\n\nBy default, all instances of the pattern that match the regular expression are replaced. In the example above, if str had been \"Hello world world\" the result would have been \"Hello planet planet\". To replace only the first instance (to produce \"Hello planet world\" you would need to add the flag\n\nstd::tr1::regex_constants::format_first_only\n\nas the fourth argument to regex_replace.\n\nBecause the default behavior of regex_replaceis a global replace, the function is analogous to the s///g operator in Perl. With the format_first_only flag the function is analogous to the unmodified s/// Perl operator.\n\nEscape sequences\n\nRegular expression processing is not as convenient in C++ as it is in languages such as Perl that have built-in regular expression support. One reason is escape sequences. To send a backslash \\ to the regular expression engine, you have to type \\\\ in the source code. For example, consider these definitions.\n\nstd::string str = \"Hello\\tworld\";\n    std::tr1::regex rx(\"o\\\\tw\");\n\nThe string str contains a tab character between the o and the w. The regular expression rx does not contain a tab character; it contains \\t, the regular expression syntax for matching a tab character.\n\nCase-sensitivity\n\nC++ regular expressions are case-sensitive by default, as in Perl and many other environments. To specify that a regular expression is case-insensitive, add the flag std::tr1::regex_constants::icase as a second argument to the regex constructor. (The constructor flags can be combined with a bit-wise. So if you're specifying a flag for the regular expression flavor, you can follow it with | icase to combine the two.)\n\nSupport for case-sensitivity highlights the differences between C++ and scripting languages. C++ allows more control over regular expressions but also requires more input. For example, Perl makes the m// (match) and s/// (replace) operators case-insensitive by simply appending an i. While the regular expression syntax in C++ is more cluttered than that of scripting languages, people who use C++ are doing so because they value control over succinct syntax.\n\nTroubleshooting\n\nIf you have trouble linking with the regex library in Visual Studio 2008, this post may help.\n\nFurther resources\n\nOther C++ articles:\n\nRandom number generation\n\nStrings\n\nFloating point exceptions\n\nMath.h: POSIX, ISO, and Microsoft\n\nUsing regular expressions in other languages:\n\nPowerShell\n\nPython\n\nR\n\nMathematica\n\nDaily tips on regular expressions\n\nHome\n\nBlog\n\nConsulting\n\nConsulting\n\njdc"}
{"slug": "csharp_erf", "canonical_url": "https://www.johndcook.com/blog/csharp_erf/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/csharp_erf.html", "title": "Stand-alone C# implementation of the error function", "heading": "Stand-alone C# code for the error function erf(x)", "description": "Stand-alone C# function for computing the error function erf(x).", "summary": "The following code first appeared as Python code in my blog post Stand-alone error function erf. See that post for documentation. See also Relating erf and Φ.", "word_count": 232, "blocks": [{"tag": "h1", "text": "Stand-alone C# code for the error function erf(x)"}, {"tag": "p", "text": "The following code first appeared as Python code in my blog post Stand-alone error function erf. See that post for documentation. See also Relating erf and Φ."}, {"tag": "pre", "text": "using System;\n    \n    static double Erf(double x)\n    {\n        // constants\n        double a1 = 0.254829592;\n        double a2 = -0.284496736;\n        double a3 = 1.421413741;\n        double a4 = -1.453152027;\n        double a5 = 1.061405429;\n        double p = 0.3275911;\n\n        // Save the sign of x\n        int sign = 1;\n        if (x < 0)\n            sign = -1;\n        x = Math.Abs(x);\n\n        // A&S formula 7.1.26\n        double t = 1.0 / (1.0 + p*x);\n        double y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*Math.Exp(-x*x);\n\n        return sign*y;\n    }\n    \n    static void TestErf()\n    {\n        // Select a few input values\n        double[] x = \n        {\n            -3, \n            -1, \n            0.0, \n            0.5, \n            2.1 \n        };\n\n        // Output computed by Mathematica\n        // y = Erf[x]\n        double[] y = \n        { \n            -0.999977909503, \n            -0.842700792950, \n            0.0, \n            0.520499877813, \n            0.997020533344 \n        };\n\n        double maxError = 0.0;\n        for (int i = 0; i < x.Length; ++i)\n        {\n            double error = Math.Abs(y[i] - Erf(x[i]));\n            if (error > maxError)\n                maxError = error;\n        }\n\n        Console.WriteLine(\"Maximum error: {0}\", maxError);\n    }"}, {"tag": "p", "text": "A&S refers to Handbook of Mathematical Functions by Abramowitz and Stegun. See Stand-alone error function for details of the algorithm."}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want with it, no strings attached."}, {"tag": "p", "text": "Other versions: C++, Python"}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Stand-alone C# code for the error function erf(x)\n\nThe following code first appeared as Python code in my blog post Stand-alone error function erf. See that post for documentation. See also Relating erf and Φ.\n\nusing System;\n    \n    static double Erf(double x)\n    {\n        // constants\n        double a1 = 0.254829592;\n        double a2 = -0.284496736;\n        double a3 = 1.421413741;\n        double a4 = -1.453152027;\n        double a5 = 1.061405429;\n        double p = 0.3275911;\n\n        // Save the sign of x\n        int sign = 1;\n        if (x < 0)\n            sign = -1;\n        x = Math.Abs(x);\n\n        // A&S formula 7.1.26\n        double t = 1.0 / (1.0 + p*x);\n        double y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*Math.Exp(-x*x);\n\n        return sign*y;\n    }\n    \n    static void TestErf()\n    {\n        // Select a few input values\n        double[] x = \n        {\n            -3, \n            -1, \n            0.0, \n            0.5, \n            2.1 \n        };\n\n        // Output computed by Mathematica\n        // y = Erf[x]\n        double[] y = \n        { \n            -0.999977909503, \n            -0.842700792950, \n            0.0, \n            0.520499877813, \n            0.997020533344 \n        };\n\n        double maxError = 0.0;\n        for (int i = 0; i < x.Length; ++i)\n        {\n            double error = Math.Abs(y[i] - Erf(x[i]));\n            if (error > maxError)\n                maxError = error;\n        }\n\n        Console.WriteLine(\"Maximum error: {0}\", maxError);\n    }\n\nA&S refers to Handbook of Mathematical Functions by Abramowitz and Stegun. See Stand-alone error function for details of the algorithm.\n\nThis code is in the public domain. Do whatever you want with it, no strings attached.\n\nOther versions: C++, Python\n\nStand-alone numerical code"}
{"slug": "csharp_expm1", "canonical_url": "https://www.johndcook.com/blog/csharp_expm1/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/csharp_expm1.html", "title": "Stand-alone C# implementation of expm1", "heading": "Stand-alone C# code for exp(x) - 1", "description": "Stand-alone C# code for computing exp(x) - 1, a.k.a. expm1(x), accurate even for small values of x where direct computation is not accurate.", "summary": "See the corresponding page for C++ for an explanation of the problem and the solution.", "word_count": 50, "blocks": [{"tag": "h1", "text": "Stand-alone C# code for exp(x) - 1"}, {"tag": "p", "text": "See the corresponding page for C++ for an explanation of the problem and the solution."}, {"tag": "p", "text": "The Python code is trivial:"}, {"tag": "pre", "text": "using System;\n\ndouble expm1(double x)\n{\n    if (Math.Abs(x) < 1e-5)\n        return x + 0.5*x*x;\n    else\n        return Math.Exp(x) - 1.0;\n}"}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Stand-alone C# code for exp(x) - 1\n\nSee the corresponding page for C++ for an explanation of the problem and the solution.\n\nThe Python code is trivial:\n\nusing System;\n\ndouble expm1(double x)\n{\n    if (Math.Abs(x) < 1e-5)\n        return x + 0.5*x*x;\n    else\n        return Math.Exp(x) - 1.0;\n}\n\nStand-alone numerical code"}
{"slug": "csharp_log_factorial", "canonical_url": "https://www.johndcook.com/blog/csharp_log_factorial/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/csharp_log_factorial.html", "title": "C# log factorial code", "heading": "C# code for computing log factorial", "description": "Stand-alone C# code for computing log factorial.", "summary": "For an explanation of this code, see How to compute log factorial.", "word_count": 342, "blocks": [{"tag": "h1", "text": "C# code for computing log factorial"}, {"tag": "p", "text": "For an explanation of this code, see How to compute log factorial."}, {"tag": "pre", "text": "using system;\n    \n    static double LogFactorial(int n)\n    {\n        if (n < 0)\n        {\n            throw new ArgumentOutOfRangeException();\n        }\n        else if (n > 254)\n        {\n            double x = n + 1;\n            return (x - 0.5)*Math.Log(x) - x + 0.5*Math.Log(2*Math.PI) + 1.0/(12.0*x);\n        }\n        else\n        {\n            double[] lf = \n            {\n                0.000000000000000,\n                0.000000000000000,\n                0.693147180559945,\n                1.791759469228055,\n                3.178053830347946,\n                4.787491742782046,\n                6.579251212010101,\n                8.525161361065415,\n                10.604602902745251,\n                12.801827480081469,\n                15.104412573075516,\n                17.502307845873887,\n                19.987214495661885,\n                22.552163853123421,\n                25.191221182738683,\n                27.899271383840894,\n                30.671860106080675,\n                33.505073450136891,\n                36.395445208033053,\n                39.339884187199495,\n                42.335616460753485,\n                45.380138898476908,\n                48.471181351835227,\n                51.606675567764377,\n                54.784729398112319,\n                58.003605222980518,\n                61.261701761002001,\n                64.557538627006323,\n                67.889743137181526,\n                71.257038967168000,\n                74.658236348830158,\n                78.092223553315307,\n                81.557959456115029,\n                85.054467017581516,\n                88.580827542197682,\n                92.136175603687079,\n                95.719694542143202,\n                99.330612454787428,\n                102.968198614513810,\n                106.631760260643450,\n                110.320639714757390,\n                114.034211781461690,\n                117.771881399745060,\n                121.533081515438640,\n                125.317271149356880,\n                129.123933639127240,\n                132.952575035616290,\n                136.802722637326350,\n                140.673923648234250,\n                144.565743946344900,\n                148.477766951773020,\n                152.409592584497350,\n                156.360836303078800,\n                160.331128216630930,\n                164.320112263195170,\n                168.327445448427650,\n                172.352797139162820,\n                176.395848406997370,\n                180.456291417543780,\n                184.533828861449510,\n                188.628173423671600,\n                192.739047287844900,\n                196.866181672889980,\n                201.009316399281570,\n                205.168199482641200,\n                209.342586752536820,\n                213.532241494563270,\n                217.736934113954250,\n                221.956441819130360,\n                226.190548323727570,\n                230.439043565776930,\n                234.701723442818260,\n                238.978389561834350,\n                243.268849002982730,\n                247.572914096186910,\n                251.890402209723190,\n                256.221135550009480,\n                260.564940971863220,\n                264.921649798552780,\n                269.291097651019810,\n                273.673124285693690,\n                278.067573440366120,\n                282.474292687630400,\n                286.893133295426990,\n                291.323950094270290,\n                295.766601350760600,\n                300.220948647014100,\n                304.686856765668720,\n                309.164193580146900,\n                313.652829949878990,\n                318.152639620209300,\n                322.663499126726210,\n                327.185287703775200,\n                331.717887196928470,\n                336.261181979198450,\n                340.815058870798960,\n                345.379407062266860,\n                349.954118040770250,\n                354.539085519440790,\n                359.134205369575340,\n                363.739375555563470,\n                368.354496072404690,\n                372.979468885689020,\n                377.614197873918670,\n                382.258588773060010,\n                386.912549123217560,\n                391.575988217329610,\n                396.248817051791490,\n                400.930948278915760,\n                405.622296161144900,\n                410.322776526937280,\n                415.032306728249580,\n                419.750805599544780,\n                424.478193418257090,\n                429.214391866651570,\n                433.959323995014870,\n                438.712914186121170,\n                443.475088120918940,\n                448.245772745384610,\n                453.024896238496130,\n                457.812387981278110,\n                462.608178526874890,\n                467.412199571608080,\n                472.224383926980520,\n                477.044665492585580,\n                481.872979229887900,\n                486.709261136839360,\n                491.553448223298010,\n                496.405478487217580,\n                501.265290891579240,\n                506.132825342034830,\n                511.008022665236070,\n                515.890824587822520,\n                520.781173716044240,\n                525.679013515995050,\n                530.584288294433580,\n                535.496943180169520,\n                540.416924105997740,\n                545.344177791154950,\n                550.278651724285620,\n                555.220294146894960,\n                560.169054037273100,\n                565.124881094874350,\n                570.087725725134190,\n                575.057539024710200,\n                580.034272767130800,\n                585.017879388839220,\n                590.008311975617860,\n                595.005524249382010,\n                600.009470555327430,\n                605.020105849423770,\n                610.037385686238740,\n                615.061266207084940,\n                620.091704128477430,\n                625.128656730891070,\n                630.172081847810200,\n                635.221937855059760,\n                640.278183660408100,\n                645.340778693435030,\n                650.409682895655240,\n                655.484856710889060,\n                660.566261075873510,\n                665.653857411105950,\n                670.747607611912710,\n                675.847474039736880,\n                680.953419513637530,\n                686.065407301994010,\n                691.183401114410800,\n                696.307365093814040,\n                701.437263808737160,\n                706.573062245787470,\n                711.714725802289990,\n                716.862220279103440,\n                722.015511873601330,\n                727.174567172815840,\n                732.339353146739310,\n                737.509837141777440,\n                742.685986874351220,\n                747.867770424643370,\n                753.055156230484160,\n                758.248113081374300,\n                763.446610112640200,\n                768.650616799717000,\n                773.860102952558460,\n                779.075038710167410,\n                784.295394535245690,\n                789.521141208958970,\n                794.752249825813460,\n                799.988691788643450,\n                805.230438803703120,\n                810.477462875863580,\n                815.729736303910160,\n                820.987231675937890,\n                826.249921864842800,\n                831.517780023906310,\n                836.790779582469900,\n                842.068894241700490,\n                847.352097970438420,\n                852.640365001133090,\n                857.933669825857460,\n                863.231987192405430,\n                868.535292100464630,\n                873.843559797865740,\n                879.156765776907600,\n                884.474885770751830,\n                889.797895749890240,\n                895.125771918679900,\n                900.458490711945270,\n                905.796028791646340,\n                911.138363043611210,\n                916.485470574328820,\n                921.837328707804890,\n                927.193914982476710,\n                932.555207148186240,\n                937.921183163208070,\n                943.291821191335660,\n                948.667099599019820,\n                954.046996952560450,\n                959.431492015349480,\n                964.820563745165940,\n                970.214191291518320,\n                975.612353993036210,\n                981.015031374908400,\n                986.422203146368590,\n                991.833849198223450,\n                997.249949600427840,\n                1002.670484599700300,\n                1008.095434617181700,\n                1013.524780246136200,\n                1018.958502249690200,\n                1024.396581558613400,\n                1029.838999269135500,\n                1035.285736640801600,\n                1040.736775094367400,\n                1046.192096209724900,\n                1051.651681723869200,\n                1057.115513528895000,\n                1062.583573670030100,\n                1068.055844343701400,\n                1073.532307895632800,\n                1079.012946818975000,\n                1084.497743752465600,\n                1089.986681478622400,\n                1095.479742921962700,\n                1100.976911147256000,\n                1106.478169357800900,\n                1111.983500893733000,\n                1117.492889230361000,\n                1123.006317976526100,\n                1128.523770872990800,\n                1134.045231790853000,\n                1139.570684729984800,\n                1145.100113817496100,\n                1150.633503306223700,\n                1156.170837573242400,\n            };\n            return lf[n];\n        }\n    }"}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want with it, no strings attached."}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "C# code for computing log factorial\n\nFor an explanation of this code, see How to compute log factorial.\n\nusing system;\n    \n    static double LogFactorial(int n)\n    {\n        if (n < 0)\n        {\n            throw new ArgumentOutOfRangeException();\n        }\n        else if (n > 254)\n        {\n            double x = n + 1;\n            return (x - 0.5)*Math.Log(x) - x + 0.5*Math.Log(2*Math.PI) + 1.0/(12.0*x);\n        }\n        else\n        {\n            double[] lf = \n            {\n                0.000000000000000,\n                0.000000000000000,\n                0.693147180559945,\n                1.791759469228055,\n                3.178053830347946,\n                4.787491742782046,\n                6.579251212010101,\n                8.525161361065415,\n                10.604602902745251,\n                12.801827480081469,\n                15.104412573075516,\n                17.502307845873887,\n                19.987214495661885,\n                22.552163853123421,\n                25.191221182738683,\n                27.899271383840894,\n                30.671860106080675,\n                33.505073450136891,\n                36.395445208033053,\n                39.339884187199495,\n                42.335616460753485,\n                45.380138898476908,\n                48.471181351835227,\n                51.606675567764377,\n                54.784729398112319,\n                58.003605222980518,\n                61.261701761002001,\n                64.557538627006323,\n                67.889743137181526,\n                71.257038967168000,\n                74.658236348830158,\n                78.092223553315307,\n                81.557959456115029,\n                85.054467017581516,\n                88.580827542197682,\n                92.136175603687079,\n                95.719694542143202,\n                99.330612454787428,\n                102.968198614513810,\n                106.631760260643450,\n                110.320639714757390,\n                114.034211781461690,\n                117.771881399745060,\n                121.533081515438640,\n                125.317271149356880,\n                129.123933639127240,\n                132.952575035616290,\n                136.802722637326350,\n                140.673923648234250,\n                144.565743946344900,\n                148.477766951773020,\n                152.409592584497350,\n                156.360836303078800,\n                160.331128216630930,\n                164.320112263195170,\n                168.327445448427650,\n                172.352797139162820,\n                176.395848406997370,\n                180.456291417543780,\n                184.533828861449510,\n                188.628173423671600,\n                192.739047287844900,\n                196.866181672889980,\n                201.009316399281570,\n                205.168199482641200,\n                209.342586752536820,\n                213.532241494563270,\n                217.736934113954250,\n                221.956441819130360,\n                226.190548323727570,\n                230.439043565776930,\n                234.701723442818260,\n                238.978389561834350,\n                243.268849002982730,\n                247.572914096186910,\n                251.890402209723190,\n                256.221135550009480,\n                260.564940971863220,\n                264.921649798552780,\n                269.291097651019810,\n                273.673124285693690,\n                278.067573440366120,\n                282.474292687630400,\n                286.893133295426990,\n                291.323950094270290,\n                295.766601350760600,\n                300.220948647014100,\n                304.686856765668720,\n                309.164193580146900,\n                313.652829949878990,\n                318.152639620209300,\n                322.663499126726210,\n                327.185287703775200,\n                331.717887196928470,\n                336.261181979198450,\n                340.815058870798960,\n                345.379407062266860,\n                349.954118040770250,\n                354.539085519440790,\n                359.134205369575340,\n                363.739375555563470,\n                368.354496072404690,\n                372.979468885689020,\n                377.614197873918670,\n                382.258588773060010,\n                386.912549123217560,\n                391.575988217329610,\n                396.248817051791490,\n                400.930948278915760,\n                405.622296161144900,\n                410.322776526937280,\n                415.032306728249580,\n                419.750805599544780,\n                424.478193418257090,\n                429.214391866651570,\n                433.959323995014870,\n                438.712914186121170,\n                443.475088120918940,\n                448.245772745384610,\n                453.024896238496130,\n                457.812387981278110,\n                462.608178526874890,\n                467.412199571608080,\n                472.224383926980520,\n                477.044665492585580,\n                481.872979229887900,\n                486.709261136839360,\n                491.553448223298010,\n                496.405478487217580,\n                501.265290891579240,\n                506.132825342034830,\n                511.008022665236070,\n                515.890824587822520,\n                520.781173716044240,\n                525.679013515995050,\n                530.584288294433580,\n                535.496943180169520,\n                540.416924105997740,\n                545.344177791154950,\n                550.278651724285620,\n                555.220294146894960,\n                560.169054037273100,\n                565.124881094874350,\n                570.087725725134190,\n                575.057539024710200,\n                580.034272767130800,\n                585.017879388839220,\n                590.008311975617860,\n                595.005524249382010,\n                600.009470555327430,\n                605.020105849423770,\n                610.037385686238740,\n                615.061266207084940,\n                620.091704128477430,\n                625.128656730891070,\n                630.172081847810200,\n                635.221937855059760,\n                640.278183660408100,\n                645.340778693435030,\n                650.409682895655240,\n                655.484856710889060,\n                660.566261075873510,\n                665.653857411105950,\n                670.747607611912710,\n                675.847474039736880,\n                680.953419513637530,\n                686.065407301994010,\n                691.183401114410800,\n                696.307365093814040,\n                701.437263808737160,\n                706.573062245787470,\n                711.714725802289990,\n                716.862220279103440,\n                722.015511873601330,\n                727.174567172815840,\n                732.339353146739310,\n                737.509837141777440,\n                742.685986874351220,\n                747.867770424643370,\n                753.055156230484160,\n                758.248113081374300,\n                763.446610112640200,\n                768.650616799717000,\n                773.860102952558460,\n                779.075038710167410,\n                784.295394535245690,\n                789.521141208958970,\n                794.752249825813460,\n                799.988691788643450,\n                805.230438803703120,\n                810.477462875863580,\n                815.729736303910160,\n                820.987231675937890,\n                826.249921864842800,\n                831.517780023906310,\n                836.790779582469900,\n                842.068894241700490,\n                847.352097970438420,\n                852.640365001133090,\n                857.933669825857460,\n                863.231987192405430,\n                868.535292100464630,\n                873.843559797865740,\n                879.156765776907600,\n                884.474885770751830,\n                889.797895749890240,\n                895.125771918679900,\n                900.458490711945270,\n                905.796028791646340,\n                911.138363043611210,\n                916.485470574328820,\n                921.837328707804890,\n                927.193914982476710,\n                932.555207148186240,\n                937.921183163208070,\n                943.291821191335660,\n                948.667099599019820,\n                954.046996952560450,\n                959.431492015349480,\n                964.820563745165940,\n                970.214191291518320,\n                975.612353993036210,\n                981.015031374908400,\n                986.422203146368590,\n                991.833849198223450,\n                997.249949600427840,\n                1002.670484599700300,\n                1008.095434617181700,\n                1013.524780246136200,\n                1018.958502249690200,\n                1024.396581558613400,\n                1029.838999269135500,\n                1035.285736640801600,\n                1040.736775094367400,\n                1046.192096209724900,\n                1051.651681723869200,\n                1057.115513528895000,\n                1062.583573670030100,\n                1068.055844343701400,\n                1073.532307895632800,\n                1079.012946818975000,\n                1084.497743752465600,\n                1089.986681478622400,\n                1095.479742921962700,\n                1100.976911147256000,\n                1106.478169357800900,\n                1111.983500893733000,\n                1117.492889230361000,\n                1123.006317976526100,\n                1128.523770872990800,\n                1134.045231790853000,\n                1139.570684729984800,\n                1145.100113817496100,\n                1150.633503306223700,\n                1156.170837573242400,\n            };\n            return lf[n];\n        }\n    }\n\nThis code is in the public domain. Do whatever you want with it, no strings attached.\n\nStand-alone numerical code"}
{"slug": "csharp_phi", "canonical_url": "https://www.johndcook.com/blog/csharp_phi/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/csharp_phi.html", "title": "Stand-alone C# implementation of the Phi function (Gaussian CDF)", "heading": "Stand-alone C# code for Φ(x)", "description": "Stand-alone C# function for computing the function Phi(x) where Phi is the normal (Gaussian) distribution function.", "summary": "The function Φ(x) is the cumulative density function (CDF) of a standard normal (Gaussian) random variable. It is closely related to the error function erf(x).", "word_count": 233, "blocks": [{"tag": "h1", "text": "Stand-alone C# code for Φ(x)"}, {"tag": "p", "text": "The function Φ(x) is the cumulative density function (CDF) of a standard normal (Gaussian) random variable. It is closely related to the error function erf(x)."}, {"tag": "pre", "text": "static double Phi(double x)\n{\n    // constants\n    double a1 = 0.254829592;\n    double a2 = -0.284496736;\n    double a3 = 1.421413741;\n    double a4 = -1.453152027;\n    double a5 = 1.061405429;\n    double p = 0.3275911;\n\t\n    // Save the sign of x\n    int sign = 1;\n    if (x < 0)\n        sign = -1;\n    x = Math.Abs(x) / Math.Sqrt(2.0);\n\t\n    // A&S formula 7.1.26\n    double t = 1.0 / (1.0 + p*x);\n    double y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t * Math.Exp(-x*x);\n\t\n    return 0.5 * (1.0 + sign*y);\n}\n\nstatic void TestPhi()\n{\n    // Select a few input values\n    double[] x = \n    {\n        -3, \n        -1, \n        0.0, \n        0.5, \n        2.1 \n    };\n\n    // Output computed by Mathematica\n    // y = Phi[x]\n    double[] y = \n    { \n        0.00134989803163, \n        0.158655253931, \n        0.5, \n        0.691462461274, \n        0.982135579437 \n    };\n\n    double maxError = 0.0;\n    for (int i = 0; i < x.Length; ++i)\n    {\n        double error = Math.Abs(y[i] - Phi(x[i]));\n        if (error > maxError)\n            maxError = error;\n    }\n\n\tConsole.WriteLine(\"Maximum error: {0}\", maxError);\n}"}, {"tag": "p", "text": "A&S refers to Handbook of Mathematical Functions by Abramowitz and Stegun. See Stand-alone error function for details of the algorithm."}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want with it, no strings attached."}, {"tag": "p", "text": "Other versions: C++, Python"}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Stand-alone C# code for Φ(x)\n\nThe function Φ(x) is the cumulative density function (CDF) of a standard normal (Gaussian) random variable. It is closely related to the error function erf(x).\n\nstatic double Phi(double x)\n{\n    // constants\n    double a1 = 0.254829592;\n    double a2 = -0.284496736;\n    double a3 = 1.421413741;\n    double a4 = -1.453152027;\n    double a5 = 1.061405429;\n    double p = 0.3275911;\n\t\n    // Save the sign of x\n    int sign = 1;\n    if (x < 0)\n        sign = -1;\n    x = Math.Abs(x) / Math.Sqrt(2.0);\n\t\n    // A&S formula 7.1.26\n    double t = 1.0 / (1.0 + p*x);\n    double y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t * Math.Exp(-x*x);\n\t\n    return 0.5 * (1.0 + sign*y);\n}\n\nstatic void TestPhi()\n{\n    // Select a few input values\n    double[] x = \n    {\n        -3, \n        -1, \n        0.0, \n        0.5, \n        2.1 \n    };\n\n    // Output computed by Mathematica\n    // y = Phi[x]\n    double[] y = \n    { \n        0.00134989803163, \n        0.158655253931, \n        0.5, \n        0.691462461274, \n        0.982135579437 \n    };\n\n    double maxError = 0.0;\n    for (int i = 0; i < x.Length; ++i)\n    {\n        double error = Math.Abs(y[i] - Phi(x[i]));\n        if (error > maxError)\n            maxError = error;\n    }\n\n\tConsole.WriteLine(\"Maximum error: {0}\", maxError);\n}\n\nA&S refers to Handbook of Mathematical Functions by Abramowitz and Stegun. See Stand-alone error function for details of the algorithm.\n\nThis code is in the public domain. Do whatever you want with it, no strings attached.\n\nOther versions: C++, Python\n\nStand-alone numerical code"}
{"slug": "csharp_poisson", "canonical_url": "https://www.johndcook.com/blog/csharp_poisson/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/csharp_poisson.html", "title": "C# Poisson random number generator", "heading": "C# code for generating Poisson random values", "description": "Stand-alone C# code for generating Poisson random values.", "summary": "The following C# code will generate random values from a Poisson distribution. It depends on a uniform random number generator function GetUniform. It also requires a function LogFactorial that returns the natural logarithm of the factorial of its argument. More on LogFactorial below.", "word_count": 405, "blocks": [{"tag": "h1", "text": "C# code for generating Poisson random values"}, {"tag": "p", "text": "The following C# code will generate random values from a Poisson distribution. It depends on a uniform random number generator function GetUniform. It also requires a function LogFactorial that returns the natural logarithm of the factorial of its argument. More on LogFactorial below."}, {"tag": "pre", "text": "public static int GetPoisson(double lambda)\n    {\n        return (lambda < 30.0) ? PoissonSmall(lambda) : PoissonLarge(lambda);\n    }\n\n    private static int PoissonSmall(double lambda)\n    {\n        // Algorithm due to Donald Knuth, 1969.\n        double p = 1.0, L = Math.Exp(-lambda);\n        int k = 0;\n        do\n        {\n            k++;\n            p *= GetUniform();\n        }\n        while (p > L);\n        return k - 1;\n    }\n\n    private static int PoissonLarge(double lambda)\n    {\n        // \"Rejection method PA\" from \"The Computer Generation of \n        // Poisson Random Variables\" by A. C. Atkinson,\n        // Journal of the Royal Statistical Society Series C \n        // (Applied Statistics) Vol. 28, No. 1. (1979)\n        // The article is on pages 29-35. \n        // The algorithm given here is on page 32.\n\n        double c = 0.767 - 3.36/lambda;\n        double beta = Math.PI/Math.Sqrt(3.0*lambda);\n        double alpha = beta*lambda;\n        double k = Math.Log(c) - lambda - Math.Log(beta);\n\n        for(;;)\n        {\n            double u = GetUniform();\n            double x = (alpha - Math.Log((1.0 - u)/u))/beta;\n            int n = (int) Math.Floor(x + 0.5);\n            if (n < 0)\n                continue;\n            double v = GetUniform();\n            double y = alpha - beta*x;\n            double temp = 1.0 + Math.Exp(y);\n            double lhs = y + Math.Log(v/(temp*temp));\n            double rhs = k + n*Math.Log(lambda) - LogFactorial(n);\n            if (lhs <= rhs)\n                return n;\n        }\n    }"}, {"tag": "p", "text": "The LogFactorial function takes some care to write well. It would be easy to write a naive implementation of factorial and take the logarithm of the output, but this would have two problems. First, the most obvious way to compute factorial has an execution time proportional to the size of the argument. Second, the factorial function can overflow for relatively small arguments."}, {"tag": "p", "text": "If you have access to a function that returns the log of the gamma function, it would be best to use it. Otherwise, you could write a function that simply looks up the value of log( n! ) from an array for some large value of n and uses an asymptotic approximation for values of n beyond the range of the lookup table. Such an implementation is available here. For details, see How to compute log factorial."}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want with it, no strings attached."}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "C# code for generating Poisson random values\n\nThe following C# code will generate random values from a Poisson distribution. It depends on a uniform random number generator function GetUniform. It also requires a function LogFactorial that returns the natural logarithm of the factorial of its argument. More on LogFactorial below.\n\npublic static int GetPoisson(double lambda)\n    {\n        return (lambda < 30.0) ? PoissonSmall(lambda) : PoissonLarge(lambda);\n    }\n\n    private static int PoissonSmall(double lambda)\n    {\n        // Algorithm due to Donald Knuth, 1969.\n        double p = 1.0, L = Math.Exp(-lambda);\n        int k = 0;\n        do\n        {\n            k++;\n            p *= GetUniform();\n        }\n        while (p > L);\n        return k - 1;\n    }\n\n    private static int PoissonLarge(double lambda)\n    {\n        // \"Rejection method PA\" from \"The Computer Generation of \n        // Poisson Random Variables\" by A. C. Atkinson,\n        // Journal of the Royal Statistical Society Series C \n        // (Applied Statistics) Vol. 28, No. 1. (1979)\n        // The article is on pages 29-35. \n        // The algorithm given here is on page 32.\n\n        double c = 0.767 - 3.36/lambda;\n        double beta = Math.PI/Math.Sqrt(3.0*lambda);\n        double alpha = beta*lambda;\n        double k = Math.Log(c) - lambda - Math.Log(beta);\n\n        for(;;)\n        {\n            double u = GetUniform();\n            double x = (alpha - Math.Log((1.0 - u)/u))/beta;\n            int n = (int) Math.Floor(x + 0.5);\n            if (n < 0)\n                continue;\n            double v = GetUniform();\n            double y = alpha - beta*x;\n            double temp = 1.0 + Math.Exp(y);\n            double lhs = y + Math.Log(v/(temp*temp));\n            double rhs = k + n*Math.Log(lambda) - LogFactorial(n);\n            if (lhs <= rhs)\n                return n;\n        }\n    }\n\nThe LogFactorial function takes some care to write well. It would be easy to write a naive implementation of factorial and take the logarithm of the output, but this would have two problems. First, the most obvious way to compute factorial has an execution time proportional to the size of the argument. Second, the factorial function can overflow for relatively small arguments.\n\nIf you have access to a function that returns the log of the gamma function, it would be best to use it. Otherwise, you could write a function that simply looks up the value of log( n! ) from an array for some large value of n and uses an asymptotic approximation for values of n beyond the range of the lookup table. Such an implementation is available here. For details, see How to compute log factorial.\n\nThis code is in the public domain. Do whatever you want with it, no strings attached.\n\nStand-alone numerical code"}
{"slug": "distribution_chart", "canonical_url": "https://www.johndcook.com/blog/distribution_chart/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/distribution_chart.html", "title": "Diagram of probability distribution relationships", "heading": "Diagram of distribution relationships", "description": "A clickable chart of probability distribution relationships with footnotes.", "summary": "Probability distributions have a surprising number inter-connections. A dashed line in the chart below indicates an approximate (limit) relationship between two distribution families. A solid line indicates an exact relationship: special case, sum, or transformation.", "word_count": 2038, "blocks": [{"tag": "h1", "text": "Diagram of distribution relationships"}, {"tag": "p", "text": "Probability distributions have a surprising number inter-connections. A dashed line in the chart below indicates an approximate (limit) relationship between two distribution families. A solid line indicates an exact relationship: special case, sum, or transformation."}, {"tag": "p", "text": "Click on a distribution for the parameterization of that distribution. Click on an arrow for details on the relationship represented by the arrow."}, {"tag": "p", "text": "Follow @ProbFact on Twitter to get one probability fact per day, such as the relationships on this diagram."}, {"tag": "p", "text": "More mathematical diagrams"}, {"tag": "p", "text": "The chart above is adapted from the chart originally published by Lawrence Leemis in 1986 (Relationships Among Common Univariate Distributions, American Statistician 40:143-146.) Leemis published a larger chart in 2008 which is available online."}, {"tag": "p", "text": "If you would like for me to do a one-day seminar explaining in detail the information in this chart, please let me know."}, {"tag": "h2", "text": "Parameterizations"}, {"tag": "p", "text": "The precise relationships between distributions depend on parameterization. The relationships detailed below depend on the following parameterizations for the PDFs."}, {"tag": "p", "text": "Let C(n, k) denote the binomial coefficient(n, k) and B(a, b) = Γ(a) Γ(b) / Γ(a + b)."}, {"tag": "p", "text": "Geometric: f(x) = p (1-p)x for non-negative integers x."}, {"tag": "p", "text": "Discrete uniform: f(x) = 1/n for x = 1, 2, ..., n."}, {"tag": "p", "text": "Negative binomial: f(x) = C(r + x - 1, x) pr(1-p)x for non-negative integers x. See notes on the negative binomial distribution."}, {"tag": "p", "text": "Beta binomial: f(x) = C(n, x) B(α + x, n + β - x) / B(α, β) for x = 0, 1, ..., n."}, {"tag": "p", "text": "Hypergeometric: f(x) = C(M, x) C(N-M, K - x) / C(N, K) for x = 0, 1, ..., N."}, {"tag": "p", "text": "Poisson: f(x) = exp(-λ) λx/ x! for non-negative integers x. The parameter λ is both the mean and the variance."}, {"tag": "p", "text": "Binomial: f(x) = C(n, x) px(1 - p)n-x for x = 0, 1, ..., n."}, {"tag": "p", "text": "Bernoulli: f(x) = px(1 - p)1-x where x = 0 or 1."}, {"tag": "p", "text": "Lognormal: f(x) = (2πσ2)-1/2 exp( -(log(x) - μ)2/ 2σ2) / x for positive x. Note that μ and σ2 are not the mean and variance of the distribution."}, {"tag": "p", "text": "Normal : f(x) = (2π σ2)-1/2 exp( - ½((x - μ)/σ)2 ) for all x."}, {"tag": "p", "text": "Beta: f(x) = Γ(α + β) xα-1(1 - x)β-1 / (Γ(α) Γ(β)) for 0 ≤ x ≤ 1."}, {"tag": "p", "text": "Standard normal: f(x) = (2π)-1/2 exp( -x2/2) for all x."}, {"tag": "p", "text": "Chi-squared: f(x) = x-ν/2-1 exp(-x/2) / Γ(ν/2) 2ν/2 for positive x. The parameter ν is called the degrees of freedom."}, {"tag": "p", "text": "Gamma: f(x) = β-α xα-1 exp(-x/β) / Γ(α) for positive x. The parameter α is called the shape and β is the scale."}, {"tag": "p", "text": "Uniform: f(x) = 1 for 0 ≤ x ≤ 1."}, {"tag": "p", "text": "Cauchy: f(x) = σ/(π( (x - μ)2 + σ2) ) for all x. Note that μ and σ are location and scale parameters. The Cauchy distribution has no mean or variance."}, {"tag": "p", "text": "Snedecor F: f(x) is proportional to x(ν1 - 2)/2 / (1 + (ν1/ν2) x)(ν1 + ν2)/2 for positive x."}, {"tag": "p", "text": "Exponential: f(x) = exp(-x/μ)/μ for positive x. The parameter μ is the mean."}, {"tag": "p", "text": "Student t: f(x) is proportional to (1 + (x2/ν))-(ν + 1)/2 for positive x. The parameter ν is called the degrees of freedom."}, {"tag": "p", "text": "Weibull: f(x) = (γ/β) xγ-1 exp(- xγ/β) for positive x. The parameter γ is the shape and β is the scale."}, {"tag": "p", "text": "Double exponential : f(x) = exp(-|x-μ|/σ) / 2σ for all x. The parameter μ is the location and mean; σ is the scale."}, {"tag": "p", "text": "For comparison, see distribution parameterizations in R/S-PLUS and Mathematica."}, {"tag": "h2", "text": "Relationships"}, {"tag": "p", "text": "In all statements about two random variables, the random variables are implicitly independent."}, {"tag": "p", "text": "Geometric / negative binomial: If each Xi is geometric random variable with probability of success p then the sum of n Xi's is a negative binomial random variable with parameters n and p."}, {"tag": "p", "text": "Negative binomial / geometric: A negative binomial distribution with r = 1 is a geometric distribution."}, {"tag": "p", "text": "Negative binomial / Poisson: If X has a negative binomial random variable with r large, p near 1, and r(1-p) = λ, then FX ≈ FY where Y is a Poisson random variable with mean λ."}, {"tag": "p", "text": "Beta-binomial / discrete uniform: A beta-binomial (n, 1, 1) random variable is a discrete uniform random variable over the values 0 ... n."}, {"tag": "p", "text": "Beta-binomial / binomial: Let X be a beta-binomial random variable with parameters (n, α, β). Let p = α/(α + β) and suppose α + β is large. If Y is a binomial(n, p) random variable then FX ≈ FY."}, {"tag": "p", "text": "Hypergeometric / binomial: The difference between a hypergeometric distribution and a binomial distribution is the difference between sampling without replacement and sampling with replacement. As the population size increases relative to the sample size, the difference becomes negligible."}, {"tag": "p", "text": "Geometric / geometric: If X1 and X2 are geometric random variables with probability of success p1 and p2 respectively, then min(X1, X2) is a geometric random variable with probability of success p = p1 + p2 - p1 p2. The relationship is simpler in terms of failure probabilities: q = q1 q2."}, {"tag": "p", "text": "Poisson / Poisson: If X1 and X2 are Poisson random variables with means μ1 and μ2 respectively, then X1 + X2 is a Poisson random variable with mean μ1 + μ2."}, {"tag": "p", "text": "Binomial / Poisson: If X is a binomial(n, p) random variable and Y is a Poisson(np) distribution then P(X = n) ≈ P(Y = n) if n is large and np is small. For more information, see Poisson approximation to binomial."}, {"tag": "p", "text": "Binomial / Bernoulli: If X is a binomial(n, p) random variable with n = 1, X is a Bernoulli(p) random variable."}, {"tag": "p", "text": "Bernoulli / Binomial: The sum of n Bernoulli(p) random variables is a binomial(n, p) random variable."}, {"tag": "p", "text": "Poisson / normal: If X is a Poisson random variable with large mean and Y is a normal distribution with the same mean and variance as X, then for integers j and k, P(j ≤ X ≤ k) ≈ P(j - 1/2 ≤ Y ≤ k + 1/2). For more information, see normal approximation to Poisson."}, {"tag": "p", "text": "Binomial / normal: If X is a binomial(n, p) random variable and Y is a normal random variable with the same mean and variance as X, i.e. np and np(1-p), then for integers j and k, P(j ≤ X ≤ k) ≈ P(j - 1/2 ≤ Y ≤ k + 1/2). The approximation is better when p ≈ 0.5 and when n is large. For more information, see normal approximation to binomial."}, {"tag": "p", "text": "Lognormal / lognormal: If X1 and X2 are lognormal random variables with parameters (μ1, σ12) and (μ2, σ22) respectively, then X1 X2 is a lognormal random variable with parameters (μ1 + μ2, σ12 + σ22)."}, {"tag": "p", "text": "Normal / lognormal: If X is a normal (μ, σ2) random variable then eX is a lognormal (μ, σ2) random variable. Conversely, if X is a lognormal (μ, σ2) random variable then log X is a normal (μ, σ2) random variable."}, {"tag": "p", "text": "Beta / normal: If X is a beta random variable with parameters α and β equal and large, FX ≈ FY where Y is a normal random variable with the same mean and variance as X, i.e. mean α/(α + β) and variance αβ/((α+β)2(α + β + 1)). For more information, see normal approximation to beta."}, {"tag": "p", "text": "Normal / standard normal: If X is a normal(μ, σ2) random variable then (X - μ)/σ is a standard normal random variable. Conversely, If X is a normal(0,1) random variable then σ X + μ is a normal (μ, σ2) random variable."}, {"tag": "p", "text": "Normal / normal: If X1 is a normal (μ1, σ12) random variable and X2 is a normal (μ2, σ22) random variable, then X1 + X2 is a normal (μ1 + μ2, σ12 + σ22) random variable."}, {"tag": "p", "text": "Gamma / normal: If X is a gamma(α, β) random variable and Y is a normal random variable with the same mean and variance as X, then FX ≈ FY if the shape parameter α is large relative to the scale parameter β. For more information, see normal approximation to gamma."}, {"tag": "p", "text": "Gamma / beta: If X1 is gamma(α1, 1) random variable and X2 is a gamma (α2, 1) random variable then X1/(X1 + X2) is a beta(α1, α2) random variable. More generally, if X1 is gamma(α1, β1) random variable and X2 is gamma(α2, β2) random variable then β2 X1/(β2 X1 + β1 X2) is a beta(α1, α2) random variable."}, {"tag": "p", "text": "Beta / uniform: A beta random variable with parameters α = β = 1 is a uniform random variable."}, {"tag": "p", "text": "Chi-squared / chi-squared: If X1 and X2 are chi-squared random variables with ν1 and ν2 degrees of freedom respectively, then X1 + X2 is a chi-squared random variable with ν1 + ν2 degrees of freedom."}, {"tag": "p", "text": "Standard normal / chi-squared: The square of a standard normal random variable has a chi-squared distribution with one degree of freedom. The sum of the squares of n standard normal random variables is has a chi-squared distribution with n degrees of freedom."}, {"tag": "p", "text": "Gamma / chi-squared: If X is a gamma (α, β) random variable with α = ν/2 and β = 2, then X is a chi-squared random variable with ν degrees of freedom."}, {"tag": "p", "text": "Cauchy / standard normal: If X and Y are standard normal random variables, X/Y is a Cauchy(0,1) random variable."}, {"tag": "p", "text": "Student t / standard normal: If X is a t random variable with a large number of degrees of freedom ν then FX ≈ FY where Y is a standard normal random variable. For more information, see normal approximation to t."}, {"tag": "p", "text": "Snedecor F / chi-squared: If X is an F(ν, ω) random variable with ω large, then ν X is approximately distributed as a chi-squared random variable with ν degrees of freedom."}, {"tag": "p", "text": "Chi-squared / Snedecor F: If X1 and X2 are chi-squared random variables with ν1 and ν2 degrees of freedom respectively, then (X1/ν1)/(X2/ν2) is an F(ν1, ν2) random variable."}, {"tag": "p", "text": "Chi-squared / exponential: A chi-squared distribution with 2 degrees of freedom is an exponential distribution with mean 2."}, {"tag": "p", "text": "Exponential / chi-squared: An exponential random variable with mean 2 is a chi-squared random variable with two degrees of freedom."}, {"tag": "p", "text": "Gamma / exponential: The sum of n exponential(β) random variables is a gamma(n, β) random variable."}, {"tag": "p", "text": "Exponential / gamma: A gamma distribution with shape parameter α = 1 and scale parameter β is an exponential(β) distribution."}, {"tag": "p", "text": "Exponential / uniform: If X is an exponential random variable with mean λ, then exp(-X/λ) is a uniform random variable. More generally, sticking any random variable into its CDF yields a uniform random variable."}, {"tag": "p", "text": "Uniform / exponential: If X is a uniform random variable, -λ log X is an exponential random variable with mean λ. More generally, applying the inverse CDF of any random variable X to a uniform random variable creates a variable with the same distribution as X."}, {"tag": "p", "text": "Cauchy reciprocal: If X is a Cauchy (μ, σ) random variable, then 1/X is a Cauchy (μ/c, σ/c) random variable where c = μ2 + σ2."}, {"tag": "p", "text": "Cauchy sum: If X1 is a Cauchy (μ1, σ1) random variable and X2 is a Cauchy (μ2, σ2), then X1 + X2 is a Cauchy (μ1 + μ2, σ1 + σ2) random variable."}, {"tag": "p", "text": "Student t / Cauchy: A random variable with a t distribution with one degree of freedom is a Cauchy(0,1) random variable."}, {"tag": "p", "text": "Student t / Snedecor F: If X is a t random variable with ν degree of freedom, then X2 is an F(1,ν) random variable."}, {"tag": "p", "text": "Snedecor F / Snedecor F: If X is an F(ν1, ν2) random variable then 1/X is an F(ν2, ν1) random variable."}, {"tag": "p", "text": "Exponential / Exponential: If X1 and X2 are exponential random variables with mean μ1 and μ2 respectively, then min(X1, X2) is an exponential random variable with mean μ1 μ2/(μ1 + μ2)."}, {"tag": "p", "text": "Exponential / Weibull: If X is an exponential random variable with mean β, then X1/γ is a Weibull(γ, β) random variable."}, {"tag": "p", "text": "Weibull / Exponential: If X is a Weibull(1, β) random variable, X is an exponential random variable with mean β."}, {"tag": "p", "text": "Exponential / Double exponential: If X and Y are exponential random variables with mean μ, then X-Y is a double exponential random variable with mean 0 and scale μ"}, {"tag": "p", "text": "Double exponential / exponential: If X is a double exponential random variable with mean 0 and scale λ, then |X| is an exponential random variable with mean λ."}], "content": "Diagram of distribution relationships\n\nProbability distributions have a surprising number inter-connections. A dashed line in the chart below indicates an approximate (limit) relationship between two distribution families. A solid line indicates an exact relationship: special case, sum, or transformation.\n\nClick on a distribution for the parameterization of that distribution. Click on an arrow for details on the relationship represented by the arrow.\n\nFollow @ProbFact on Twitter to get one probability fact per day, such as the relationships on this diagram.\n\nMore mathematical diagrams\n\nThe chart above is adapted from the chart originally published by Lawrence Leemis in 1986 (Relationships Among Common Univariate Distributions, American Statistician 40:143-146.) Leemis published a larger chart in 2008 which is available online.\n\nIf you would like for me to do a one-day seminar explaining in detail the information in this chart, please let me know.\n\nParameterizations\n\nThe precise relationships between distributions depend on parameterization. The relationships detailed below depend on the following parameterizations for the PDFs.\n\nLet C(n, k) denote the binomial coefficient(n, k) and B(a, b) = Γ(a) Γ(b) / Γ(a + b).\n\nGeometric: f(x) = p (1-p)x for non-negative integers x.\n\nDiscrete uniform: f(x) = 1/n for x = 1, 2, ..., n.\n\nNegative binomial: f(x) = C(r + x - 1, x) pr(1-p)x for non-negative integers x. See notes on the negative binomial distribution.\n\nBeta binomial: f(x) = C(n, x) B(α + x, n + β - x) / B(α, β) for x = 0, 1, ..., n.\n\nHypergeometric: f(x) = C(M, x) C(N-M, K - x) / C(N, K) for x = 0, 1, ..., N.\n\nPoisson: f(x) = exp(-λ) λx/ x! for non-negative integers x. The parameter λ is both the mean and the variance.\n\nBinomial: f(x) = C(n, x) px(1 - p)n-x for x = 0, 1, ..., n.\n\nBernoulli: f(x) = px(1 - p)1-x where x = 0 or 1.\n\nLognormal: f(x) = (2πσ2)-1/2 exp( -(log(x) - μ)2/ 2σ2) / x for positive x. Note that μ and σ2 are not the mean and variance of the distribution.\n\nNormal : f(x) = (2π σ2)-1/2 exp( - ½((x - μ)/σ)2 ) for all x.\n\nBeta: f(x) = Γ(α + β) xα-1(1 - x)β-1 / (Γ(α) Γ(β)) for 0 ≤ x ≤ 1.\n\nStandard normal: f(x) = (2π)-1/2 exp( -x2/2) for all x.\n\nChi-squared: f(x) = x-ν/2-1 exp(-x/2) / Γ(ν/2) 2ν/2 for positive x. The parameter ν is called the degrees of freedom.\n\nGamma: f(x) = β-α xα-1 exp(-x/β) / Γ(α) for positive x. The parameter α is called the shape and β is the scale.\n\nUniform: f(x) = 1 for 0 ≤ x ≤ 1.\n\nCauchy: f(x) = σ/(π( (x - μ)2 + σ2) ) for all x. Note that μ and σ are location and scale parameters. The Cauchy distribution has no mean or variance.\n\nSnedecor F: f(x) is proportional to x(ν1 - 2)/2 / (1 + (ν1/ν2) x)(ν1 + ν2)/2 for positive x.\n\nExponential: f(x) = exp(-x/μ)/μ for positive x. The parameter μ is the mean.\n\nStudent t: f(x) is proportional to (1 + (x2/ν))-(ν + 1)/2 for positive x. The parameter ν is called the degrees of freedom.\n\nWeibull: f(x) = (γ/β) xγ-1 exp(- xγ/β) for positive x. The parameter γ is the shape and β is the scale.\n\nDouble exponential : f(x) = exp(-|x-μ|/σ) / 2σ for all x. The parameter μ is the location and mean; σ is the scale.\n\nFor comparison, see distribution parameterizations in R/S-PLUS and Mathematica.\n\nRelationships\n\nIn all statements about two random variables, the random variables are implicitly independent.\n\nGeometric / negative binomial: If each Xi is geometric random variable with probability of success p then the sum of n Xi's is a negative binomial random variable with parameters n and p.\n\nNegative binomial / geometric: A negative binomial distribution with r = 1 is a geometric distribution.\n\nNegative binomial / Poisson: If X has a negative binomial random variable with r large, p near 1, and r(1-p) = λ, then FX ≈ FY where Y is a Poisson random variable with mean λ.\n\nBeta-binomial / discrete uniform: A beta-binomial (n, 1, 1) random variable is a discrete uniform random variable over the values 0 ... n.\n\nBeta-binomial / binomial: Let X be a beta-binomial random variable with parameters (n, α, β). Let p = α/(α + β) and suppose α + β is large. If Y is a binomial(n, p) random variable then FX ≈ FY.\n\nHypergeometric / binomial: The difference between a hypergeometric distribution and a binomial distribution is the difference between sampling without replacement and sampling with replacement. As the population size increases relative to the sample size, the difference becomes negligible.\n\nGeometric / geometric: If X1 and X2 are geometric random variables with probability of success p1 and p2 respectively, then min(X1, X2) is a geometric random variable with probability of success p = p1 + p2 - p1 p2. The relationship is simpler in terms of failure probabilities: q = q1 q2.\n\nPoisson / Poisson: If X1 and X2 are Poisson random variables with means μ1 and μ2 respectively, then X1 + X2 is a Poisson random variable with mean μ1 + μ2.\n\nBinomial / Poisson: If X is a binomial(n, p) random variable and Y is a Poisson(np) distribution then P(X = n) ≈ P(Y = n) if n is large and np is small. For more information, see Poisson approximation to binomial.\n\nBinomial / Bernoulli: If X is a binomial(n, p) random variable with n = 1, X is a Bernoulli(p) random variable.\n\nBernoulli / Binomial: The sum of n Bernoulli(p) random variables is a binomial(n, p) random variable.\n\nPoisson / normal: If X is a Poisson random variable with large mean and Y is a normal distribution with the same mean and variance as X, then for integers j and k, P(j ≤ X ≤ k) ≈ P(j - 1/2 ≤ Y ≤ k + 1/2). For more information, see normal approximation to Poisson.\n\nBinomial / normal: If X is a binomial(n, p) random variable and Y is a normal random variable with the same mean and variance as X, i.e. np and np(1-p), then for integers j and k, P(j ≤ X ≤ k) ≈ P(j - 1/2 ≤ Y ≤ k + 1/2). The approximation is better when p ≈ 0.5 and when n is large. For more information, see normal approximation to binomial.\n\nLognormal / lognormal: If X1 and X2 are lognormal random variables with parameters (μ1, σ12) and (μ2, σ22) respectively, then X1 X2 is a lognormal random variable with parameters (μ1 + μ2, σ12 + σ22).\n\nNormal / lognormal: If X is a normal (μ, σ2) random variable then eX is a lognormal (μ, σ2) random variable. Conversely, if X is a lognormal (μ, σ2) random variable then log X is a normal (μ, σ2) random variable.\n\nBeta / normal: If X is a beta random variable with parameters α and β equal and large, FX ≈ FY where Y is a normal random variable with the same mean and variance as X, i.e. mean α/(α + β) and variance αβ/((α+β)2(α + β + 1)). For more information, see normal approximation to beta.\n\nNormal / standard normal: If X is a normal(μ, σ2) random variable then (X - μ)/σ is a standard normal random variable. Conversely, If X is a normal(0,1) random variable then σ X + μ is a normal (μ, σ2) random variable.\n\nNormal / normal: If X1 is a normal (μ1, σ12) random variable and X2 is a normal (μ2, σ22) random variable, then X1 + X2 is a normal (μ1 + μ2, σ12 + σ22) random variable.\n\nGamma / normal: If X is a gamma(α, β) random variable and Y is a normal random variable with the same mean and variance as X, then FX ≈ FY if the shape parameter α is large relative to the scale parameter β. For more information, see normal approximation to gamma.\n\nGamma / beta: If X1 is gamma(α1, 1) random variable and X2 is a gamma (α2, 1) random variable then X1/(X1 + X2) is a beta(α1, α2) random variable. More generally, if X1 is gamma(α1, β1) random variable and X2 is gamma(α2, β2) random variable then β2 X1/(β2 X1 + β1 X2) is a beta(α1, α2) random variable.\n\nBeta / uniform: A beta random variable with parameters α = β = 1 is a uniform random variable.\n\nChi-squared / chi-squared: If X1 and X2 are chi-squared random variables with ν1 and ν2 degrees of freedom respectively, then X1 + X2 is a chi-squared random variable with ν1 + ν2 degrees of freedom.\n\nStandard normal / chi-squared: The square of a standard normal random variable has a chi-squared distribution with one degree of freedom. The sum of the squares of n standard normal random variables is has a chi-squared distribution with n degrees of freedom.\n\nGamma / chi-squared: If X is a gamma (α, β) random variable with α = ν/2 and β = 2, then X is a chi-squared random variable with ν degrees of freedom.\n\nCauchy / standard normal: If X and Y are standard normal random variables, X/Y is a Cauchy(0,1) random variable.\n\nStudent t / standard normal: If X is a t random variable with a large number of degrees of freedom ν then FX ≈ FY where Y is a standard normal random variable. For more information, see normal approximation to t.\n\nSnedecor F / chi-squared: If X is an F(ν, ω) random variable with ω large, then ν X is approximately distributed as a chi-squared random variable with ν degrees of freedom.\n\nChi-squared / Snedecor F: If X1 and X2 are chi-squared random variables with ν1 and ν2 degrees of freedom respectively, then (X1/ν1)/(X2/ν2) is an F(ν1, ν2) random variable.\n\nChi-squared / exponential: A chi-squared distribution with 2 degrees of freedom is an exponential distribution with mean 2.\n\nExponential / chi-squared: An exponential random variable with mean 2 is a chi-squared random variable with two degrees of freedom.\n\nGamma / exponential: The sum of n exponential(β) random variables is a gamma(n, β) random variable.\n\nExponential / gamma: A gamma distribution with shape parameter α = 1 and scale parameter β is an exponential(β) distribution.\n\nExponential / uniform: If X is an exponential random variable with mean λ, then exp(-X/λ) is a uniform random variable. More generally, sticking any random variable into its CDF yields a uniform random variable.\n\nUniform / exponential: If X is a uniform random variable, -λ log X is an exponential random variable with mean λ. More generally, applying the inverse CDF of any random variable X to a uniform random variable creates a variable with the same distribution as X.\n\nCauchy reciprocal: If X is a Cauchy (μ, σ) random variable, then 1/X is a Cauchy (μ/c, σ/c) random variable where c = μ2 + σ2.\n\nCauchy sum: If X1 is a Cauchy (μ1, σ1) random variable and X2 is a Cauchy (μ2, σ2), then X1 + X2 is a Cauchy (μ1 + μ2, σ1 + σ2) random variable.\n\nStudent t / Cauchy: A random variable with a t distribution with one degree of freedom is a Cauchy(0,1) random variable.\n\nStudent t / Snedecor F: If X is a t random variable with ν degree of freedom, then X2 is an F(1,ν) random variable.\n\nSnedecor F / Snedecor F: If X is an F(ν1, ν2) random variable then 1/X is an F(ν2, ν1) random variable.\n\nExponential / Exponential: If X1 and X2 are exponential random variables with mean μ1 and μ2 respectively, then min(X1, X2) is an exponential random variable with mean μ1 μ2/(μ1 + μ2).\n\nExponential / Weibull: If X is an exponential random variable with mean β, then X1/γ is a Weibull(γ, β) random variable.\n\nWeibull / Exponential: If X is a Weibull(1, β) random variable, X is an exponential random variable with mean β.\n\nExponential / Double exponential: If X and Y are exponential random variables with mean μ, then X-Y is a double exponential random variable with mean 0 and scale μ\n\nDouble exponential / exponential: If X is a double exponential random variable with mean 0 and scale λ, then |X| is an exponential random variable with mean λ."}
{"slug": "distributions_excel", "canonical_url": "https://www.johndcook.com/blog/distributions_excel/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/distributions_Excel.html", "title": "Distributions in Excel 2007", "heading": "Probability distributions in Excel 2007", "description": "Working with probability distribution functions in Microsoft Excel.", "summary": "These notes explain how to compute probabilities for common statistical distributions using Microsoft Excel 2007. See also notes on working with distributions in Mathematica, R and S-PLUS and in Python with SciPy.", "word_count": 388, "blocks": [{"tag": "h1", "text": "Probability distributions in Excel 2007"}, {"tag": "p", "text": "These notes explain how to compute probabilities for common statistical distributions using Microsoft Excel 2007. See also notes on working with distributions in Mathematica, R and S-PLUS and in Python with SciPy."}, {"tag": "p", "text": "Probability function support in Excel is incomplete and inconsistent. All distributions have a PDF function. Some have a CDF. Some have an inverse CDF. There's a general pattern to function names, but there are exceptions."}, {"tag": "p", "text": "Probability function names in Excel typically consist of a base name and a suffix. The base name is an abbreviation of the distribution name. The suffix is either DIST or INV. For example, the functions for the normal (Gaussian) distribution are NORMDIST and NORMINV. The corresponding functions for the Gamma distribution are GAMMADIST and GAMMAINV. Several distributions follow this pattern."}, {"tag": "p", "text": "The \"DIST\" function evaluates the PDF and possibly the CDF. If the function has a CUMULATIVE argument, setting this argument to TRUE causes the DIST function to compute the CDF. If the argument is FALSE, the function returns the PDF. If there is no CUMULATIVE argument, the DIST function can only compute the PDF. The \"INV\" function evaluates the inverse CDF (quantile) function."}, {"tag": "p", "text": "(By convention, Excel functions are written in all capital letters. That's the way they are documented. But Excel is case-insensitive.)"}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "The command to evaluate the PDF of a normal distribution with mean 1 and standard deviation 2 at the point x = 3 is"}, {"tag": "p", "text": "NORMDIST(3, 1, 2, FALSE)."}, {"tag": "p", "text": "The FALSE argument says that the cumulative flag is not set. To compute the CDF of the same distribution at the point x = 3, the command is"}, {"tag": "p", "text": "NORMDIST(3, 1, 2, TRUE)."}, {"tag": "p", "text": "To evaluate the inverse CDF (quantile function) of the distribution at p = 0.2 the command is"}, {"tag": "p", "text": "NORMINV(0.2, 1, 2)."}, {"tag": "p", "text": "Not all distributions have an \"INV\" function, as indicated in the table below. Also, the lognormal distribution is exceptional in that its CDF inverse function is named LOGINV rather than LOGNORMINV."}, {"tag": "p", "text": "For the Poisson and Weibull functions, the \"DIST\" function name does not end in \"DIST.\" Instead, these functions are POISSON and WEIBULL."}, {"tag": "p", "text": "The exponential distribution is parameterized in terms of its rate, the reciprocal of the mean. Other distributions that are sometimes parameterized differently are the hypergeometric and the lognormal."}, {"tag": "p", "text": "For more details, see the Excel online documentation or statistical functions."}], "content": "Probability distributions in Excel 2007\n\nThese notes explain how to compute probabilities for common statistical distributions using Microsoft Excel 2007. See also notes on working with distributions in Mathematica, R and S-PLUS and in Python with SciPy.\n\nProbability function support in Excel is incomplete and inconsistent. All distributions have a PDF function. Some have a CDF. Some have an inverse CDF. There's a general pattern to function names, but there are exceptions.\n\nProbability function names in Excel typically consist of a base name and a suffix. The base name is an abbreviation of the distribution name. The suffix is either DIST or INV. For example, the functions for the normal (Gaussian) distribution are NORMDIST and NORMINV. The corresponding functions for the Gamma distribution are GAMMADIST and GAMMAINV. Several distributions follow this pattern.\n\nThe \"DIST\" function evaluates the PDF and possibly the CDF. If the function has a CUMULATIVE argument, setting this argument to TRUE causes the DIST function to compute the CDF. If the argument is FALSE, the function returns the PDF. If there is no CUMULATIVE argument, the DIST function can only compute the PDF. The \"INV\" function evaluates the inverse CDF (quantile) function.\n\n(By convention, Excel functions are written in all capital letters. That's the way they are documented. But Excel is case-insensitive.)\n\nExample:\n\nThe command to evaluate the PDF of a normal distribution with mean 1 and standard deviation 2 at the point x = 3 is\n\nNORMDIST(3, 1, 2, FALSE).\n\nThe FALSE argument says that the cumulative flag is not set. To compute the CDF of the same distribution at the point x = 3, the command is\n\nNORMDIST(3, 1, 2, TRUE).\n\nTo evaluate the inverse CDF (quantile function) of the distribution at p = 0.2 the command is\n\nNORMINV(0.2, 1, 2).\n\nNot all distributions have an \"INV\" function, as indicated in the table below. Also, the lognormal distribution is exceptional in that its CDF inverse function is named LOGINV rather than LOGNORMINV.\n\nFor the Poisson and Weibull functions, the \"DIST\" function name does not end in \"DIST.\" Instead, these functions are POISSON and WEIBULL.\n\nThe exponential distribution is parameterized in terms of its rate, the reciprocal of the mean. Other distributions that are sometimes parameterized differently are the hypergeometric and the lognormal.\n\nFor more details, see the Excel online documentation or statistical functions."}
{"slug": "distributions_mathematica", "canonical_url": "https://www.johndcook.com/blog/distributions_mathematica/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/distributions_Mathematica.html", "title": "Distributions in Mathematica", "heading": "Distributions in Mathematica", "description": "Notes on working with statistical distribution functions in Mathematica.", "summary": "These notes explain how to compute probabilities for common statistical distributions using Mathematica. See also notes on working with distributions in R and S-PLUS, Excel, and in Python with SciPy.", "word_count": 609, "blocks": [{"tag": "h1", "text": "Distributions in Mathematica"}, {"tag": "p", "text": "These notes explain how to compute probabilities for common statistical distributions using Mathematica. See also notes on working with distributions in R and S-PLUS, Excel, and in Python with SciPy."}, {"tag": "h2", "text": "Distribution objects"}, {"tag": "p", "text": "Statistical distributions are standard in Mathematica version 6. Prior to that version, you had to load either the DiscreteDistributions or ContinuousDistributions package. For example, to load the latter you would enter the following."}, {"tag": "pre", "text": "<<Statistics`ContinuousDistributions`"}, {"tag": "p", "text": "As with everything else in Mathematica, names use Pascal case (concatenated capitalized words). The name of every distribution object ends with Distribution. For example, the Mathematica object representing the normal (Gaussian) distribution is NormalDistribution. The arguments to a distribution object constructor are the distribution parameters. (See notes below about possible problems with parameterization conventions.)"}, {"tag": "h2", "text": "Probability density function (PDF)"}, {"tag": "p", "text": "To calculate the PDF (probability density function) of a distribution, pass the distribution as the first argument to PDF[] and the PDF argument as the second argument. For example,"}, {"tag": "pre", "text": "PDF[ GammaDistribution[2, 3], 17.2 ]"}, {"tag": "p", "text": "gives the value of fX(17.2) where fX is the PDF of a random variable X with a gamma distribution with shape parameter 2 and scale parameters 3. For another example,"}, {"tag": "pre", "text": "f[x_] := PDF[ NormalDistribution[0, 1], x ]"}, {"tag": "p", "text": "defines a function f as the PDF of a standard normal random variable."}, {"tag": "p", "text": "Note that Mathematica uses the term \"PDF\" for both continuous and discrete random variables. Technically, discrete distributions have or probability mass functions but Mathematica ignores this pedantic detail."}, {"tag": "h2", "text": "Cumulative density function (CDF)"}, {"tag": "p", "text": "Mathematica computes the CDF (cumulative density function) of a distribution analogously to the way it computes the PDF. For example,"}, {"tag": "pre", "text": "g[x_] := CDF[ NormalDistribution[0, 1], x ]"}, {"tag": "p", "text": "defines g to be CDF of a standard normal random variable."}, {"tag": "h2", "text": "Quantiles (inverse CDF)"}, {"tag": "p", "text": "To compute the quantile function, i.e. the inverse of the CDF function, use the Mathematica function Quantile[] analogous to the functions PDF[] and CDF[] described above."}, {"tag": "h2", "text": "Other associated functions"}, {"tag": "p", "text": "You can find the mean or variance of a distribution by passing a distribution object to Mean[] or Variance[] respectively. To get a random sample, pass a distribution object to Random[]. To get an array of random samples, call RandomArray[]."}, {"tag": "h2", "text": "Distribution names"}, {"tag": "p", "text": "The following gives Mathematica names and parameterizations for common distributions."}, {"tag": "p", "text": "Note that ChiSquareDistribution contains the word \"Square\" but not \"Squared.\" Also, Student's t distribution is StudentTDistribution and not TDistribution."}, {"tag": "p", "text": "The Laplace distribution is also known as the double exponential distribution."}, {"tag": "h2", "text": "Notes on parameterizations"}, {"tag": "p", "text": "You always need to verify parameterizations in statistical software to avoid unexpected results. One way to do this is to pass a distribution object to the Mean[] and Variance[] functions to see whether you get what you expect"}, {"tag": "p", "text": "The exponential distribution is sometimes parameterized in terms of its mean, but Mathematica uses the rate, the reciprocal of the mean or scale."}, {"tag": "p", "text": "Mathematica parameterizes the geometric distribution in terms of its shape and scale. Some other packages use the shape and the rate (reciprocal of the scale)."}, {"tag": "p", "text": "There are two common parameterizations for a hypergeometric distribution. Suppose an urn has M red balls and N blue balls. You draw n balls at once and want to know the probability of various numbers of red balls in your sample. Some software packages parameterize the hypergeometric distribution in terms of n, M, and N, but Mathematica uses n, M, and the total number of balls, M+N."}, {"tag": "p", "text": "If X has a log-normal distribution, then log(X) has a normal distribution. Note that the mean and standard deviation parameters are the mean and standard deviation of log(X), not of X itself. Said another way, X has the same distribution as exp(Y) where Y is a normal random variable with mean and standard deviation given by the parameters."}], "content": "Distributions in Mathematica\n\nThese notes explain how to compute probabilities for common statistical distributions using Mathematica. See also notes on working with distributions in R and S-PLUS, Excel, and in Python with SciPy.\n\nDistribution objects\n\nStatistical distributions are standard in Mathematica version 6. Prior to that version, you had to load either the DiscreteDistributions or ContinuousDistributions package. For example, to load the latter you would enter the following.\n\n<<Statistics`ContinuousDistributions`\n\nAs with everything else in Mathematica, names use Pascal case (concatenated capitalized words). The name of every distribution object ends with Distribution. For example, the Mathematica object representing the normal (Gaussian) distribution is NormalDistribution. The arguments to a distribution object constructor are the distribution parameters. (See notes below about possible problems with parameterization conventions.)\n\nProbability density function (PDF)\n\nTo calculate the PDF (probability density function) of a distribution, pass the distribution as the first argument to PDF[] and the PDF argument as the second argument. For example,\n\nPDF[ GammaDistribution[2, 3], 17.2 ]\n\ngives the value of fX(17.2) where fX is the PDF of a random variable X with a gamma distribution with shape parameter 2 and scale parameters 3. For another example,\n\nf[x_] := PDF[ NormalDistribution[0, 1], x ]\n\ndefines a function f as the PDF of a standard normal random variable.\n\nNote that Mathematica uses the term \"PDF\" for both continuous and discrete random variables. Technically, discrete distributions have or probability mass functions but Mathematica ignores this pedantic detail.\n\nCumulative density function (CDF)\n\nMathematica computes the CDF (cumulative density function) of a distribution analogously to the way it computes the PDF. For example,\n\ng[x_] := CDF[ NormalDistribution[0, 1], x ]\n\ndefines g to be CDF of a standard normal random variable.\n\nQuantiles (inverse CDF)\n\nTo compute the quantile function, i.e. the inverse of the CDF function, use the Mathematica function Quantile[] analogous to the functions PDF[] and CDF[] described above.\n\nOther associated functions\n\nYou can find the mean or variance of a distribution by passing a distribution object to Mean[] or Variance[] respectively. To get a random sample, pass a distribution object to Random[]. To get an array of random samples, call RandomArray[].\n\nDistribution names\n\nThe following gives Mathematica names and parameterizations for common distributions.\n\nNote that ChiSquareDistribution contains the word \"Square\" but not \"Squared.\" Also, Student's t distribution is StudentTDistribution and not TDistribution.\n\nThe Laplace distribution is also known as the double exponential distribution.\n\nNotes on parameterizations\n\nYou always need to verify parameterizations in statistical software to avoid unexpected results. One way to do this is to pass a distribution object to the Mean[] and Variance[] functions to see whether you get what you expect\n\nThe exponential distribution is sometimes parameterized in terms of its mean, but Mathematica uses the rate, the reciprocal of the mean or scale.\n\nMathematica parameterizes the geometric distribution in terms of its shape and scale. Some other packages use the shape and the rate (reciprocal of the scale).\n\nThere are two common parameterizations for a hypergeometric distribution. Suppose an urn has M red balls and N blue balls. You draw n balls at once and want to know the probability of various numbers of red balls in your sample. Some software packages parameterize the hypergeometric distribution in terms of n, M, and N, but Mathematica uses n, M, and the total number of balls, M+N.\n\nIf X has a log-normal distribution, then log(X) has a normal distribution. Note that the mean and standard deviation parameters are the mean and standard deviation of log(X), not of X itself. Said another way, X has the same distribution as exp(Y) where Y is a normal random variable with mean and standard deviation given by the parameters."}
{"slug": "distributions_r_splus", "canonical_url": "https://www.johndcook.com/blog/distributions_r_splus/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/distributions_R_SPLUS.html", "title": "Distributions in R and S-PLUS", "heading": "Distributions in R and S-PLUS", "description": "Notes on working with statistical distribution functions in R and S-PLUS.", "summary": "This page summarizes how to work with univariate probability distributions in R and S-PLUS. See also notes on working with distributions in Mathematica, Excel, and in Python with SciPy.", "word_count": 360, "blocks": [{"tag": "h1", "text": "Distributions in R and S-PLUS"}, {"tag": "p", "text": "This page summarizes how to work with univariate probability distributions in R and S-PLUS. See also notes on working with distributions in Mathematica, Excel, and in Python with SciPy."}, {"tag": "p", "text": "R and S-PLUS use prefixes and bases to denote functions related to a distribution. The prefixes are d, p, q, and r. The bases are the name of the distribution family such as norm for the normal distribution."}, {"tag": "p", "text": "The prefix d is for density, i.e. PDF. The prefix p is for CDF (cumulative density function), unless the argument lower.tail = FALSE is supplied, in which case it turns into the CCDF (complementary CDF). The prefix q is for the CDF inverse, unless the argument lower.tail = FALSE is supplied, in which case it turns into the CCDF inverse. The prefix r is for random sample."}, {"tag": "p", "text": "The first argument to a distribution-related function is the ostensible argument. Next come the distribution parameters followed by other options."}, {"tag": "h2", "text": "Examples"}, {"tag": "p", "text": "pnorm(0.77, 0, 2.1) computes FX(0.77) where X is a normal random variable with mean 0 and standard deviation 2.1 and FX is its CDF."}, {"tag": "p", "text": "dbeta(0.7, 2.1, 3.4) computes fX(0.7) where X is a beta random variable with parameters 2.1 and 3.4 and fX is its PDF."}, {"tag": "p", "text": "qgamma(0.1, 3.1, 1.0, lower.tail = FALSE) finds a value y so that P(Y > y) = 0.1 where Y has a gamma distribution with shape 3.1 and scale 1."}, {"tag": "h2", "text": "Distributions and parameterizations"}, {"tag": "p", "text": "Note that the exponential is parameterized in terms of the rate, the reciprocal of the mean."}, {"tag": "p", "text": "The gamma can be parameterized by its shape and either the rate or the scale. The rate is the default argument by position, but you can specify the scale by name."}, {"tag": "p", "text": "The hypergeometric distribution gives the probability of various numbers of red balls when k balls are taken from an urn containing m red balls and n blue balls. Note that another popular convention uses the number of red balls and the total number of balls m+n."}, {"tag": "p", "text": "Note that the parameters for the log-normal are the mean and standard deviation of the log of the distribution, not the mean and standard deviation of the distribution itself."}], "content": "Distributions in R and S-PLUS\n\nThis page summarizes how to work with univariate probability distributions in R and S-PLUS. See also notes on working with distributions in Mathematica, Excel, and in Python with SciPy.\n\nR and S-PLUS use prefixes and bases to denote functions related to a distribution. The prefixes are d, p, q, and r. The bases are the name of the distribution family such as norm for the normal distribution.\n\nThe prefix d is for density, i.e. PDF. The prefix p is for CDF (cumulative density function), unless the argument lower.tail = FALSE is supplied, in which case it turns into the CCDF (complementary CDF). The prefix q is for the CDF inverse, unless the argument lower.tail = FALSE is supplied, in which case it turns into the CCDF inverse. The prefix r is for random sample.\n\nThe first argument to a distribution-related function is the ostensible argument. Next come the distribution parameters followed by other options.\n\nExamples\n\npnorm(0.77, 0, 2.1) computes FX(0.77) where X is a normal random variable with mean 0 and standard deviation 2.1 and FX is its CDF.\n\ndbeta(0.7, 2.1, 3.4) computes fX(0.7) where X is a beta random variable with parameters 2.1 and 3.4 and fX is its PDF.\n\nqgamma(0.1, 3.1, 1.0, lower.tail = FALSE) finds a value y so that P(Y > y) = 0.1 where Y has a gamma distribution with shape 3.1 and scale 1.\n\nDistributions and parameterizations\n\nNote that the exponential is parameterized in terms of the rate, the reciprocal of the mean.\n\nThe gamma can be parameterized by its shape and either the rate or the scale. The rate is the default argument by position, but you can specify the scale by name.\n\nThe hypergeometric distribution gives the probability of various numbers of red balls when k balls are taken from an urn containing m red balls and n blue balls. Note that another popular convention uses the number of red balls and the total number of balls m+n.\n\nNote that the parameters for the log-normal are the mean and standard deviation of the log of the distribution, not the mean and standard deviation of the distribution itself."}
{"slug": "distributions_scipy", "canonical_url": "https://www.johndcook.com/blog/distributions_scipy/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/distributions_scipy.html", "title": "Distributions in SciPy", "heading": "Distributions in SciPy", "description": "Notes on working with statistical distribution functions in SciPy.", "summary": "This page summarizes how to work with univariate probability distributions using Python's SciPy library. See also notes on working with distributions in Mathematica, Excel, and R/S-PLUS.", "word_count": 504, "blocks": [{"tag": "h1", "text": "Distributions in SciPy"}, {"tag": "p", "text": "This page summarizes how to work with univariate probability distributions using Python's SciPy library. See also notes on working with distributions in Mathematica, Excel, and R/S-PLUS."}, {"tag": "p", "text": "Probability distribution classes are located in scipy.stats."}, {"tag": "p", "text": "The methods on continuous distribution classes are as follows."}, {"tag": "p", "text": "Functions such as pdf and cdf are defined over the entire real line. For example, the beta distribution is commonly defined on the interval [0, 1]. If you ask for the pdf outside this interval, you simply get 0. If you ask for the cdf to the left of the interval you get 0, and to the right of the interval you get 1."}, {"tag": "p", "text": "Distributions have a general form and a \"frozen\" form. The general form is stateless: you supply the distribution parameters as arguments to every call. The frozen form creates an object with the distribution parameters set. For example, you could evaluate the PDF of a normal(3, 4) distribution at the value 5 by"}, {"tag": "pre", "text": "stats.norm.pdf(5, 3, 4)"}, {"tag": "p", "text": "or by"}, {"tag": "pre", "text": "mydist = stats.norm(3, 4)\n\tmydist.pdf(5)"}, {"tag": "p", "text": "Note that the argument of the PDF, in this example 5, comes before the distribution parameters. Note also that for discrete distributions, one would call pmf (probability mass function) rather than the pdf (probability density function)."}, {"tag": "h2", "text": "Distributions and parameterizations"}, {"tag": "p", "text": "SciPy makes every continuous distribution into a location-scale family, including some distributions that typically do not have location scale parameters. This unusual approach has its advantages. For example, the question of whether an exponential distribution is parameterized in terms of its mean or its rate goes away: there is no mean or rate parameter per se, only a scale parameter like every other continuous distribution."}, {"tag": "p", "text": "The table below only lists parameters in addition to location and scale."}, {"tag": "p", "text": "SciPy does not have a simple Weibull distribution but instead has a generalization of the Weibull called the exponentiated Weibull. Set the exponential parameter to 1 and you get the ordinary Weibull distribution."}, {"tag": "p", "text": "The hypergeometric distribution gives the probability of various numbers of red balls when N balls are taken from an urn containing n red balls and M-n blue balls. Note that another popular convention uses the number of red and blue balls rather than the number of red balls and the total number of balls."}, {"tag": "p", "text": "Note that the parameters for the log-normal are the mean and standard deviation of the log of the distribution, not the mean and standard deviation of the distribution itself."}, {"tag": "p", "text": "The PDF or PMF of a distribution is contained in the extradoc string. For example:"}, {"tag": "pre", "text": ">>> stats.poisson.extradoc\n\tPoisson distribution\n\tpoisson.pmf(k, mu) = exp(-mu) * mu**k / k!\n\tfor k >= 0"}, {"tag": "p", "text": "The lognormal distribution as implemented in SciPy may not be the same as the lognormal distribution implemented elsewhere. When the location parameter is 0, the stats.lognorm with parameter s corresponds to a lognormal(0, s) distribution as defined here. But if the location parameter is not 0, stats.lognorm does not correspond to a log-normal distribution under the other distribution. The difference is whether the PDF contains log(x-μ) or log(x) - μ."}], "content": "Distributions in SciPy\n\nThis page summarizes how to work with univariate probability distributions using Python's SciPy library. See also notes on working with distributions in Mathematica, Excel, and R/S-PLUS.\n\nProbability distribution classes are located in scipy.stats.\n\nThe methods on continuous distribution classes are as follows.\n\nFunctions such as pdf and cdf are defined over the entire real line. For example, the beta distribution is commonly defined on the interval [0, 1]. If you ask for the pdf outside this interval, you simply get 0. If you ask for the cdf to the left of the interval you get 0, and to the right of the interval you get 1.\n\nDistributions have a general form and a \"frozen\" form. The general form is stateless: you supply the distribution parameters as arguments to every call. The frozen form creates an object with the distribution parameters set. For example, you could evaluate the PDF of a normal(3, 4) distribution at the value 5 by\n\nstats.norm.pdf(5, 3, 4)\n\nor by\n\nmydist = stats.norm(3, 4)\n\tmydist.pdf(5)\n\nNote that the argument of the PDF, in this example 5, comes before the distribution parameters. Note also that for discrete distributions, one would call pmf (probability mass function) rather than the pdf (probability density function).\n\nDistributions and parameterizations\n\nSciPy makes every continuous distribution into a location-scale family, including some distributions that typically do not have location scale parameters. This unusual approach has its advantages. For example, the question of whether an exponential distribution is parameterized in terms of its mean or its rate goes away: there is no mean or rate parameter per se, only a scale parameter like every other continuous distribution.\n\nThe table below only lists parameters in addition to location and scale.\n\nSciPy does not have a simple Weibull distribution but instead has a generalization of the Weibull called the exponentiated Weibull. Set the exponential parameter to 1 and you get the ordinary Weibull distribution.\n\nThe hypergeometric distribution gives the probability of various numbers of red balls when N balls are taken from an urn containing n red balls and M-n blue balls. Note that another popular convention uses the number of red and blue balls rather than the number of red balls and the total number of balls.\n\nNote that the parameters for the log-normal are the mean and standard deviation of the log of the distribution, not the mean and standard deviation of the distribution itself.\n\nThe PDF or PMF of a distribution is contained in the extradoc string. For example:\n\n>>> stats.poisson.extradoc\n\tPoisson distribution\n\tpoisson.pmf(k, mu) = exp(-mu) * mu**k / k!\n\tfor k >= 0\n\nThe lognormal distribution as implemented in SciPy may not be the same as the lognormal distribution implemented elsewhere. When the location parameter is 0, the stats.lognorm with parameter s corresponds to a lognormal(0, s) distribution as defined here. But if the location parameter is not 0, stats.lognorm does not correspond to a log-normal distribution under the other distribution. The difference is whether the PDF contains log(x-μ) or log(x) - μ."}
{"slug": "double_exponential_integration", "canonical_url": "https://www.johndcook.com/blog/double_exponential_integration/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/double_exponential_integration.html", "title": "Double exponential integration", "heading": "Double exponential integration", "description": "Details on the double exponential transformation method in numerical integration.", "summary": "This page gives mathematical details behind the double exponential transformation technique for numerical integration. Software for implementing this method is available in the article Fast Numerical Integration [link died].", "word_count": 431, "blocks": [{"tag": "h1", "text": "Double exponential integration"}, {"tag": "p", "text": "This page gives mathematical details behind the double exponential transformation technique for numerical integration. Software for implementing this method is available in the article Fast Numerical Integration [link died]."}, {"tag": "p", "text": "The trapezoid rule is a crude method of numerical integration often presented in calculus classes. The error in approximating integrals by the trapezoid rule is typically too large for the method to be practical in application. However, in some special cases, this crude method is astonishingly accurate. In particular, the trapezoid rule works remarkably well for integrating analytic functions over the real line that approach zero rapidly in the tails (i.e. f(x) goes to zero like exp(-c exp |x|) as |x| → ∞). The double exponential integration method takes advantage the excellent performance in this case by transforming other integrals to have this form."}, {"tag": "h2", "text": "Transformation from finite interval to real line"}, {"tag": "p", "text": "There have been several transformations studied for transforming a function over the finite open interval (a, b) into a function over the real line (-∞, ∞). We use the transformation"}, {"tag": "p", "text": "x = tanh( π sinh(t)/2 )"}, {"tag": "p", "text": "in the software referenced above to map x values in (-1, 1) into t values in (-∞, ∞). The functions hyperbolic tangent and hyperbolic sine functions are not evaluated at run time; they enter the software via pre-calculated node and weight values."}, {"tag": "p", "text": "For information on other transformations from finite intervals to the real line in the double exponential integration method, see [1]. Also, [2] has more information about the specific transformation x = tanh( π sinh(t)/2 )."}, {"tag": "h2", "text": "Error estimates"}, {"tag": "p", "text": "The error in the double exponential integration method is on the order of"}, {"tag": "p", "text": "exp( - c N/log(N) )"}, {"tag": "p", "text": "where N is the number of integration points. This is optimal in the sense that no integration method achieves better accuracy for the same number of function evaluations for integrands belonging to the Hardy space Hp for p > 1."}, {"tag": "p", "text": "Note that while elementary numerical integration schemes have error estimates that decrease like a polynomial in N, ie. N-k, the method presented here has error estimates that decrease exponentially."}, {"tag": "h2", "text": "References"}, {"tag": "p", "text": "[1] Masatake Mori and Masaaki Sugihara. The double-exponential transformation in numerical analysis. Journal of Computational and Applied Mathematics, 127 (2001), 287–296."}, {"tag": "p", "text": "[2] Mayinur Muhammad and Masatake Mori. Double exponential formulas for numerical indefinite integration. Journal of Computationa dn Applied Mathematics, 161 (2003) 431–448."}, {"tag": "p", "text": "[3] Masatake Mori. Quadrature formulas obtained by variable transformation and the DE-rule, Journal of Computational and Applied Mathematics 12 & 13 (1985) 119–130."}, {"tag": "p", "text": "[4] Hidetosi Takahasi and Masatake Mori. Double Exponential Formulas for Numerical Integration. Publ. RIMS, Kyoto University 9 (1974) 721–741."}], "content": "Double exponential integration\n\nThis page gives mathematical details behind the double exponential transformation technique for numerical integration. Software for implementing this method is available in the article Fast Numerical Integration [link died].\n\nThe trapezoid rule is a crude method of numerical integration often presented in calculus classes. The error in approximating integrals by the trapezoid rule is typically too large for the method to be practical in application. However, in some special cases, this crude method is astonishingly accurate. In particular, the trapezoid rule works remarkably well for integrating analytic functions over the real line that approach zero rapidly in the tails (i.e. f(x) goes to zero like exp(-c exp |x|) as |x| → ∞). The double exponential integration method takes advantage the excellent performance in this case by transforming other integrals to have this form.\n\nTransformation from finite interval to real line\n\nThere have been several transformations studied for transforming a function over the finite open interval (a, b) into a function over the real line (-∞, ∞). We use the transformation\n\nx = tanh( π sinh(t)/2 )\n\nin the software referenced above to map x values in (-1, 1) into t values in (-∞, ∞). The functions hyperbolic tangent and hyperbolic sine functions are not evaluated at run time; they enter the software via pre-calculated node and weight values.\n\nFor information on other transformations from finite intervals to the real line in the double exponential integration method, see [1]. Also, [2] has more information about the specific transformation x = tanh( π sinh(t)/2 ).\n\nError estimates\n\nThe error in the double exponential integration method is on the order of\n\nexp( - c N/log(N) )\n\nwhere N is the number of integration points. This is optimal in the sense that no integration method achieves better accuracy for the same number of function evaluations for integrands belonging to the Hardy space Hp for p > 1.\n\nNote that while elementary numerical integration schemes have error estimates that decrease like a polynomial in N, ie. N-k, the method presented here has error estimates that decrease exponentially.\n\nReferences\n\n[1] Masatake Mori and Masaaki Sugihara. The double-exponential transformation in numerical analysis. Journal of Computational and Applied Mathematics, 127 (2001), 287–296.\n\n[2] Mayinur Muhammad and Masatake Mori. Double exponential formulas for numerical indefinite integration. Journal of Computationa dn Applied Mathematics, 161 (2003) 431–448.\n\n[3] Masatake Mori. Quadrature formulas obtained by variable transformation and the DE-rule, Journal of Computational and Applied Mathematics 12 & 13 (1985) 119–130.\n\n[4] Hidetosi Takahasi and Masatake Mori. Double Exponential Formulas for Numerical Integration. Publ. RIMS, Kyoto University 9 (1974) 721–741."}
{"slug": "draw_icosahedron", "canonical_url": "https://www.johndcook.com/blog/draw_icosahedron/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/draw_icosahedron.html", "title": "Drawing an icosahedron", "heading": "Drawing an icosahedron", "description": "How to draw an icosahedron, or any other Platonic solid, using Mathemtaica.", "summary": "Here’s how I drew the icosahedron in this post.", "word_count": 105, "blocks": [{"tag": "h1", "text": "Drawing an icosahedron"}, {"tag": "p", "text": "Here’s how I drew the icosahedron in this post."}, {"tag": "p", "text": "I started with the following Mathematica code:"}, {"tag": "pre", "text": "Graphics3D[{PolyhedronData[\"Icosahedron\", \"Faces\"]}, Boxed -> False]"}, {"tag": "p", "text": "By default, Mathematica draws 3D figures in a bounding box. The option Boxed -> False removes the box to just show the icosahedron."}, {"tag": "p", "text": "The PolyhedraData function can take many other polyhedra as arguments."}, {"tag": "p", "text": "I exported the image from Mathematica as an SVG file and used Inkscape to export the SVG file to a PNG image. I could have saved the image directly to PNG from Mathematica, but the Mathematica image was jagged and I thought Inkscape might handle the edges better."}], "content": "Drawing an icosahedron\n\nHere’s how I drew the icosahedron in this post.\n\nI started with the following Mathematica code:\n\nGraphics3D[{PolyhedronData[\"Icosahedron\", \"Faces\"]}, Boxed -> False]\n\nBy default, Mathematica draws 3D figures in a bounding box. The option Boxed -> False removes the box to just show the icosahedron.\n\nThe PolyhedraData function can take many other polyhedra as arguments.\n\nI exported the image from Mathematica as an SVG file and used Inkscape to export the SVG file to a PNG image. I could have saved the image directly to PNG from Mathematica, but the Mathematica image was jagged and I thought Inkscape might handle the edges better."}
{"slug": "emacs_kill_commands", "canonical_url": "https://www.johndcook.com/blog/emacs_kill_commands/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/emacs_kill_commands.html", "title": "Emacs kill and cut commands", "heading": "Emacs kill (cut) commands", "description": "Emacs commands for killing (cutting) text", "summary": "Emacs uses the term \"kill\" for what most people call \"cut.\" It keeps a \"kill ring,\" a history of kills, similar to what most people call a \"clipboard.\"", "word_count": 268, "blocks": [{"tag": "h1", "text": "Emacs kill (cut) commands"}, {"tag": "p", "text": "Emacs uses the term \"kill\" for what most people call \"cut.\" It keeps a \"kill ring,\" a history of kills, similar to what most people call a \"clipboard.\""}, {"tag": "p", "text": "The most general way to cut a chunk of text is to select the text as a region and then use C-w. (You can select the region by using C-SPACE at one end and then moving the point (cursor) to the other end.) But there are more convenient commands for killing common units of text."}, {"tag": "p", "text": "Also, you can kill an entire line with C-SHIFT-DEL."}, {"tag": "p", "text": "These commands are not as symmetric as the commands for cursor movement. They are like irregular verbs in the Emacs grammar. But they're not entirely irregular."}, {"tag": "p", "text": "The commands for kill to the end of a line and end of a sentence follow the usual pattern: control commands for lines, corresponding meta command for sentences."}, {"tag": "p", "text": "The DEL key deletes backward a character. M-DEL kills backward a word, and C-x DEL kills backward a sentence."}, {"tag": "p", "text": "C-d deletes the character in front of the cursor, and M-d kills the word in front of the cursor (or to the end of the word the cursor is in). As usual, the control command is for basic units (characters) and the meta command is for context-dependent units (words)."}, {"tag": "p", "text": "The command for killing to the beginning of a sentence is just odd. It's the same command as killing to the end of a sentence, but with a zero argument."}, {"tag": "h2", "text": "Emacs resources"}, {"tag": "p", "text": "Getting started with Emacs on Windows Emacs cursor movement Emacs and Unicode One program to rule them all"}], "content": "Emacs kill (cut) commands\n\nEmacs uses the term \"kill\" for what most people call \"cut.\" It keeps a \"kill ring,\" a history of kills, similar to what most people call a \"clipboard.\"\n\nThe most general way to cut a chunk of text is to select the text as a region and then use C-w. (You can select the region by using C-SPACE at one end and then moving the point (cursor) to the other end.) But there are more convenient commands for killing common units of text.\n\nAlso, you can kill an entire line with C-SHIFT-DEL.\n\nThese commands are not as symmetric as the commands for cursor movement. They are like irregular verbs in the Emacs grammar. But they're not entirely irregular.\n\nThe commands for kill to the end of a line and end of a sentence follow the usual pattern: control commands for lines, corresponding meta command for sentences.\n\nThe DEL key deletes backward a character. M-DEL kills backward a word, and C-x DEL kills backward a sentence.\n\nC-d deletes the character in front of the cursor, and M-d kills the word in front of the cursor (or to the end of the word the cursor is in). As usual, the control command is for basic units (characters) and the meta command is for context-dependent units (words).\n\nThe command for killing to the beginning of a sentence is just odd. It's the same command as killing to the end of a sentence, but with a zero argument.\n\nEmacs resources\n\nGetting started with Emacs on Windows Emacs cursor movement Emacs and Unicode One program to rule them all"}
{"slug": "emacs_move_cursor", "canonical_url": "https://www.johndcook.com/blog/emacs_move_cursor/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/emacs_move_cursor.html", "title": "Emacs point (cursor) movement", "heading": "Emacs point (cursor) movement", "description": "Emacs commands for moving the point (cursor)", "summary": "One of the features of Emacs is fine control of things such as cursor movement. (Emacs \"point\" is essentially what the rest of humanity calls \"cursor.\") If you want to move forward or backward a word, a sentence, a line, or a paragraph, you can do so directly rather than just holding down arrow keys until you arrive at your ...", "word_count": 203, "blocks": [{"tag": "h1", "text": "Emacs point (cursor) movement"}, {"tag": "p", "text": "One of the features of Emacs is fine control of things such as cursor movement. (Emacs \"point\" is essentially what the rest of humanity calls \"cursor.\") If you want to move forward or backward a word, a sentence, a line, or a paragraph, you can do so directly rather than just holding down arrow keys until you arrive at your destination. This page summarizes the basic commands for moving the point"}, {"tag": "p", "text": "Move by"}, {"tag": "p", "text": "Move to"}, {"tag": "p", "text": "Notice that control commands generally move by context-independent units (characters, lines) whereas meta commands move by context-dependent units (words, sentences, paragraphs). These commands are often parallel, such as the commands for moving by lines versus sentences."}, {"tag": "p", "text": "(What constitutes something like a \"word\" or \"sentence\" or \"paragraph\" depends on context. It may mean one thing in English prose and another thing in a source code file.)"}, {"tag": "p", "text": "Notice also that the commands are somewhat mnemonic:"}, {"tag": "li", "text": "\"f\" stands for \"forward\""}, {"tag": "li", "text": "\"b\" stands for \"backward\""}, {"tag": "li", "text": "\"n\" stands for \"next\""}, {"tag": "li", "text": "\"p\" stands for \"previous\""}, {"tag": "li", "text": "\"a\" stands for \"beginning\" (like the beginning of the alphabet)"}, {"tag": "li", "text": "\"e\" stands for \"end\""}, {"tag": "h2", "text": "Emacs resources"}, {"tag": "p", "text": "Getting started with Emacs on Windows Emacs kill (cut) commands Emacs and Unicode One program to rule them all"}], "content": "Emacs point (cursor) movement\n\nOne of the features of Emacs is fine control of things such as cursor movement. (Emacs \"point\" is essentially what the rest of humanity calls \"cursor.\") If you want to move forward or backward a word, a sentence, a line, or a paragraph, you can do so directly rather than just holding down arrow keys until you arrive at your destination. This page summarizes the basic commands for moving the point\n\nMove by\n\nMove to\n\nNotice that control commands generally move by context-independent units (characters, lines) whereas meta commands move by context-dependent units (words, sentences, paragraphs). These commands are often parallel, such as the commands for moving by lines versus sentences.\n\n(What constitutes something like a \"word\" or \"sentence\" or \"paragraph\" depends on context. It may mean one thing in English prose and another thing in a source code file.)\n\nNotice also that the commands are somewhat mnemonic:\n\n\"f\" stands for \"forward\"\n\n\"b\" stands for \"backward\"\n\n\"n\" stands for \"next\"\n\n\"p\" stands for \"previous\"\n\n\"a\" stands for \"beginning\" (like the beginning of the alphabet)\n\n\"e\" stands for \"end\"\n\nEmacs resources\n\nGetting started with Emacs on Windows Emacs kill (cut) commands Emacs and Unicode One program to rule them all"}
{"slug": "emacs_unicode", "canonical_url": "https://www.johndcook.com/blog/emacs_unicode/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/emacs_unicode.html", "title": "Notes on Unicode and Emacs", "heading": "Notes on Unicode in Emacs", "description": "Notes on using Unicode in Emacs", "summary": "Much more information is available in the Emacs online help and in the Emacs manual. This page just gives some brief notes.", "word_count": 218, "blocks": [{"tag": "h1", "text": "Notes on Unicode in Emacs"}, {"tag": "p", "text": "Much more information is available in the Emacs online help and in the Emacs manual. This page just gives some brief notes."}, {"tag": "h2", "text": "How can I enter accented letters not present on your keyboard?"}, {"tag": "p", "text": "You can type C-x 8 followed by a punctuation mark and the letter to accent. For example, C-x 8 ’ e inserts é. The punctuation marks used to specify accents are fairly mnemonic. Here are some of the more common examples."}, {"tag": "p", "text": "For more information, type C-x 8 C-h"}, {"tag": "h2", "text": "How can I enter arbitrary Unicode characters?"}, {"tag": "p", "text": "C-x 8 RET followed by either the code point in hex or the name of the official Unicode name of the character."}, {"tag": "h2", "text": "How can I find out information about a character?"}, {"tag": "p", "text": "C-u C-x = will give information about the character under the cursor."}, {"tag": "h2", "text": "How can I tell what encoding your file is using?"}, {"tag": "p", "text": "Type C-h C RET. (Note that’s an upper case C.) This will tell you the coding system for the current buffer and much more."}, {"tag": "h2", "text": "How can I change a file’s encoding?"}, {"tag": "p", "text": "C-x RET f will prompt you for an encoding. You could then enter utf-8, for example, to specify UTF-8 encoding."}, {"tag": "h2", "text": "Related resources"}, {"tag": "p", "text": "Emacs cursor movement Emacs kill (cut) commands Using Emacs on Windows Accented letters in HTML, TeX, and Microsoft Word"}], "content": "Notes on Unicode in Emacs\n\nMuch more information is available in the Emacs online help and in the Emacs manual. This page just gives some brief notes.\n\nHow can I enter accented letters not present on your keyboard?\n\nYou can type C-x 8 followed by a punctuation mark and the letter to accent. For example, C-x 8 ’ e inserts é. The punctuation marks used to specify accents are fairly mnemonic. Here are some of the more common examples.\n\nFor more information, type C-x 8 C-h\n\nHow can I enter arbitrary Unicode characters?\n\nC-x 8 RET followed by either the code point in hex or the name of the official Unicode name of the character.\n\nHow can I find out information about a character?\n\nC-u C-x = will give information about the character under the cursor.\n\nHow can I tell what encoding your file is using?\n\nType C-h C RET. (Note that’s an upper case C.) This will tell you the coding system for the current buffer and much more.\n\nHow can I change a file’s encoding?\n\nC-x RET f will prompt you for an encoding. You could then enter utf-8, for example, to specify UTF-8 encoding.\n\nRelated resources\n\nEmacs cursor movement Emacs kill (cut) commands Using Emacs on Windows Accented letters in HTML, TeX, and Microsoft Word"}
{"slug": "emacs_windows", "canonical_url": "https://www.johndcook.com/blog/emacs_windows/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/emacs_windows.html", "title": "Using Emacs on Windows", "heading": "Getting started with Emacs on Windows", "description": "Setting up Emacs on Windows and first steps in learning to use it", "summary": "These notes summarize some of my discoveries (re-)learning GNU Emacs. Since these are my personal notes, it may help to briefly describe my background. I used Emacs on Unix from somewhere around 1990 until 1995. Then in 1995 I began using Windows as my primary operating system and stopped using Emacs. In 2010 I decided to give Emacs on Windows ...", "word_count": 2458, "blocks": [{"tag": "h1", "text": "Getting started with Emacs on Windows"}, {"tag": "p", "text": "These notes summarize some of my discoveries (re-)learning GNU Emacs. Since these are my personal notes, it may help to briefly describe my background. I used Emacs on Unix from somewhere around 1990 until 1995. Then in 1995 I began using Windows as my primary operating system and stopped using Emacs. In 2010 I decided to give Emacs on Windows another try. I may not mention some basic things just because I remember them from my initial experience."}, {"tag": "p", "text": "These notes are not a thorough introduction to Emacs. For a more systematic reference, the Emacs Wiki is a good place to start. I wanted to write these things down for future reference, and I put this file up on my website in case someone else finds it useful. If you have comments or corrections, please let me know."}, {"tag": "p", "text": "Table of contents"}, {"tag": "li", "text": "Installation and configuration"}, {"tag": "li", "text": "Installing Emacs and setting up .emacs"}, {"tag": "li", "text": "Backup files"}, {"tag": "li", "text": "Recycle bin"}, {"tag": "li", "text": "Integration with the Windows File Explorer"}, {"tag": "li", "text": "Getting rid of the start-up screen and toolbar"}, {"tag": "li", "text": "Changing fonts"}, {"tag": "li", "text": "Enabling commands to change case"}, {"tag": "li", "text": "Spell check"}, {"tag": "li", "text": "Installing color-theme"}, {"tag": "li", "text": "Installing nXhtml"}, {"tag": "li", "text": "Installing powershell-mode"}, {"tag": "li", "text": "Remapping my keyboard"}, {"tag": "li", "text": "Line wrapping"}, {"tag": "li", "text": "Column position"}, {"tag": "li", "text": "Emacs vocabulary"}, {"tag": "li", "text": "Editing LaTeX"}, {"tag": "li", "text": "Editing source code"}, {"tag": "li", "text": "Selecting and delting text"}, {"tag": "li", "text": "Searching and replacing"}, {"tag": "li", "text": "Searching for strings"}, {"tag": "li", "text": "Regular expressions"}, {"tag": "li", "text": "Replacing"}, {"tag": "li", "text": "Saving text and positions"}, {"tag": "li", "text": "Saving text"}, {"tag": "li", "text": "Saving positions"}, {"tag": "li", "text": "The Emacs help system"}, {"tag": "li", "text": "Navigating files, buffers, and windows"}, {"tag": "li", "text": "Files"}, {"tag": "li", "text": "Buffers"}, {"tag": "li", "text": "Windows"}, {"tag": "li", "text": "Miscellaneous commands"}, {"tag": "li", "text": "Emacs resources"}, {"tag": "h2", "text": "Installation and configuration"}, {"tag": "h3", "text": "Installing Emacs and setting up .emacs"}, {"tag": "p", "text": "I install all my Unix-like software under C:\\bin and have that directory in my Windows PATH environment variable. I installed Emacs 23.1 in C:\\bin\\emacs-23.1 and created an environment variable HOME set to C:\\bin. The significance of HOME is that Emacs can find your configuration file if you put it there. This file is called “dot emacs” because the traditional name for the file on Unix systems is .emacs. On Windows, it is more convenient to name the file _emacs. (You can also name the file _emacs.el. Giving the file the .el extension causes Emacs to open the file in Lisp mode. And if someday the file becomes huge, you can compile it to make startup faster.)"}, {"tag": "h3", "text": "Backup files"}, {"tag": "p", "text": "Emacs automatically saves backup versions of file and by default leaves these backup files beside the files being edited. This can be annoying. Some people call these extra files “Emacs droppings.” Adding the following lines to .emacs instructs Emacs to put all backup files in a temporary folder."}, {"tag": "pre", "text": "(setq backup-directory-alist\n`((\".*\" . ,temporary-file-directory)))\n(setq auto-save-file-name-transforms\n`((\".*\" ,temporary-file-directory t)))"}, {"tag": "h3", "text": "Recycle Bin"}, {"tag": "p", "text": "The following line configures Emacs so that files deleted via Emacs are moved to the Recycle."}, {"tag": "pre", "text": "(setq delete-by-moving-to-trash t)"}, {"tag": "h3", "text": "Integration with the Windows File Explorer"}, {"tag": "p", "text": "The following registry script creates an “Open with Emacs” option in the Windows file explorer context menu."}, {"tag": "pre", "text": "Windows Registry Editor Version 5.00\n\n[HKEY_CLASSES_ROOT\\*\\Shell\\Open In Emacs\\Command]\n@=\"\\\"C:\\\\bin\\\\Emacs-23.1\\\\bin\\\\emacsclientw.exe\\\" -a \\\"C:\\\\bin\\\\Emacs-23.1\\\\bin\\\\runemacs.exe\\\" \\\"%1\\\"\""}, {"tag": "p", "text": "Putting these two lines in .emacs creates a menu item File -> Open recent."}, {"tag": "pre", "text": "(require 'recentf)\n(recentf-mode 1)"}, {"tag": "p", "text": "If you have a desktop shortcut to runemacs, you can open a file in Emacs by dropping it on the shortcut icon."}, {"tag": "h3", "text": "Getting rid of the start-up screen and toolbar"}, {"tag": "p", "text": "I turned off initial start-up screen by adding (setq inhibit-startup-screen t) to .emacs. This had the pleasant side effect of making the “Open with Emacs” context menu work as expected. (Before, Emacs would open with a split window. Now it opens with just the “sent” file.)"}, {"tag": "p", "text": "Got rid of the toolbar by using the configuration editor under Options / Customize Emacs."}, {"tag": "h3", "text": "Changing fonts"}, {"tag": "p", "text": "I used the Options menu to change the default font to Consolas."}, {"tag": "h3", "text": "Enabling commands to change case"}, {"tag": "p", "text": "The commands for converting the text in a region to upper or lower case are disabled by default. (The GNU Emacs manual says beginners find these commands confusing and so you have turn them on. That seems very strange. Many other Emacs commands are more confusing.) The following turns the commands on."}, {"tag": "pre", "text": "(put 'upcase-region 'disabled nil)\n(put 'downcase-region 'disabled nil)"}, {"tag": "p", "text": "Once this is enabled, you can make the text in a region lowercase with C-x C-l or uppercase with C-x C-u."}, {"tag": "h3", "text": "Spell check"}, {"tag": "p", "text": "GNU Emacs does not provide a spell checker. Instead, it provides hooks to install your own spell checker, usually Aspell. I downloaded Aspell version 0.50.3 (win32) from here. I then installed the English dictionary from the same page. The dictionary installer warned me that Aspell was already installed and suggested that I uninstall it. I did, thinking that it might install a newer version. That didn't work. I re-installed Aspell, then installed the dictionary, ignoring the warning. Everything worked fine."}, {"tag": "p", "text": "After installing Aspell, I let Emacs know where to find it by adding these lines to my .emacs file."}, {"tag": "pre", "text": "(setq-default ispell-program-name \"C:/bin/Aspell/bin/aspell.exe\")\n(setq text-mode-hook '(lambda() (flyspell-mode t) ))"}, {"tag": "p", "text": "The command M-x ispell will run the spell checker on your file. If flyspell-mode is turned on, as it is in the lines above, misspelled words are underlined in red as you type."}, {"tag": "h3", "text": "Installing color-theme"}, {"tag": "p", "text": "It was difficult to find a more direct way to configure the color schemes that Emacs uses, so I installed color-theme version 6.60. I then used a color theme creator to create a basic theme then tweaked the colors."}, {"tag": "h3", "text": "Installing nXhtml"}, {"tag": "p", "text": "The default support for editing HTML files was less than I expected. I heard good things about nXhtml and decided to go with it. Notice that it inserts extra menus when you open a file in nXhtml mode. You can use the commands from the menu until you learn their keyboard shortcuts."}, {"tag": "p", "text": "nXhtml mode requires HTML to be valid XHTML. If your HTML is not valid, you can use HTML Tidy to bring it into standard compliance. HTML Tidy appears as a menu option under nXhtml, but it must be installed separately. Installing HTML Tidy is very simple: download two files, the executable and a DLL, and copy them to somewhere in your path. Once HTML Tidy is installed, it will continually check the validity of the XHTML. It will display its status in the mode line and will turn angle brackets red that are not in the correct place."}, {"tag": "p", "text": "Incidentally, the table of contents for this page was automatically generated using nXhtml. Just give every <h> tag an id. Then you can use commands from the nXhtml menu to insert the table of contents and its style sheet."}, {"tag": "p", "text": "NB: Apparently the nXhtml code does not allow a space on either side of the equal sign when specifying the id value."}, {"tag": "h3", "text": "Installing powershell-mode"}, {"tag": "p", "text": "I installed a mode for editing PowerShell code by copying powershell-mode.el, downloaded from here, by copying the file to C:\\bin\\emacs-23.1\\site-lisp, which is in my Emacs load-path. I tried installing some code that would allow me to run PowerShell as a shell inside Emacs. That did not work on the first try and I did not pursue it further."}, {"tag": "h3", "text": "Remapping my keyboard"}, {"tag": "p", "text": "Many emacs users recommend remapping your keyboard so that the caps lock key becomes a control key. I don't like the idea of changing my keyboard just to accommodate one program, even a program I may use very often. However, I recently bought a laptop that came with a Fn key right where my muscle memory expects the left control key. I hardly ever use the caps lock key, so I made it a control key for the sake of emacs and for making it easier to use my laptop. I mapped the scroll lock key, a key I have not used in a decade or two, to caps lock in case I ever need a caps lock key. My initial intention was to keep the original left control key as an addition control key, but then I disabled it to force myself to get into the habit of using my new control key. I mapped the keyboard of every computer I use to be the same. This has been hard to get used to."}, {"tag": "p", "text": "I used the KeyTweak application to remap my keyboards."}, {"tag": "p", "text": "I don't know what I want to do for my “Meta” key. For now I'm using the Esc key. Some recommend using the original Control key after remapping the Caps Lock key. I have two problems with that: it will not work on my laptop, and I first have to break my habit of using the original Control key as a Control key. (Why not just remap the Fn key on my laptop? Unfortunately this key cannot be remapped like an ordinary key.) I may try to get in the habit of using the right Alt key as my Meta key."}, {"tag": "h3", "text": "Line wrapping"}, {"tag": "p", "text": "I set global-visual-line-mode as the default way to handle line wrap. I did this through the menu sequence Options / Customize Emacs / Specific Option. This causes text to flow as it does in most Windows programs."}, {"tag": "h3", "text": "Column position"}, {"tag": "p", "text": "By default, Emacs displays the current line number in the mode line but not the current column number. To display the column number, add the following to your .emacs file."}, {"tag": "pre", "text": "(setq column-number-mode t)"}, {"tag": "h2", "text": "Emacs vocabulary"}, {"tag": "p", "text": "Emacs uses a set of terminology that is not commonly used elsewhere. The following correspondences are not exact, but they are a good first approximation."}, {"tag": "p", "text": "The “echo area” is the very bottom of an Emacs window. It echoes commands, displays the minibuffer, and provides a place to type extra arguments for commands."}, {"tag": "h2", "text": "Editing LaTeX"}, {"tag": "p", "text": "One of the most useful key sequences for editing LaTeX files are C-c C-o to insert a \\begin and \\end pair. Emacs will prompt you for the keyword to put inside the \\begin{} statement. Another useful key sequence is C-c C-f to run latex on a file. (Emacs can detect whether a file is plain TeX or LaTeX. I use LaTeX exclusively.)"}, {"tag": "p", "text": "There is Emacs package AUCTex for editing (La)TeX files, but I have not tried it."}, {"tag": "p", "text": "I would like to have C-c C-f run pdflatex rather than latex, but I have not found out how to configure that."}, {"tag": "h2", "text": "Editing source code"}, {"tag": "p", "text": "Here are a few useful commands for editing source code files."}, {"tag": "p", "text": "I put these lines in my .emacs file to make the C++ mode behave more like what I am accustomed to."}, {"tag": "pre", "text": "(add-hook 'c++-mode-hook\n  '(lambda ()\n     (c-set-style \"stroustrup\")\n     (setq indent-tabs-mode nil)))"}, {"tag": "h2", "text": "Selecting and delting text"}, {"tag": "p", "text": "C-x h selects the entire current buffer."}, {"tag": "p", "text": "You select a region by using C-SPACE at one end of the region and a selection command and moving the point (cursor) to the other end of the region. Then you can use C-w to cut or M-y to copy. The paste command is C-y. Emacs maintains a “kill ring”, something analogous to the Windows clipboard but containing more than just the latest cut or copy. For example, C-y M-y. lets you paste the next-to-last thing that was cut. Use M-y again to paste the cut before that, etc."}, {"tag": "p", "text": "You can kill all but one whitespace character with M-SPACE. You can kill all but one blank line with C-x C-o."}, {"tag": "p", "text": "Emacs has commands for working with rectangular regions, analogous to vertical selection in some Windows programs. Specify a rectangular region by setting the mark at one corner and the point at the opposite corner. All commands for working with rectangular regions start with C-x r. Here are a few rectangular region commands."}, {"tag": "h2", "text": "Searching and replacing"}, {"tag": "h3", "text": "Searching for strings"}, {"tag": "p", "text": "Use C-s for forward incremental search, C-r for backward incremental search. Type another C-s or C-r to repeat the search. Type RET to exit search mode."}, {"tag": "h3", "text": "Regular expressions"}, {"tag": "p", "text": "C-M-s and C-M-r are the regular expression counterparts of C-s and C-r."}, {"tag": "p", "text": "Emacs regular expressions must escape the vertical bar | and parentheses. For example, the Perl regular expression (a|b) becomes \\(a\\|b\\) in Emacs."}, {"tag": "p", "text": "Emacs regular expressions do not support lookaround."}, {"tag": "p", "text": "The whitespace patterns \\s and \\S in Perl are written as \\s- and \\S- in Emacs. There is no equivalent of Perl's \\d except to use the range [0-9]."}, {"tag": "h3", "text": "Replacing"}, {"tag": "p", "text": "Use M-x replace-string and M-x replace-regex for replacing text. There are also interactive counterparts M-x query-replace and M-x query-replace-regex."}, {"tag": "h2", "text": "Saving text and positions"}, {"tag": "h3", "text": "Saving text"}, {"tag": "p", "text": "You can save a region of text to a named register for later pasting. Register names can be any single character. The command to save to a register a is C-x r s a. The command to insert the contents of register a is C-x r i a ."}, {"tag": "h3", "text": "Saving positions"}, {"tag": "p", "text": "Bookmarks are named positions in a buffer. The command to create a bookmark is C-x r m bookmark_name. The command to go to a bookmark is C-x r b bookmark_name."}, {"tag": "h2", "text": "The Emacs help system"}, {"tag": "p", "text": "All help commands start with C-h. If you don't know a more specific location to go to, you can start by typing C-h C-h to get to the top of a navigation system for help."}, {"tag": "p", "text": "C-h m is very useful. It displays all active modes and describes key bindings."}, {"tag": "p", "text": "C-h k tells what command is bound to a key and gives documentation on how it is used. C-h w is a sort of opposite: given a command, it sells what keys are bound to that command."}, {"tag": "h2", "text": "Navigating files, buffers, and windows"}, {"tag": "h3", "text": "Files"}, {"tag": "p", "text": "The command to open a file is C-x C-f. The command for ‘save as” is C-x C-w."}, {"tag": "p", "text": "Emacs has a sort of File Explorer named Dired. You can open Dired with the command C-x d. You can move up and down in the Dired buffer by using p and n just as you can use C-p and C-n in any other buffer. You can still use the control key, but you do not have to."}, {"tag": "p", "text": "Here are a few of the most important Dired commands."}, {"tag": "p", "text": "Adding the following two lines to your .emacs file will create an Open Recent submenu under the File menu."}, {"tag": "pre", "text": "(require 'recentf)\n(recentf-mode 1)"}, {"tag": "h3", "text": "Buffers"}, {"tag": "p", "text": "The command C-x b takes you to your previous buffer."}, {"tag": "p", "text": "The command C-x C-b creates a new window with a list of open buffers. You can navigate this list much as you would the Dired buffer."}, {"tag": "p", "text": "You can type the letter o to open the file on the current line in another window. You can type the number 1 to open the file as the only window."}, {"tag": "p", "text": "The command M-x kill-some-buffers lets you go through your open buffers and select which ones to kill."}, {"tag": "h3", "text": "Windows"}, {"tag": "p", "text": "The command C-x 1 closes all windows except the current one."}, {"tag": "p", "text": "C-x 2 splits the current window horizontally, one buffer on top of the other."}, {"tag": "p", "text": "C-x 3 splits the current window vertically, one beside the other."}, {"tag": "p", "text": "C-x o cycles through windows."}, {"tag": "h2", "text": "Miscellaneous commands"}, {"tag": "h2", "text": "Emacs resources"}, {"tag": "p", "text": "One program to rule them all Emacs cursor movement Emacs and Unicode Emacs kill (cut) commands Real Programmers (xkcd cartoon) 10 Specific Ways to Improve Your Productivity With Emacs from Steve Yegge"}], "content": "Getting started with Emacs on Windows\n\nThese notes summarize some of my discoveries (re-)learning GNU Emacs. Since these are my personal notes, it may help to briefly describe my background. I used Emacs on Unix from somewhere around 1990 until 1995. Then in 1995 I began using Windows as my primary operating system and stopped using Emacs. In 2010 I decided to give Emacs on Windows another try. I may not mention some basic things just because I remember them from my initial experience.\n\nThese notes are not a thorough introduction to Emacs. For a more systematic reference, the Emacs Wiki is a good place to start. I wanted to write these things down for future reference, and I put this file up on my website in case someone else finds it useful. If you have comments or corrections, please let me know.\n\nTable of contents\n\nInstallation and configuration\n\nInstalling Emacs and setting up .emacs\n\nBackup files\n\nRecycle bin\n\nIntegration with the Windows File Explorer\n\nGetting rid of the start-up screen and toolbar\n\nChanging fonts\n\nEnabling commands to change case\n\nSpell check\n\nInstalling color-theme\n\nInstalling nXhtml\n\nInstalling powershell-mode\n\nRemapping my keyboard\n\nLine wrapping\n\nColumn position\n\nEmacs vocabulary\n\nEditing LaTeX\n\nEditing source code\n\nSelecting and delting text\n\nSearching and replacing\n\nSearching for strings\n\nRegular expressions\n\nReplacing\n\nSaving text and positions\n\nSaving text\n\nSaving positions\n\nThe Emacs help system\n\nNavigating files, buffers, and windows\n\nFiles\n\nBuffers\n\nWindows\n\nMiscellaneous commands\n\nEmacs resources\n\nInstallation and configuration\n\nInstalling Emacs and setting up .emacs\n\nI install all my Unix-like software under C:\\bin and have that directory in my Windows PATH environment variable. I installed Emacs 23.1 in C:\\bin\\emacs-23.1 and created an environment variable HOME set to C:\\bin. The significance of HOME is that Emacs can find your configuration file if you put it there. This file is called “dot emacs” because the traditional name for the file on Unix systems is .emacs. On Windows, it is more convenient to name the file _emacs. (You can also name the file _emacs.el. Giving the file the .el extension causes Emacs to open the file in Lisp mode. And if someday the file becomes huge, you can compile it to make startup faster.)\n\nBackup files\n\nEmacs automatically saves backup versions of file and by default leaves these backup files beside the files being edited. This can be annoying. Some people call these extra files “Emacs droppings.” Adding the following lines to .emacs instructs Emacs to put all backup files in a temporary folder.\n\n(setq backup-directory-alist\n`((\".*\" . ,temporary-file-directory)))\n(setq auto-save-file-name-transforms\n`((\".*\" ,temporary-file-directory t)))\n\nRecycle Bin\n\nThe following line configures Emacs so that files deleted via Emacs are moved to the Recycle.\n\n(setq delete-by-moving-to-trash t)\n\nIntegration with the Windows File Explorer\n\nThe following registry script creates an “Open with Emacs” option in the Windows file explorer context menu.\n\nWindows Registry Editor Version 5.00\n\n[HKEY_CLASSES_ROOT\\*\\Shell\\Open In Emacs\\Command]\n@=\"\\\"C:\\\\bin\\\\Emacs-23.1\\\\bin\\\\emacsclientw.exe\\\" -a \\\"C:\\\\bin\\\\Emacs-23.1\\\\bin\\\\runemacs.exe\\\" \\\"%1\\\"\"\n\nPutting these two lines in .emacs creates a menu item File -> Open recent.\n\n(require 'recentf)\n(recentf-mode 1)\n\nIf you have a desktop shortcut to runemacs, you can open a file in Emacs by dropping it on the shortcut icon.\n\nGetting rid of the start-up screen and toolbar\n\nI turned off initial start-up screen by adding (setq inhibit-startup-screen t) to .emacs. This had the pleasant side effect of making the “Open with Emacs” context menu work as expected. (Before, Emacs would open with a split window. Now it opens with just the “sent” file.)\n\nGot rid of the toolbar by using the configuration editor under Options / Customize Emacs.\n\nChanging fonts\n\nI used the Options menu to change the default font to Consolas.\n\nEnabling commands to change case\n\nThe commands for converting the text in a region to upper or lower case are disabled by default. (The GNU Emacs manual says beginners find these commands confusing and so you have turn them on. That seems very strange. Many other Emacs commands are more confusing.) The following turns the commands on.\n\n(put 'upcase-region 'disabled nil)\n(put 'downcase-region 'disabled nil)\n\nOnce this is enabled, you can make the text in a region lowercase with C-x C-l or uppercase with C-x C-u.\n\nSpell check\n\nGNU Emacs does not provide a spell checker. Instead, it provides hooks to install your own spell checker, usually Aspell. I downloaded Aspell version 0.50.3 (win32) from here. I then installed the English dictionary from the same page. The dictionary installer warned me that Aspell was already installed and suggested that I uninstall it. I did, thinking that it might install a newer version. That didn't work. I re-installed Aspell, then installed the dictionary, ignoring the warning. Everything worked fine.\n\nAfter installing Aspell, I let Emacs know where to find it by adding these lines to my .emacs file.\n\n(setq-default ispell-program-name \"C:/bin/Aspell/bin/aspell.exe\")\n(setq text-mode-hook '(lambda() (flyspell-mode t) ))\n\nThe command M-x ispell will run the spell checker on your file. If flyspell-mode is turned on, as it is in the lines above, misspelled words are underlined in red as you type.\n\nInstalling color-theme\n\nIt was difficult to find a more direct way to configure the color schemes that Emacs uses, so I installed color-theme version 6.60. I then used a color theme creator to create a basic theme then tweaked the colors.\n\nInstalling nXhtml\n\nThe default support for editing HTML files was less than I expected. I heard good things about nXhtml and decided to go with it. Notice that it inserts extra menus when you open a file in nXhtml mode. You can use the commands from the menu until you learn their keyboard shortcuts.\n\nnXhtml mode requires HTML to be valid XHTML. If your HTML is not valid, you can use HTML Tidy to bring it into standard compliance. HTML Tidy appears as a menu option under nXhtml, but it must be installed separately. Installing HTML Tidy is very simple: download two files, the executable and a DLL, and copy them to somewhere in your path. Once HTML Tidy is installed, it will continually check the validity of the XHTML. It will display its status in the mode line and will turn angle brackets red that are not in the correct place.\n\nIncidentally, the table of contents for this page was automatically generated using nXhtml. Just give every <h> tag an id. Then you can use commands from the nXhtml menu to insert the table of contents and its style sheet.\n\nNB: Apparently the nXhtml code does not allow a space on either side of the equal sign when specifying the id value.\n\nInstalling powershell-mode\n\nI installed a mode for editing PowerShell code by copying powershell-mode.el, downloaded from here, by copying the file to C:\\bin\\emacs-23.1\\site-lisp, which is in my Emacs load-path. I tried installing some code that would allow me to run PowerShell as a shell inside Emacs. That did not work on the first try and I did not pursue it further.\n\nRemapping my keyboard\n\nMany emacs users recommend remapping your keyboard so that the caps lock key becomes a control key. I don't like the idea of changing my keyboard just to accommodate one program, even a program I may use very often. However, I recently bought a laptop that came with a Fn key right where my muscle memory expects the left control key. I hardly ever use the caps lock key, so I made it a control key for the sake of emacs and for making it easier to use my laptop. I mapped the scroll lock key, a key I have not used in a decade or two, to caps lock in case I ever need a caps lock key. My initial intention was to keep the original left control key as an addition control key, but then I disabled it to force myself to get into the habit of using my new control key. I mapped the keyboard of every computer I use to be the same. This has been hard to get used to.\n\nI used the KeyTweak application to remap my keyboards.\n\nI don't know what I want to do for my “Meta” key. For now I'm using the Esc key. Some recommend using the original Control key after remapping the Caps Lock key. I have two problems with that: it will not work on my laptop, and I first have to break my habit of using the original Control key as a Control key. (Why not just remap the Fn key on my laptop? Unfortunately this key cannot be remapped like an ordinary key.) I may try to get in the habit of using the right Alt key as my Meta key.\n\nLine wrapping\n\nI set global-visual-line-mode as the default way to handle line wrap. I did this through the menu sequence Options / Customize Emacs / Specific Option. This causes text to flow as it does in most Windows programs.\n\nColumn position\n\nBy default, Emacs displays the current line number in the mode line but not the current column number. To display the column number, add the following to your .emacs file.\n\n(setq column-number-mode t)\n\nEmacs vocabulary\n\nEmacs uses a set of terminology that is not commonly used elsewhere. The following correspondences are not exact, but they are a good first approximation.\n\nThe “echo area” is the very bottom of an Emacs window. It echoes commands, displays the minibuffer, and provides a place to type extra arguments for commands.\n\nEditing LaTeX\n\nOne of the most useful key sequences for editing LaTeX files are C-c C-o to insert a \\begin and \\end pair. Emacs will prompt you for the keyword to put inside the \\begin{} statement. Another useful key sequence is C-c C-f to run latex on a file. (Emacs can detect whether a file is plain TeX or LaTeX. I use LaTeX exclusively.)\n\nThere is Emacs package AUCTex for editing (La)TeX files, but I have not tried it.\n\nI would like to have C-c C-f run pdflatex rather than latex, but I have not found out how to configure that.\n\nEditing source code\n\nHere are a few useful commands for editing source code files.\n\nI put these lines in my .emacs file to make the C++ mode behave more like what I am accustomed to.\n\n(add-hook 'c++-mode-hook\n  '(lambda ()\n     (c-set-style \"stroustrup\")\n     (setq indent-tabs-mode nil)))\n\nSelecting and delting text\n\nC-x h selects the entire current buffer.\n\nYou select a region by using C-SPACE at one end of the region and a selection command and moving the point (cursor) to the other end of the region. Then you can use C-w to cut or M-y to copy. The paste command is C-y. Emacs maintains a “kill ring”, something analogous to the Windows clipboard but containing more than just the latest cut or copy. For example, C-y M-y. lets you paste the next-to-last thing that was cut. Use M-y again to paste the cut before that, etc.\n\nYou can kill all but one whitespace character with M-SPACE. You can kill all but one blank line with C-x C-o.\n\nEmacs has commands for working with rectangular regions, analogous to vertical selection in some Windows programs. Specify a rectangular region by setting the mark at one corner and the point at the opposite corner. All commands for working with rectangular regions start with C-x r. Here are a few rectangular region commands.\n\nSearching and replacing\n\nSearching for strings\n\nUse C-s for forward incremental search, C-r for backward incremental search. Type another C-s or C-r to repeat the search. Type RET to exit search mode.\n\nRegular expressions\n\nC-M-s and C-M-r are the regular expression counterparts of C-s and C-r.\n\nEmacs regular expressions must escape the vertical bar | and parentheses. For example, the Perl regular expression (a|b) becomes \\(a\\|b\\) in Emacs.\n\nEmacs regular expressions do not support lookaround.\n\nThe whitespace patterns \\s and \\S in Perl are written as \\s- and \\S- in Emacs. There is no equivalent of Perl's \\d except to use the range [0-9].\n\nReplacing\n\nUse M-x replace-string and M-x replace-regex for replacing text. There are also interactive counterparts M-x query-replace and M-x query-replace-regex.\n\nSaving text and positions\n\nSaving text\n\nYou can save a region of text to a named register for later pasting. Register names can be any single character. The command to save to a register a is C-x r s a. The command to insert the contents of register a is C-x r i a .\n\nSaving positions\n\nBookmarks are named positions in a buffer. The command to create a bookmark is C-x r m bookmark_name. The command to go to a bookmark is C-x r b bookmark_name.\n\nThe Emacs help system\n\nAll help commands start with C-h. If you don't know a more specific location to go to, you can start by typing C-h C-h to get to the top of a navigation system for help.\n\nC-h m is very useful. It displays all active modes and describes key bindings.\n\nC-h k tells what command is bound to a key and gives documentation on how it is used. C-h w is a sort of opposite: given a command, it sells what keys are bound to that command.\n\nNavigating files, buffers, and windows\n\nFiles\n\nThe command to open a file is C-x C-f. The command for ‘save as” is C-x C-w.\n\nEmacs has a sort of File Explorer named Dired. You can open Dired with the command C-x d. You can move up and down in the Dired buffer by using p and n just as you can use C-p and C-n in any other buffer. You can still use the control key, but you do not have to.\n\nHere are a few of the most important Dired commands.\n\nAdding the following two lines to your .emacs file will create an Open Recent submenu under the File menu.\n\n(require 'recentf)\n(recentf-mode 1)\n\nBuffers\n\nThe command C-x b takes you to your previous buffer.\n\nThe command C-x C-b creates a new window with a list of open buffers. You can navigate this list much as you would the Dired buffer.\n\nYou can type the letter o to open the file on the current line in another window. You can type the number 1 to open the file as the only window.\n\nThe command M-x kill-some-buffers lets you go through your open buffers and select which ones to kill.\n\nWindows\n\nThe command C-x 1 closes all windows except the current one.\n\nC-x 2 splits the current window horizontally, one buffer on top of the other.\n\nC-x 3 splits the current window vertically, one beside the other.\n\nC-x o cycles through windows.\n\nMiscellaneous commands\n\nEmacs resources\n\nOne program to rule them all Emacs cursor movement Emacs and Unicode Emacs kill (cut) commands Real Programmers (xkcd cartoon) 10 Specific Ways to Improve Your Productivity With Emacs from Steve Yegge"}
{"slug": "faith", "canonical_url": "https://www.johndcook.com/blog/faith/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/faith.html", "title": "What I believe", "heading": "What I believe", "description": "What I believe, summarized in the Apostle's Creed", "summary": "I believe in God the Father Almighty, and in Jesus Christ His only Son our Lord, who was born of the Holy Spirit and the Virgin Mary; crucified under Pontius Pilate, and buried; the third day He rose from the dead; he ascended into heaven and sits at the right hand of the Father, from thence he shall come to ...", "word_count": 89, "blocks": [{"tag": "h1", "text": "What I believe"}, {"tag": "p", "text": "I believe in God the Father Almighty, and in Jesus Christ His only Son our Lord, who was born of the Holy Spirit and the Virgin Mary; crucified under Pontius Pilate, and buried; the third day He rose from the dead; he ascended into heaven and sits at the right hand of the Father, from thence he shall come to judge the living and the dead. And in the Holy Spirit; the holy Church; the forgiveness of sins; the resurrection of the body; the life everlasting."}], "content": "What I believe\n\nI believe in God the Father Almighty, and in Jesus Christ His only Son our Lord, who was born of the Holy Spirit and the Virgin Mary; crucified under Pontius Pilate, and buried; the third day He rose from the dead; he ascended into heaven and sits at the right hand of the Father, from thence he shall come to judge the living and the dead. And in the Holy Spirit; the holy Church; the forgiveness of sins; the resurrection of the body; the life everlasting."}
{"slug": "fsharp_longitude_latitude", "canonical_url": "https://www.johndcook.com/blog/fsharp_longitude_latitude/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/fsharp_longitude_latitude.html", "title": "F# code for computing the distance between two (longitude, latitude) pairs", "heading": "Computing the distance between two locations on Earth from coordinates", "description": "Stand-alone F# code for computing the distance between two locations on the globe based on longitude and latitude.", "summary": "The following code returns the distance between to locations based on each point's longitude and latitude. The distance returned is relative to Earth's radius. To get the distance in miles, multiply by 3960. To get the distance in kilometers, multiply by 6373.", "word_count": 543, "blocks": [{"tag": "h1", "text": "Computing the distance between two locations on Earth from coordinates"}, {"tag": "p", "text": "The following code returns the distance between to locations based on each point's longitude and latitude. The distance returned is relative to Earth's radius. To get the distance in miles, multiply by 3960. To get the distance in kilometers, multiply by 6373."}, {"tag": "p", "text": "Latitude is measured in degrees north of the equator; southern locations have negative latitude. Similarly, longitude is measured in degrees east of the Prime Meridian. A location 10° west of the Prime Meridian, for example, could be expressed as either 350° east or as -10° east."}, {"tag": "p", "text": "The following F# implementation comes from Kevin Hazzard. See also Python implementation."}, {"tag": "pre", "text": "[<Measure>] type radian\n[<Measure>] type degree\n[<Measure>] type km\n[<Measure>] type mi\ntype LatLong = { Lat : float<degree>; Long : float<degree> }\n \n// a unit-of-measure (UoM) generic F# function based on \n// John D. Cook's Python version - https://www.johndcook.com\n// a UoM generic suface distance \nlet SphereSurfaceDistance<[<Measure>] 't>\n        (radius : float<'t>) (loc1 : LatLong) (loc2 : LatLong) =\n    // Convert latitude and longitude to\n    // spherical coordinates in radians.\n    let degrees_to_radians (d : float<degree>) =\n        d * System.Math.PI / 180.0<degree/radian>\n \n    // phi = 90 - latitude (coded F# pipeline style)\n    let phi1 = (90.0<degree> - loc1.Lat) |> degrees_to_radians\n    let phi2 = (90.0<degree> - loc2.Lat) |> degrees_to_radians\n \n    // theta = longitude (coded F# function call style)\n    let theta1 = degrees_to_radians loc1.Long\n    let theta2 = degrees_to_radians loc2.Long\n \n    // Compute spherical distance from spherical coordinates.\n    \n    // For two locations in spherical coordinates      \n    // (1, theta, phi) and (1, theta, phi)     \n    // cosine( arc length ) =      \n    //    sin phi sin phi' cos(theta-theta') + cos phi cos phi'     \n    // distance = rho * arc length\n    \n    radius * acos(sin(float(phi1)) * sin(float(phi2)) *\n        cos(float(theta1 - theta2)) + \n        cos(float(phi1)) * cos(float(phi2)))\n \n// instantiate Earth-relative functions in km and miles\nlet SurfaceDistanceOnEarthKm = SphereSurfaceDistance 6371.0<km>\nlet SurfaceDistanceOnEarthMile = SphereSurfaceDistance 3959.0<mi>\n \nlet test1_SurfaceDistanceOnEarthMile =\n    let richmond = { Lat = 37.542979<degree>; Long = -77.469092<degree> }\n    let saopaulo = { Lat = -23.548943<degree>; Long = -46.638818<degree> }\n \n    // distance between Richmond, Virginia USA and São Paulo, Brazil\n    // Google Earth says this should be about 4677.562 miles\n\n    let tst_dist = SurfaceDistanceOnEarthMile richmond saopaulo\n    let std_dist = 4677.562<mi>\n    let delta = abs (tst_dist - std_dist) / std_dist\n    // assert delta from standard < 1/4 of a percent\n    delta < 0.0025\n \nlet test1_SurfaceDistanceOnEarthKm =\n    let richmond = { Lat = 37.542979<degree>; Long = -77.469092<degree> }\n    let saopaulo = { Lat = -23.548943<degree>; Long = -46.638818<degree> }\n \n    // distance between Richmond, Virginia USA and São Paulo, Brazil\n    // Google Earth says this should be about 7527.806 km\n    let tst_dist = SurfaceDistanceOnEarthKm richmond saopaulo\n    let std_dist = 7527.806<km>\n    let delta = abs (tst_dist - std_dist) / std_dist\n    // assert delta from standard < 1/4 of a percent\n    delta < 0.0025"}, {"tag": "p", "text": "The code above assumes the earth is perfectly spherical. For a discussion of how accurate this assumption is, see my blog post on the shape of the Earth."}, {"tag": "p", "text": "The algorithm used to calculate distances is described in detail here."}, {"tag": "p", "text": "A web page to calculate the distance between to cities based on longitude and latitude is available here."}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want with it, no strings attached."}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Computing the distance between two locations on Earth from coordinates\n\nThe following code returns the distance between to locations based on each point's longitude and latitude. The distance returned is relative to Earth's radius. To get the distance in miles, multiply by 3960. To get the distance in kilometers, multiply by 6373.\n\nLatitude is measured in degrees north of the equator; southern locations have negative latitude. Similarly, longitude is measured in degrees east of the Prime Meridian. A location 10° west of the Prime Meridian, for example, could be expressed as either 350° east or as -10° east.\n\nThe following F# implementation comes from Kevin Hazzard. See also Python implementation.\n\n[<Measure>] type radian\n[<Measure>] type degree\n[<Measure>] type km\n[<Measure>] type mi\ntype LatLong = { Lat : float<degree>; Long : float<degree> }\n \n// a unit-of-measure (UoM) generic F# function based on \n// John D. Cook's Python version - https://www.johndcook.com\n// a UoM generic suface distance \nlet SphereSurfaceDistance<[<Measure>] 't>\n        (radius : float<'t>) (loc1 : LatLong) (loc2 : LatLong) =\n    // Convert latitude and longitude to\n    // spherical coordinates in radians.\n    let degrees_to_radians (d : float<degree>) =\n        d * System.Math.PI / 180.0<degree/radian>\n \n    // phi = 90 - latitude (coded F# pipeline style)\n    let phi1 = (90.0<degree> - loc1.Lat) |> degrees_to_radians\n    let phi2 = (90.0<degree> - loc2.Lat) |> degrees_to_radians\n \n    // theta = longitude (coded F# function call style)\n    let theta1 = degrees_to_radians loc1.Long\n    let theta2 = degrees_to_radians loc2.Long\n \n    // Compute spherical distance from spherical coordinates.\n    \n    // For two locations in spherical coordinates      \n    // (1, theta, phi) and (1, theta, phi)     \n    // cosine( arc length ) =      \n    //    sin phi sin phi' cos(theta-theta') + cos phi cos phi'     \n    // distance = rho * arc length\n    \n    radius * acos(sin(float(phi1)) * sin(float(phi2)) *\n        cos(float(theta1 - theta2)) + \n        cos(float(phi1)) * cos(float(phi2)))\n \n// instantiate Earth-relative functions in km and miles\nlet SurfaceDistanceOnEarthKm = SphereSurfaceDistance 6371.0<km>\nlet SurfaceDistanceOnEarthMile = SphereSurfaceDistance 3959.0<mi>\n \nlet test1_SurfaceDistanceOnEarthMile =\n    let richmond = { Lat = 37.542979<degree>; Long = -77.469092<degree> }\n    let saopaulo = { Lat = -23.548943<degree>; Long = -46.638818<degree> }\n \n    // distance between Richmond, Virginia USA and São Paulo, Brazil\n    // Google Earth says this should be about 4677.562 miles\n\n    let tst_dist = SurfaceDistanceOnEarthMile richmond saopaulo\n    let std_dist = 4677.562<mi>\n    let delta = abs (tst_dist - std_dist) / std_dist\n    // assert delta from standard < 1/4 of a percent\n    delta < 0.0025\n \nlet test1_SurfaceDistanceOnEarthKm =\n    let richmond = { Lat = 37.542979<degree>; Long = -77.469092<degree> }\n    let saopaulo = { Lat = -23.548943<degree>; Long = -46.638818<degree> }\n \n    // distance between Richmond, Virginia USA and São Paulo, Brazil\n    // Google Earth says this should be about 7527.806 km\n    let tst_dist = SurfaceDistanceOnEarthKm richmond saopaulo\n    let std_dist = 7527.806<km>\n    let delta = abs (tst_dist - std_dist) / std_dist\n    // assert delta from standard < 1/4 of a percent\n    delta < 0.0025\n\nThe code above assumes the earth is perfectly spherical. For a discussion of how accurate this assumption is, see my blog post on the shape of the Earth.\n\nThe algorithm used to calculate distances is described in detail here.\n\nA web page to calculate the distance between to cities based on longitude and latitude is available here.\n\nThis code is in the public domain. Do whatever you want with it, no strings attached.\n\nStand-alone numerical code"}
{"slug": "gamma_identities", "canonical_url": "https://www.johndcook.com/blog/gamma_identities/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/gamma_identities.html", "title": "Identities for gamma and related functions", "heading": "Identities for gamma and related functions", "description": "A diagram organizing the main identities for the gamma function and related functions.", "summary": "There are many identities relating the values of the gamma function at one point to values at other points. These identities mostly derive from four basic identities. There are analogous identities for functions related to the gamma function — log gamma, digamma, trigamma, etc. — that follow the same pattern.", "word_count": 453, "blocks": [{"tag": "h1", "text": "Identities for gamma and related functions"}, {"tag": "p", "text": "There are many identities relating the values of the gamma function at one point to values at other points. These identities mostly derive from four basic identities. There are analogous identities for functions related to the gamma function — log gamma, digamma, trigamma, etc. — that follow the same pattern."}, {"tag": "p", "text": "The diagram below helps to visualize the identity landscape."}, {"tag": "li", "text": "The conjugation identities all have the same form. They say conjugation commutes with function application."}, {"tag": "li", "text": "The addition identities relate the values of a function at z and z+n where n is an integer."}, {"tag": "li", "text": "The reflection identities relate values of a function at z and 1-z."}, {"tag": "li", "text": "The multiplication identities relate the value of a function at integer multiples of z to a sum or product of function applications at fractional shifts of z."}, {"tag": "p", "text": "The following table gives links to the specifics of each kind of identity."}, {"tag": "p", "text": "The digamma function ψ(z) is the derivative of log Γ(z). The polygamma function ψ(n)(z) is the nth derivative of ψ(z) for integer n ≥ 1 and ψ(0)(z) = ψ(z)."}, {"tag": "p", "text": "References to A&S are formula numbers in Abramowitz and Stegun."}, {"tag": "h2", "text": "Gamma identities"}, {"tag": "h3", "text": "Conjugation"}, {"tag": "p", "text": "A&S 6.1.23"}, {"tag": "p", "text": "Table"}, {"tag": "h3", "text": "Addition"}, {"tag": "p", "text": "The simplest case of the addition identity corresponds to n = 1."}, {"tag": "p", "text": "A&S 6.1.15"}, {"tag": "p", "text": "Here is the general case for positive integer n."}, {"tag": "p", "text": "A&S 6.1.16"}, {"tag": "p", "text": "Table"}, {"tag": "h3", "text": "Reflection"}, {"tag": "p", "text": "A&S 6.1.17"}, {"tag": "p", "text": "Table"}, {"tag": "h3", "text": "Multiplication"}, {"tag": "p", "text": "The simplest case of the multiplication identity is the duplication formula."}, {"tag": "p", "text": "A&S 6.1.18"}, {"tag": "p", "text": "Here is the general case for positive integer n."}, {"tag": "p", "text": "A&S 6.1.20"}, {"tag": "p", "text": "Table"}, {"tag": "h2", "text": "Log gamma identities"}, {"tag": "h3", "text": "Conjugation"}, {"tag": "p", "text": "Take log of A&S 6.1.23"}, {"tag": "p", "text": "Table"}, {"tag": "h3", "text": "Addition"}, {"tag": "p", "text": "The simplest case of the addition identity corresponds to n = 1."}, {"tag": "p", "text": "Take log of A&S 6.1.15"}, {"tag": "p", "text": "Here is the general case for positive integer n."}, {"tag": "p", "text": "Take log of A&S 6.1.16"}, {"tag": "p", "text": "Table"}, {"tag": "h3", "text": "Reflection"}, {"tag": "p", "text": "Take log of A&S 6.1.17"}, {"tag": "p", "text": "Table"}, {"tag": "h3", "text": "Multiplication"}, {"tag": "p", "text": "The simplest case of the multiplication identity is the duplication formula."}, {"tag": "p", "text": "Take log of A&S 6.1.18"}, {"tag": "p", "text": "Here is the general case for positive integer n."}, {"tag": "p", "text": "Take log of A&S 6.1.20"}, {"tag": "p", "text": "Table"}, {"tag": "h2", "text": "Polygamma identities"}, {"tag": "h3", "text": "Conjugation"}, {"tag": "p", "text": "A&S 6.3.9"}, {"tag": "p", "text": "Table"}, {"tag": "h3", "text": "Addition"}, {"tag": "p", "text": "The simplest case of the addition identity corresponds to n = 1."}, {"tag": "p", "text": "A&S 6.4.6"}, {"tag": "p", "text": "Here is the general case for positive integer n."}, {"tag": "p", "text": "Iterate A&S 6.4.6"}, {"tag": "p", "text": "Table"}, {"tag": "h3", "text": "Reflection"}, {"tag": "p", "text": "A&S 6.4.7"}, {"tag": "p", "text": "Table"}, {"tag": "h3", "text": "Multiplication"}, {"tag": "p", "text": "The simplest case of the multiplication identity is the duplication formula. For the digamma function this is"}, {"tag": "p", "text": "A&S 6.4.8"}, {"tag": "p", "text": "For the polygamma function with n ≥ 1 the duplication formula is"}, {"tag": "p", "text": "For the digamma function, general multiplication formula for positive integer m is"}, {"tag": "p", "text": "For the polygamma function with n ≥ 1 the multiplication formula is"}, {"tag": "p", "text": "A&S 6.4.9"}, {"tag": "p", "text": "Table"}, {"tag": "p", "text": "Other diagrams:"}, {"tag": "p", "text": "See this page for more diagrams on this site including diagrams for probability and statistics, analysis, topology, and category theory."}], "content": "Identities for gamma and related functions\n\nThere are many identities relating the values of the gamma function at one point to values at other points. These identities mostly derive from four basic identities. There are analogous identities for functions related to the gamma function — log gamma, digamma, trigamma, etc. — that follow the same pattern.\n\nThe diagram below helps to visualize the identity landscape.\n\nThe conjugation identities all have the same form. They say conjugation commutes with function application.\n\nThe addition identities relate the values of a function at z and z+n where n is an integer.\n\nThe reflection identities relate values of a function at z and 1-z.\n\nThe multiplication identities relate the value of a function at integer multiples of z to a sum or product of function applications at fractional shifts of z.\n\nThe following table gives links to the specifics of each kind of identity.\n\nThe digamma function ψ(z) is the derivative of log Γ(z). The polygamma function ψ(n)(z) is the nth derivative of ψ(z) for integer n ≥ 1 and ψ(0)(z) = ψ(z).\n\nReferences to A&S are formula numbers in Abramowitz and Stegun.\n\nGamma identities\n\nConjugation\n\nA&S 6.1.23\n\nTable\n\nAddition\n\nThe simplest case of the addition identity corresponds to n = 1.\n\nA&S 6.1.15\n\nHere is the general case for positive integer n.\n\nA&S 6.1.16\n\nTable\n\nReflection\n\nA&S 6.1.17\n\nTable\n\nMultiplication\n\nThe simplest case of the multiplication identity is the duplication formula.\n\nA&S 6.1.18\n\nHere is the general case for positive integer n.\n\nA&S 6.1.20\n\nTable\n\nLog gamma identities\n\nConjugation\n\nTake log of A&S 6.1.23\n\nTable\n\nAddition\n\nThe simplest case of the addition identity corresponds to n = 1.\n\nTake log of A&S 6.1.15\n\nHere is the general case for positive integer n.\n\nTake log of A&S 6.1.16\n\nTable\n\nReflection\n\nTake log of A&S 6.1.17\n\nTable\n\nMultiplication\n\nThe simplest case of the multiplication identity is the duplication formula.\n\nTake log of A&S 6.1.18\n\nHere is the general case for positive integer n.\n\nTake log of A&S 6.1.20\n\nTable\n\nPolygamma identities\n\nConjugation\n\nA&S 6.3.9\n\nTable\n\nAddition\n\nThe simplest case of the addition identity corresponds to n = 1.\n\nA&S 6.4.6\n\nHere is the general case for positive integer n.\n\nIterate A&S 6.4.6\n\nTable\n\nReflection\n\nA&S 6.4.7\n\nTable\n\nMultiplication\n\nThe simplest case of the multiplication identity is the duplication formula. For the digamma function this is\n\nA&S 6.4.8\n\nFor the polygamma function with n ≥ 1 the duplication formula is\n\nFor the digamma function, general multiplication formula for positive integer m is\n\nFor the polygamma function with n ≥ 1 the multiplication formula is\n\nA&S 6.4.9\n\nTable\n\nOther diagrams:\n\nSee this page for more diagrams on this site including diagrams for probability and statistics, analysis, topology, and category theory."}
{"slug": "gamma_python", "canonical_url": "https://www.johndcook.com/blog/gamma_python/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/gamma_python.html", "title": "Gamma and related functions in SciPy", "heading": "Gamma and related functions in SciPy", "description": "An overview of gamma and related functions in Python’s SciPy library.", "summary": "This page explains the following SciPy functions for computing the gamma function and related functions.", "word_count": 515, "blocks": [{"tag": "h1", "text": "Gamma and related functions in SciPy"}, {"tag": "p", "text": "This page explains the following SciPy functions for computing the gamma function and related functions."}, {"tag": "li", "text": "gamma"}, {"tag": "li", "text": "gammaln"}, {"tag": "li", "text": "gammainc"}, {"tag": "li", "text": "gamaincinv"}, {"tag": "li", "text": "gammaincc"}, {"tag": "li", "text": "gammainccinv"}, {"tag": "li", "text": "beta"}, {"tag": "li", "text": "betaln"}, {"tag": "li", "text": "betainc"}, {"tag": "li", "text": "betaincinv"}, {"tag": "li", "text": "psi"}, {"tag": "li", "text": "polygamma"}, {"tag": "p", "text": "The SciPy function names correspond easily to mathematical functions. However, as noted below, there are a couple things to be aware of."}, {"tag": "li", "text": "The functions with named ending in ln do indeed take natural logarithms, but they take absolute values first."}, {"tag": "li", "text": "The functions containing inc compute regularized incomplete functions and not simply incomplete functions."}, {"tag": "p", "text": "These points will be made clear below."}, {"tag": "h2", "text": "Gamma and beta functions"}, {"tag": "p", "text": "The SciPy functions for gamma function Γ(z) and the beta function B(x, y) are unsurprisingly called gamma and beta. The function gamma(z) computes"}, {"tag": "h2", "text": "Logarithms"}, {"tag": "p", "text": "Often the gamma and beta functions are not as useful in practice as their logarithms. (See explanation here.) These are implemented in SciPy as gammaln and betaln. Note that gammaln actually returns the natural logarithm of the absolute value of the gamma function, i.e. gammaln(z) computes log| γ(z) |. Similarly betaln(x, y) computes log | B(x, y) |."}, {"tag": "h2", "text": "Psi and polygamma functions"}, {"tag": "p", "text": "The derivative of log( Γ(z) ) is denoted ψ(z) and is implemented in the psi function. The nth derivative of ψ(z) is implemented in psi(n, z)."}, {"tag": "h2", "text": "Incomplete and complementary functions"}, {"tag": "p", "text": "Both the gamma and the beta function have “incomplete” versions. However, SciPy’s incomplete gamma function gammainc corresponds to the regularized gamma function. Similarly, SciPy’s incomplete beta function betainc corresponds to the regularized beta function."}, {"tag": "p", "text": "The (lower) incomplete gamma function is defined by"}, {"tag": "p", "text": "These are called “incomplete” because they integrate over part of the region defining the gamma function. We will see below that the incomplete beta function follows the same pattern."}, {"tag": "p", "text": "The regularized versions of the upper and lower incomplete gamma functions are"}, {"tag": "p", "text": "respectively."}, {"tag": "p", "text": "The SciPy function gammainc(a, z) computes P(a, z) and the function gammaincc computes Q(a, z). Note the extra ‘c’ in gammaincc that stands for “complement.” It may not seem necessary to provide functions for both P(a, z) and its complement Q(a, z) since Q(a, z) = 1 - P(a, z). However, the complementary functions may be necessary for numerical precision. See the explanation here regarding the error function and its complement; the same reasoning applies to incomplete gamma functions."}, {"tag": "p", "text": "The incomplete beta function is defined by"}, {"tag": "p", "text": "and the regularized incomplete beta function is defined by"}, {"tag": "p", "text": "The SciPy function betainc(a, b, x) computes Bx(a, b) and the function betaincc computes Ix(a, b)."}, {"tag": "h2", "text": "Inverse functions"}, {"tag": "p", "text": "SciPy functions ending in inv compute the inverse of the corresponding function. gammaincinv(a, y) returns x such that gammainc(a, x) equals y. Similarly gammainccinv is the inverse of gammaincc. Finally, betaincinv(a, b, y) returns x such that betaincinv(a, b, x) equals y."}, {"tag": "p", "text": "There is an asymmetry in the SciPy implementations of the gamma and beta functions: there is no functions betaincc or betainccinv. These functions are unnecessary because of symmetries in the beta function. One could define betaincc(a, b, x) as betainc(b, a, 1-x) and betainccinv(a, b, x) as betaincinv(b, a, 1-x)."}, {"tag": "h2", "text": "See also"}, {"tag": "p", "text": "Bessel functions in SciPy Gamma function identities Special function relationship diagram"}], "content": "Gamma and related functions in SciPy\n\nThis page explains the following SciPy functions for computing the gamma function and related functions.\n\ngamma\n\ngammaln\n\ngammainc\n\ngamaincinv\n\ngammaincc\n\ngammainccinv\n\nbeta\n\nbetaln\n\nbetainc\n\nbetaincinv\n\npsi\n\npolygamma\n\nThe SciPy function names correspond easily to mathematical functions. However, as noted below, there are a couple things to be aware of.\n\nThe functions with named ending in ln do indeed take natural logarithms, but they take absolute values first.\n\nThe functions containing inc compute regularized incomplete functions and not simply incomplete functions.\n\nThese points will be made clear below.\n\nGamma and beta functions\n\nThe SciPy functions for gamma function Γ(z) and the beta function B(x, y) are unsurprisingly called gamma and beta. The function gamma(z) computes\n\nLogarithms\n\nOften the gamma and beta functions are not as useful in practice as their logarithms. (See explanation here.) These are implemented in SciPy as gammaln and betaln. Note that gammaln actually returns the natural logarithm of the absolute value of the gamma function, i.e. gammaln(z) computes log| γ(z) |. Similarly betaln(x, y) computes log | B(x, y) |.\n\nPsi and polygamma functions\n\nThe derivative of log( Γ(z) ) is denoted ψ(z) and is implemented in the psi function. The nth derivative of ψ(z) is implemented in psi(n, z).\n\nIncomplete and complementary functions\n\nBoth the gamma and the beta function have “incomplete” versions. However, SciPy’s incomplete gamma function gammainc corresponds to the regularized gamma function. Similarly, SciPy’s incomplete beta function betainc corresponds to the regularized beta function.\n\nThe (lower) incomplete gamma function is defined by\n\nThese are called “incomplete” because they integrate over part of the region defining the gamma function. We will see below that the incomplete beta function follows the same pattern.\n\nThe regularized versions of the upper and lower incomplete gamma functions are\n\nrespectively.\n\nThe SciPy function gammainc(a, z) computes P(a, z) and the function gammaincc computes Q(a, z). Note the extra ‘c’ in gammaincc that stands for “complement.” It may not seem necessary to provide functions for both P(a, z) and its complement Q(a, z) since Q(a, z) = 1 - P(a, z). However, the complementary functions may be necessary for numerical precision. See the explanation here regarding the error function and its complement; the same reasoning applies to incomplete gamma functions.\n\nThe incomplete beta function is defined by\n\nand the regularized incomplete beta function is defined by\n\nThe SciPy function betainc(a, b, x) computes Bx(a, b) and the function betaincc computes Ix(a, b).\n\nInverse functions\n\nSciPy functions ending in inv compute the inverse of the corresponding function. gammaincinv(a, y) returns x such that gammainc(a, x) equals y. Similarly gammainccinv is the inverse of gammaincc. Finally, betaincinv(a, b, y) returns x such that betaincinv(a, b, x) equals y.\n\nThere is an asymmetry in the SciPy implementations of the gamma and beta functions: there is no functions betaincc or betainccinv. These functions are unnecessary because of symmetries in the beta function. One could define betaincc(a, b, x) as betainc(b, a, 1-x) and betainccinv(a, b, x) as betaincinv(b, a, 1-x).\n\nSee also\n\nBessel functions in SciPy Gamma function identities Special function relationship diagram"}
{"slug": "greek_letters", "canonical_url": "https://www.johndcook.com/blog/greek_letters/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/greek_letters.html", "title": "Greek letters in HTML, XML, TeX, and Unicode", "heading": "Greek letters in HTML, XML, TeX, and Unicode", "description": "Representing Greek letters in HTML, XML, TeX, and Unicode.", "summary": "Greek letters are simple to represent in HTML and TeX. In both systems, the code for a letter is essentially its name. The letter name is capitalized to get the capitalized form. HTML surrounds the letter name with an ampersand and semicolon. TeX prefixes the name with a backslash. For example, in HTML, λ is &lambda; and Λ is &Lambda;. ...", "word_count": 379, "blocks": [{"tag": "h1", "text": "Greek letters in HTML, XML, TeX, and Unicode"}, {"tag": "p", "text": "Greek letters are simple to represent in HTML and TeX. In both systems, the code for a letter is essentially its name. The letter name is capitalized to get the capitalized form. HTML surrounds the letter name with an ampersand and semicolon. TeX prefixes the name with a backslash. For example, in HTML, λ is &lambda; and Λ is &Lambda;. In TeX these are \\lambda and \\Lambda."}, {"tag": "p", "text": "HTML and Unicode distinguish between Roman letters and Greek letters that look similar. For example, the English letter \"A\" has Unicode value x41, but the Greek letter Alpha has Unicode value x391. TeX, however, only provides symbols for Greek letters that do not look like their English counterparts. TeX also has alternate versions of some Greek letters [1]."}, {"tag": "p", "text": "See note below about representing Greek letters in XML [2]."}, {"tag": "h2", "text": "Capital Greek letters"}, {"tag": "p", "text": "Note that the Unicode numbers above are consecutive with one exception: there is no Unicode character x3A2 between Ρ and Σ. The reason is given below. [3]"}, {"tag": "h2", "text": "Lower-case Greek letters"}, {"tag": "h2", "text": "Notes"}, {"tag": "p", "text": "[1] TeX also has several variations on Greek letters that are commonly used in mathematics: \\varepsilon, \\vartheta, \\varpi, \\varrho, and \\varphi."}, {"tag": "p", "text": "[2] HTML entities are illegal in XML and but legal in XHTML. XML applications require a numeric encoding of Greek letters. (You can simply put Unicode characters directly into your HTML page provided you set your content-type correctly. However, character entities and hexadecimal encodings allow you to represent non-ASCII symbols using only ASCII characters.) You can create an XML numeric encoding from a Unicode character by adding &# on the left and ; on the right, such as &#x393; for Γ, Unicode value x393."}, {"tag": "p", "text": "[3] The Unicode value of a lower-case Greek letter is the value of its upper-case counterpart plus x20. However, there is one complication. Greek has two forms of the lower-case sigma: ς used only as the final letter of a word and the more familiar σ used everywhere else. That's why there is no upper-case character x3A2 corresponding to the lower-case x3C2. The letter ς is an exception to the rule that HTML entities and TeX command sequences simply use the name of the Greek letter."}, {"tag": "p", "text": "See also Common math symbols in HTML, XML, TeX, and Unicode."}], "content": "Greek letters in HTML, XML, TeX, and Unicode\n\nGreek letters are simple to represent in HTML and TeX. In both systems, the code for a letter is essentially its name. The letter name is capitalized to get the capitalized form. HTML surrounds the letter name with an ampersand and semicolon. TeX prefixes the name with a backslash. For example, in HTML, λ is &lambda; and Λ is &Lambda;. In TeX these are \\lambda and \\Lambda.\n\nHTML and Unicode distinguish between Roman letters and Greek letters that look similar. For example, the English letter \"A\" has Unicode value x41, but the Greek letter Alpha has Unicode value x391. TeX, however, only provides symbols for Greek letters that do not look like their English counterparts. TeX also has alternate versions of some Greek letters [1].\n\nSee note below about representing Greek letters in XML [2].\n\nCapital Greek letters\n\nNote that the Unicode numbers above are consecutive with one exception: there is no Unicode character x3A2 between Ρ and Σ. The reason is given below. [3]\n\nLower-case Greek letters\n\nNotes\n\n[1] TeX also has several variations on Greek letters that are commonly used in mathematics: \\varepsilon, \\vartheta, \\varpi, \\varrho, and \\varphi.\n\n[2] HTML entities are illegal in XML and but legal in XHTML. XML applications require a numeric encoding of Greek letters. (You can simply put Unicode characters directly into your HTML page provided you set your content-type correctly. However, character entities and hexadecimal encodings allow you to represent non-ASCII symbols using only ASCII characters.) You can create an XML numeric encoding from a Unicode character by adding &# on the left and ; on the right, such as &#x393; for Γ, Unicode value x393.\n\n[3] The Unicode value of a lower-case Greek letter is the value of its upper-case counterpart plus x20. However, there is one complication. Greek has two forms of the lower-case sigma: ς used only as the final letter of a word and the more familiar σ used everywhere else. That's why there is no upper-case character x3A2 corresponding to the lower-case x3C2. The letter ς is an exception to the rule that HTML entities and TeX command sequences simply use the name of the Greek letter.\n\nSee also Common math symbols in HTML, XML, TeX, and Unicode."}
{"slug": "height_details", "canonical_url": "https://www.johndcook.com/blog/height_details/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/height_details.html", "title": "Details for predicting adult heights", "heading": "Details for predicting adult heights", "description": "Details for how this site predicts adult heights from child heights.", "summary": "This page explains how the distances are calculated on the page predicting a child’s adult height.", "word_count": 103, "blocks": [{"tag": "h1", "text": "Details for predicting adult heights"}, {"tag": "p", "text": "This page explains how the distances are calculated on the page predicting a child’s adult height."}, {"tag": "p", "text": "The formulas used here are taken from the book Go Figure!: Using Math to Answer Everyday Imponderables"}, {"tag": "p", "text": "Height for girls is predicted by"}, {"tag": "p", "text": "where h is the current height and x is the current age."}, {"tag": "p", "text": "For boys the corresponding formula is"}, {"tag": "p", "text": "These formulas assume that girls reach their adult height at around 13.5 years and that boys reach their adult height around 16.5 years."}, {"tag": "p", "text": "Of course these are only predictions based on averages. Individuals may not follow the predictions of the formula."}], "content": "Details for predicting adult heights\n\nThis page explains how the distances are calculated on the page predicting a child’s adult height.\n\nThe formulas used here are taken from the book Go Figure!: Using Math to Answer Everyday Imponderables\n\nHeight for girls is predicted by\n\nwhere h is the current height and x is the current age.\n\nFor boys the corresponding formula is\n\nThese formulas assume that girls reach their adult height at around 13.5 years and that boys reach their adult height around 16.5 years.\n\nOf course these are only predictions based on averages. Individuals may not follow the predictions of the formula."}
{"slug": "services-2", "canonical_url": "https://www.johndcook.com/blog/services-2/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/julia_rng.html", "title": "Non-uniform random number generators for Julia", "heading": "Random number generators for Julia", "description": "Non-uniform random number generation code for Julia programming language", "summary": "This code is in the public domain. Do whatever you want to with it, no strings attached.", "word_count": 574, "blocks": [{"tag": "h1", "text": "Random number generators for Julia"}, {"tag": "pre", "text": "## return a uniform random sample from the interval (a, b)\nfunction rand_uniform(a, b)\n    a + rand()*(b - a)\nend\n\n## return a random sample from a normal (Gaussian) distribution\nfunction rand_normal(mean, stdev)\n    if stdev <= 0.0\n        error(\"standard deviation must be positive\")\n    end\n    u1 = rand()\n    u2 = rand()\n    r = sqrt( -2.0*log(u1) )\n    theta = 2.0*pi*u2\n    mean + stdev*r*sin(theta)\nend\n\n## return a random sample from an exponential distribution\nfunction rand_exponential(mean)\n    if mean <= 0.0\n        error(\"mean must be positive\")\n    end\n    -mean*log(rand())\nend\n\n## return a random sample from a gamma distribution\nfunction rand_gamma(shape, scale)\n    if shape <= 0.0\n        error(\"Shape parameter must be positive\")\n    end\n    if scale <= 0.0\n        error(\"Scale parameter must be positive\")\n    end\n    \n    ## Implementation based on \"A Simple Method for Generating Gamma Variables\"\n    ## by George Marsaglia and Wai Wan Tsang.  \n    ## ACM Transactions on Mathematical Software\n    ## Vol 26, No 3, September 2000, pages 363-372.\n\n    if shape >= 1.0\n        d = shape - 1.0/3.0\n        c = 1.0/sqrt(9.0*d)\n        while true\n            x = rand_normal(0, 1)\n            v = 1.0 + c*x\n            while v <= 0.0\n                x = rand_normal(0, 1)\n                v = 1.0 + c*x\n            end\n            v = v*v*v\n            u = rand()\n            xsq = x*x\n            if u < 1.0 -.0331*xsq*xsq || log(u) < 0.5*xsq + d*(1.0 - v + log(v))\n                return scale*d*v\n            end\n        end\n    else\n        g = rand_gamma(shape+1.0, 1.0)\n        w = rand()\n        return scale*g*pow(w, 1.0/shape)\n    end\nend\n\n## return a random sample from a chi square distribution\n## with the specified degrees of freedom\nfunction rand_chi_square(dof)\n    rand_gamma(0.5*dof, 2.0)\nend\n\n## return a random sample from an inverse gamma random variable\nfunction rand_inverse_gamma(shape, scale)\n    ## If X is gamma(shape, scale) then\n    ## 1/Y is inverse gamma(shape, 1/scale)\n    1.0 / rand_gamma(shape, 1.0 / scale)\nend\n\n## return a sample from a Weibull distribution\nfunction rand_weibull(shape, scale)\n    if shape <= 0.0\n        error(\"Shape parameter must be positive\")\n    end\n    if scale <= 0.0\n        error(\"Scale parameter must be positive\")\n    end\n    scale * pow(-log(rand()), 1.0 / shape)\nend\n\n## return a random sample from a Cauchy distribution\nfunction rand_cauchy(median, scale)\n    if scale <= 0.0\n        error(\"Scale parameter must be positive\")\n    end\n    p = rand()\n    median + scale*tan(pi*(p - 0.5))\nend\n\n## return a random sample from a Student t distribution\nfunction rand_student_t(dof)\n    if dof <= 0\n        error(\"Degrees of freedom must be positive\")\n    end\n\n    ## See Seminumerical Algorithms by Knuth\n    y1 = rand_normal(0, 1)\n    y2 = rand_chi_square(dof)\n    y1 / sqrt(y2 / dof)\nend\n \n## return a random sample from a Laplace distribution\n## The Laplace distribution is also known as the double exponential distribution.\nfunction rand_laplace(mean, scale)   \n    if scale <= 0.0\n        error(\"Scale parameter must be positive\")\n    end\n    u = rand()\n    if u < 0.5\n        retval = mean + scale*log(2.0*u) \n    else\n        retval = mean - scale*log(2*(1-u))\n    end\n    retval\nend\n\n## return a random sample from a log-normal distribution\nfunction rand_log_normal(mu, sigma)\n    return exp(rand_normal(mu, sigma))\nend\n\n## return a random sample from a beta distribution\nfunction rand_beta(a, b)\n    if a <= 0 || b <= 0\n        error(\"Beta parameters must be positive\")\n    end\n    \n    ## There are more efficient methods for generating beta samples.\n    ## However such methods are a little more efficient and much more complicated.\n    ## For an explanation of why the following method works, see\n    ## https://www.johndcook.com/distribution_chart.html#gamma_beta\n\n    u = rand_gamma(a, 1.0)\n    v = rand_gamma(b, 1.0)\n    u / (u + v)\nend"}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want to with it, no strings attached."}, {"tag": "p", "text": "See also Stand-alone numerical code"}, {"tag": "p", "text": "Need help making sense of data?"}], "content": "Random number generators for Julia\n\n## return a uniform random sample from the interval (a, b)\nfunction rand_uniform(a, b)\n    a + rand()*(b - a)\nend\n\n## return a random sample from a normal (Gaussian) distribution\nfunction rand_normal(mean, stdev)\n    if stdev <= 0.0\n        error(\"standard deviation must be positive\")\n    end\n    u1 = rand()\n    u2 = rand()\n    r = sqrt( -2.0*log(u1) )\n    theta = 2.0*pi*u2\n    mean + stdev*r*sin(theta)\nend\n\n## return a random sample from an exponential distribution\nfunction rand_exponential(mean)\n    if mean <= 0.0\n        error(\"mean must be positive\")\n    end\n    -mean*log(rand())\nend\n\n## return a random sample from a gamma distribution\nfunction rand_gamma(shape, scale)\n    if shape <= 0.0\n        error(\"Shape parameter must be positive\")\n    end\n    if scale <= 0.0\n        error(\"Scale parameter must be positive\")\n    end\n    \n    ## Implementation based on \"A Simple Method for Generating Gamma Variables\"\n    ## by George Marsaglia and Wai Wan Tsang.  \n    ## ACM Transactions on Mathematical Software\n    ## Vol 26, No 3, September 2000, pages 363-372.\n\n    if shape >= 1.0\n        d = shape - 1.0/3.0\n        c = 1.0/sqrt(9.0*d)\n        while true\n            x = rand_normal(0, 1)\n            v = 1.0 + c*x\n            while v <= 0.0\n                x = rand_normal(0, 1)\n                v = 1.0 + c*x\n            end\n            v = v*v*v\n            u = rand()\n            xsq = x*x\n            if u < 1.0 -.0331*xsq*xsq || log(u) < 0.5*xsq + d*(1.0 - v + log(v))\n                return scale*d*v\n            end\n        end\n    else\n        g = rand_gamma(shape+1.0, 1.0)\n        w = rand()\n        return scale*g*pow(w, 1.0/shape)\n    end\nend\n\n## return a random sample from a chi square distribution\n## with the specified degrees of freedom\nfunction rand_chi_square(dof)\n    rand_gamma(0.5*dof, 2.0)\nend\n\n## return a random sample from an inverse gamma random variable\nfunction rand_inverse_gamma(shape, scale)\n    ## If X is gamma(shape, scale) then\n    ## 1/Y is inverse gamma(shape, 1/scale)\n    1.0 / rand_gamma(shape, 1.0 / scale)\nend\n\n## return a sample from a Weibull distribution\nfunction rand_weibull(shape, scale)\n    if shape <= 0.0\n        error(\"Shape parameter must be positive\")\n    end\n    if scale <= 0.0\n        error(\"Scale parameter must be positive\")\n    end\n    scale * pow(-log(rand()), 1.0 / shape)\nend\n\n## return a random sample from a Cauchy distribution\nfunction rand_cauchy(median, scale)\n    if scale <= 0.0\n        error(\"Scale parameter must be positive\")\n    end\n    p = rand()\n    median + scale*tan(pi*(p - 0.5))\nend\n\n## return a random sample from a Student t distribution\nfunction rand_student_t(dof)\n    if dof <= 0\n        error(\"Degrees of freedom must be positive\")\n    end\n\n    ## See Seminumerical Algorithms by Knuth\n    y1 = rand_normal(0, 1)\n    y2 = rand_chi_square(dof)\n    y1 / sqrt(y2 / dof)\nend\n \n## return a random sample from a Laplace distribution\n## The Laplace distribution is also known as the double exponential distribution.\nfunction rand_laplace(mean, scale)   \n    if scale <= 0.0\n        error(\"Scale parameter must be positive\")\n    end\n    u = rand()\n    if u < 0.5\n        retval = mean + scale*log(2.0*u) \n    else\n        retval = mean - scale*log(2*(1-u))\n    end\n    retval\nend\n\n## return a random sample from a log-normal distribution\nfunction rand_log_normal(mu, sigma)\n    return exp(rand_normal(mu, sigma))\nend\n\n## return a random sample from a beta distribution\nfunction rand_beta(a, b)\n    if a <= 0 || b <= 0\n        error(\"Beta parameters must be positive\")\n    end\n    \n    ## There are more efficient methods for generating beta samples.\n    ## However such methods are a little more efficient and much more complicated.\n    ## For an explanation of why the following method works, see\n    ## https://www.johndcook.com/distribution_chart.html#gamma_beta\n\n    u = rand_gamma(a, 1.0)\n    v = rand_gamma(b, 1.0)\n    u / (u + v)\nend\n\nThis code is in the public domain. Do whatever you want to with it, no strings attached.\n\nSee also Stand-alone numerical code\n\nNeed help making sense of data?"}
{"slug": "magic_knight/ ", "canonical_url": "https://www.johndcook.com/blog/magic_knight/ ", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/magic_knight.html", "title": "Knight’s tour magic square", "heading": "Python code to test knight’s tour magic square", "description": "Python script to verify Euler’ knight’s tour magic square has the advertised properties.", "summary": "Leonhard Euler created the following magic square.", "word_count": 319, "blocks": [{"tag": "h1", "text": "Python code to test knight’s tour magic square"}, {"tag": "p", "text": "Leonhard Euler created the following magic square."}, {"tag": "p", "text": "This magic square has three properties:"}, {"tag": "li", "text": "Each row and each column sums to 260."}, {"tag": "li", "text": "Each half-row and each half-column sums to 130."}, {"tag": "li", "text": "The sequence of squares containing 1, 2, 3, …, 64 form a knight’s tour."}, {"tag": "p", "text": "The following Python code verifies that the magic square has the advertised properties."}, {"tag": "pre", "text": "# Euler's knight's tour magic square\na = [[ 1, 48, 31, 50, 33, 16, 63, 18],\n     [30, 51, 46,  3, 62, 19, 14, 35],\n     [47,  2, 49, 32, 15, 34, 17, 64],\n     [52, 29,  4, 45, 20, 61, 36, 13],\n     [ 5, 44, 25, 56,  9, 40, 21, 60],\n     [28, 53,  8, 41, 24, 57, 12, 37],\n     [43,  6, 55, 26, 39, 10, 59, 22],\n     [54, 27, 42,  7, 58, 23, 38, 11]]\n\n# Divide the chess board into four 4x4 blocks.\n# Verify that in each block the rows and colums sum to 130.\n# It follows that each entire row and entire column sums to 260.\nfor hblock in [0, 1]:\n    for vblock in [0, 1]: \n        for i in range(0,4):\n            row_sum = col_sum = 0\n            for j in range(0,4):\n                row_sum += a[hblock*4 + i][vblock*4 + j]\n                col_sum += a[hblock*4 + j][vblock*4 + i]\n            assert(row_sum == 130)\n            assert(col_sum == 130)\n\n# Report whether it is legal for a knight to move from a to b.\ndef knight_move(a, b):\n    return ((abs(a[0] - b[0]) == 2 and abs(a[1] - b[1]) == 1) or\n            (abs(a[0] - b[0]) == 1 and abs(a[1] - b[1]) == 2))\n\n# Map each square to its coordinates.\ncoordinate = {}\nfor row in range(0, 8):\n    for col in range(0, 8):\n        coordinate[ a[row][col] ] = (row, col)\n\n# Verify that the sequence of numbers are legal knight moves.\nfor i in range(1, 64):\n    assert knight_move( coordinate[i], coordinate[i+1] )\n\n# If the script gets this far, no assertion failed.\nprint \"All tests pass.\""}], "content": "Python code to test knight’s tour magic square\n\nLeonhard Euler created the following magic square.\n\nThis magic square has three properties:\n\nEach row and each column sums to 260.\n\nEach half-row and each half-column sums to 130.\n\nThe sequence of squares containing 1, 2, 3, …, 64 form a knight’s tour.\n\nThe following Python code verifies that the magic square has the advertised properties.\n\n# Euler's knight's tour magic square\na = [[ 1, 48, 31, 50, 33, 16, 63, 18],\n     [30, 51, 46,  3, 62, 19, 14, 35],\n     [47,  2, 49, 32, 15, 34, 17, 64],\n     [52, 29,  4, 45, 20, 61, 36, 13],\n     [ 5, 44, 25, 56,  9, 40, 21, 60],\n     [28, 53,  8, 41, 24, 57, 12, 37],\n     [43,  6, 55, 26, 39, 10, 59, 22],\n     [54, 27, 42,  7, 58, 23, 38, 11]]\n\n# Divide the chess board into four 4x4 blocks.\n# Verify that in each block the rows and colums sum to 130.\n# It follows that each entire row and entire column sums to 260.\nfor hblock in [0, 1]:\n    for vblock in [0, 1]: \n        for i in range(0,4):\n            row_sum = col_sum = 0\n            for j in range(0,4):\n                row_sum += a[hblock*4 + i][vblock*4 + j]\n                col_sum += a[hblock*4 + j][vblock*4 + i]\n            assert(row_sum == 130)\n            assert(col_sum == 130)\n\n# Report whether it is legal for a knight to move from a to b.\ndef knight_move(a, b):\n    return ((abs(a[0] - b[0]) == 2 and abs(a[1] - b[1]) == 1) or\n            (abs(a[0] - b[0]) == 1 and abs(a[1] - b[1]) == 2))\n\n# Map each square to its coordinates.\ncoordinate = {}\nfor row in range(0, 8):\n    for col in range(0, 8):\n        coordinate[ a[row][col] ] = (row, col)\n\n# Verify that the sequence of numbers are legal knight moves.\nfor i in range(1, 64):\n    assert knight_move( coordinate[i], coordinate[i+1] )\n\n# If the script gets this far, no assertion failed.\nprint \"All tests pass.\""}
{"slug": "math_diagrams", "canonical_url": "https://www.johndcook.com/blog/math_diagrams/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/math_diagrams.html", "title": "Math diagrams", "heading": "Math diagrams", "description": "List of links to math diagrams: relationships between probability distributions, topological spaces, modes of convergence, etc.", "summary": "These are diagrams I've created to visualize mathematical definitions and theorems.", "word_count": 39, "blocks": [{"tag": "h1", "text": "Math diagrams"}, {"tag": "p", "text": "These are diagrams I've created to visualize mathematical definitions and theorems."}, {"tag": "li", "text": "Probability distributions"}, {"tag": "li", "text": "Modes of convergence"}, {"tag": "li", "text": "Special functions"}, {"tag": "li", "text": "Gamma function identities"}, {"tag": "li", "text": "Bessel functions"}, {"tag": "li", "text": "Conjugate priors"}, {"tag": "li", "text": "Topological vector spaces"}, {"tag": "li", "text": "Category theory concepts"}, {"tag": "li", "text": "General topology"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Math diagrams\n\nThese are diagrams I've created to visualize mathematical definitions and theorems.\n\nProbability distributions\n\nModes of convergence\n\nSpecial functions\n\nGamma function identities\n\nBessel functions\n\nConjugate priors\n\nTopological vector spaces\n\nCategory theory concepts\n\nGeneral topology\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "math_h", "canonical_url": "https://www.johndcook.com/blog/math_h/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/math_h.html", "title": "Math.h in POSIX, ISO, and Visual Studio", "heading": "Math.h in POSIX, ISO, and Visual Studio", "description": "Comparing math.h functions in the ISO and POSIX standards with Microsoft's implementation in Visual Studio.", "summary": "This page compares the list of functions in the C math.h header across four versions: ISO (ANSI) pre-1999, ISO post-1999, POSIX, and Microsoft Visual Studio.", "word_count": 250, "blocks": [{"tag": "h1", "text": "Math.h in POSIX, ISO, and Visual Studio"}, {"tag": "p", "text": "This page compares the list of functions in the C math.h header across four versions: ISO (ANSI) pre-1999, ISO post-1999, POSIX, and Microsoft Visual Studio."}, {"tag": "p", "text": "The smallest list is the ISO standard prior to 1999. All other lists completely contain the original ISO standard."}, {"tag": "p", "text": "The latest POSIX and ISO standards are essentially identical. The only difference is that the POSIX standard requires Bessel functions of the first kind j0, j1, and jn and Bessel functions of the second kind y0, y1, and yn."}, {"tag": "p", "text": "Microsoft supports the older ISO standard and supports the Bessel functions mentioned above. Microsoft supports some of the functions added to the ISO standard in 1999. The Microsoft names for these new functions are the ISO names with an underscore prefix. The only minor exception is that the Microsoft function corresponding to the ISO function fpclassify is _fpclass rather than _fpclassify."}, {"tag": "p", "text": "Details are given in the table below."}, {"tag": "p", "text": "[1] The inverse hyperbolic functions can be reduced to more familiar functions."}, {"tag": "li", "text": "asinh(x) = log(x + sqrt(x2 + 1))"}, {"tag": "li", "text": "acosh(x) = log(x + sqrt(x2 - 1))"}, {"tag": "li", "text": "atanh(x) = (log(1+x) - log(1-x))/2"}, {"tag": "p", "text": "[2] Here is stand-alone code for these functions: erf, expm1, log1p, lgamma, tgamma."}, {"tag": "p", "text": "[3] You can test whether a number is a NaN by testing whether it equals itself. The expression (x == x) returns true if and only if x is not a NaN. (See floating point exceptions.)"}, {"tag": "h2", "text": "Other C++ resources"}, {"tag": "li", "text": "Floating point exceptions"}, {"tag": "li", "text": "Random number generation"}, {"tag": "li", "text": "Regular expressions"}, {"tag": "li", "text": "Strings"}], "content": "Math.h in POSIX, ISO, and Visual Studio\n\nThis page compares the list of functions in the C math.h header across four versions: ISO (ANSI) pre-1999, ISO post-1999, POSIX, and Microsoft Visual Studio.\n\nThe smallest list is the ISO standard prior to 1999. All other lists completely contain the original ISO standard.\n\nThe latest POSIX and ISO standards are essentially identical. The only difference is that the POSIX standard requires Bessel functions of the first kind j0, j1, and jn and Bessel functions of the second kind y0, y1, and yn.\n\nMicrosoft supports the older ISO standard and supports the Bessel functions mentioned above. Microsoft supports some of the functions added to the ISO standard in 1999. The Microsoft names for these new functions are the ISO names with an underscore prefix. The only minor exception is that the Microsoft function corresponding to the ISO function fpclassify is _fpclass rather than _fpclassify.\n\nDetails are given in the table below.\n\n[1] The inverse hyperbolic functions can be reduced to more familiar functions.\n\nasinh(x) = log(x + sqrt(x2 + 1))\n\nacosh(x) = log(x + sqrt(x2 - 1))\n\natanh(x) = (log(1+x) - log(1-x))/2\n\n[2] Here is stand-alone code for these functions: erf, expm1, log1p, lgamma, tgamma.\n\n[3] You can test whether a number is a NaN by testing whether it equals itself. The expression (x == x) returns true if and only if x is not a NaN. (See floating point exceptions.)\n\nOther C++ resources\n\nFloating point exceptions\n\nRandom number generation\n\nRegular expressions\n\nStrings"}
{"slug": "math_h_to_python", "canonical_url": "https://www.johndcook.com/blog/math_h_to_python/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/math_h_to_python.html", "title": "Python counterparts to functions in C math.h", "heading": "Python counterparts for C math functions", "description": "Python and SciPy counterparts to math.h functions.", "summary": "Python has direct counterparts to most of the functions in C's standard math library.The Python functions have the same name as their C counterparts with three exceptions:", "word_count": 145, "blocks": [{"tag": "h1", "text": "Python counterparts for C math functions"}, {"tag": "p", "text": "Python has direct counterparts to most of the functions in C's standard math library.The Python functions have the same name as their C counterparts with three exceptions:"}, {"tag": "li", "text": "fabs in C corrresponds to abs in Python"}, {"tag": "li", "text": "tgamma in C correspons to gamma in Python"}, {"tag": "li", "text": "lgamma in C corresponds to gammaln in Python"}, {"tag": "p", "text": "The function abs is at the top level of Python. Aside from this exception, all mathematical functions are found in math, scipy, or scipy.speci."}, {"tag": "p", "text": "All functions in math have counterparts in scipy with the same name. However, the scipy version may behave differently. For example, math.sqrt(-1) returns a domain error but scipy.sqrt(-1) returns 1j."}, {"tag": "p", "text": "Python (specifically SciPy) has far more functions than C. The following chart lists Python counterparts for C functions in math.h."}, {"tag": "h2", "text": "Related resources"}, {"tag": "li", "text": "Getting started with SciPy [link died]"}, {"tag": "li", "text": "Comparing math.h on Windows and POSIX"}], "content": "Python counterparts for C math functions\n\nPython has direct counterparts to most of the functions in C's standard math library.The Python functions have the same name as their C counterparts with three exceptions:\n\nfabs in C corrresponds to abs in Python\n\ntgamma in C correspons to gamma in Python\n\nlgamma in C corresponds to gammaln in Python\n\nThe function abs is at the top level of Python. Aside from this exception, all mathematical functions are found in math, scipy, or scipy.speci.\n\nAll functions in math have counterparts in scipy with the same name. However, the scipy version may behave differently. For example, math.sqrt(-1) returns a domain error but scipy.sqrt(-1) returns 1j.\n\nPython (specifically SciPy) has far more functions than C. The following chart lists Python counterparts for C functions in math.h.\n\nRelated resources\n\nGetting started with SciPy [link died]\n\nComparing math.h on Windows and POSIX"}
{"slug": "services-2", "canonical_url": "https://www.johndcook.com/blog/services-2/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/math_stat.html", "title": "Math and statistics articles by category", "heading": "Math and statistics articles by category", "description": "Informal math and statistics articles by John D. Cook categorized by topic. Preventing errors, interpreting probabilities, clinical trials, practicalities.", "summary": "This page categorizes my informal math and statistics articles and blog posts. See also journal articles and technical reports.", "word_count": 936, "blocks": [{"tag": "h1", "text": "Math and statistics articles by category"}, {"tag": "p", "text": "This page categorizes my informal math and statistics articles and blog posts. See also journal articles and technical reports."}, {"tag": "h2", "text": "Categories"}, {"tag": "p", "text": "Elementary Preventing and detecting errors Interpreting and misinterpreting probabilities Modeling Clinical trials Mathematical statistics Random number generation Practicalities Pure math"}, {"tag": "h2", "text": "Elementary"}, {"tag": "p", "text": "Tukey tallying Honeybee genealogy Coping with exponential growth Selection bias and bombers Canonical example of Bayes' theorem in detail Linear interpolation Quadratic interpolation Sales tax included Six degrees of Paul Erdős Connecting Fibonacci and geometric sequences Divisibility rules in hexadecimal Golden ratio and special angles Silver ratio Three rules of thumb Approximations with Pythagorean triangles Best rational approximation There isn’t a googol of anything"}, {"tag": "h2", "text": "Preventing and detecting errors"}, {"tag": "p", "text": "Reproducible research In praise of tedious proofs Errors in math papers Proofs of false statements Probability that a study result is true Literate programming and statistics Irreproducible analysis Programming the last mile Complementary validation Troubleshooting Sweave Preventing an unpleasant Sweave surprise"}, {"tag": "h2", "text": "Interpreting and misinterpreting probabilities"}, {"tag": "p", "text": "Plausible reasoning How loud is the evidence? What is a confidence interval? Probability that a number is prime Learning is not the same as gaining information Laws of large numbers and small numbers The law of small numbers Probability and information Example of the law of small numbers What a probability means Irrelevant uncertainty False positives for medical papers False positives for medical tests Most published research results are false Musicians, drunks, and Oliver Cromwell"}, {"tag": "h2", "text": "Modeling"}, {"tag": "p", "text": "C. S. Lewis on modeling Thick tails Metabolism and power laws Time spent in coffee shops Networks and power laws Why heights are normally distributed Why heights are not normally distributed How to linearize data for regression Approximate problems and approximate solutions Wine, Beer, and Statistics What happens when you add a new teller? (queuing theory) Adult heights and mixture distributions The probability that Shakespeare wrote a play Hard disk array failure probabilities Are men better than women at chess? Small advantages show up in the extremes Ignorance doesn't change reality Server utilization: Joel on queuing Cost-benefit analysis versus benefit-only analysis"}, {"tag": "h2", "text": "Clinical trials"}, {"tag": "p", "text": "Stopping trials of ineffective drugs earlier Three ways of tuning an adaptively randomized trial Galen and clinical trials Dose-finding: why start at the lowest dose? Innovation II (scurvy trial from 1601) Most published research results are false False positives for medical papers Population drift Randomized trials of parachute use Small effective sample size does not mean uninformative Early evidence-based medicine"}, {"tag": "h2", "text": "Mathematical statistics"}, {"tag": "p", "text": "Clickable chart of distribution relationships Upper and lower bounds for normal probabilities Student-t distribution as a mixture of normals Sums of uniform random variables Conjugate prior relationships Robust priors Determining parameters from quantiles Quantifying the error in the central limit theorem Four characterizations of the normal distribution Random inequalities I, II, III, IV, V, VI, VII, VIII Estimating Cauchy distribution location parameter Inverse gamma distribution Negative binomial distribution Relating two definitions of expectation Bias Four types of errors Stochastic independence Normal approximation details: beta binomial gamma Poisson Student-t Logistic Cosine Poisson approximation to binomial Wilson-Hilferty approximation for the Poisson CDF Camp-Paulson approximation for the binomial CDF Relative error in normal approximations Stable distributions Unbiased estimators can be terrible Bias, consistency, and efficiency Criticisms of significance testing Canonical examples from robust statistics Example of efficiency for mean vs. median Estimating reporting rates"}, {"tag": "h2", "text": "Random number generation"}, {"tag": "p", "text": "Pitfalls in Random Number Generation Random number generator controversy Random number generation in C++ TR1 Rolling dice for normal samples"}, {"tag": "h2", "text": "Practicalities"}, {"tag": "p", "text": "Approximating a solution that doesn't exist Making a singular matrix non-singular Ten surprises from numerical linear algebra Microsoft equation editor improved Soft maximum and how to compute it IEEE arithmetic exceptions in C++, Python Computing standard deviation Computing regression coefficients Fibonacci numbers at work (numerical integration) Five kinds of subscripts in R Computing the inverse of the normal CDF Approximate problems and approximate solutions How to calculate binomial probabilities How to calculate correlation accurately Overflow and loss of precision What to make “u” in integration by parts Integration and pragmatism Quick TeX to graphic utility LaTeX on Windows Everything begins with “p” Mercator projection Asymptotic notation R for Programmers coming from other languages Step size for numerical differential equations The error function and the normal distribution Working with distributions in Mathematica, R/S-PLUS, Excel, and Python Random number generation in C++ Innovation IV (Tukey on efficiency) Index of tail weight Delta method When the normal approximation to the Student t isn't good enough Double exponential integration Floating point numbers are a leaky abstraction Convex optimization Code for the error function and normal CDF Anatomy of a floating point number"}, {"tag": "h2", "text": "Pure math"}, {"tag": "p", "text": "Modes of convergence Breastfeeding, the golden ratio, and rational approximation Finite differences A knight’s random walk How to differentiate a non-differentiable function (intro to distribution theory) Navier-Stokes equations Splitting a convex set through its center Log concave functions The gamma function Binomial coefficient generalizations Multi-index notation Computing binomial coefficients Upper and lower bounds on binomial coefficients Selecting with replacement (Stanley's symbol) Stanley's twelvefold way (combinatorics notes) Jenga mathematics What is the cosine of a matrix? Gilbreath's conjecture Orthogonal polynomials Chebyshev polynomials Hypergeometric functions Functions of regular variation Determining whether a number is a square How many numbers are squares mod m Fast exponentiation Linear congruences Quadratic congruences Constructive proof of the Chinese remainder theorem Old math books The smoothest curve through a set of points Quasi-random sequences in art and integration Fractional derivatives Means and inequalities Polynomial interpolation error Sledgehammer technique for trig integrals Spherical trigonometry Why care about spherical trig? The pqr theorem A surprising theorem in complex variables Perfect numbers: even, odd Fractional integration Diagram of Bessel functions Gamma function identities Special function diagram"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Math and statistics articles by category\n\nThis page categorizes my informal math and statistics articles and blog posts. See also journal articles and technical reports.\n\nCategories\n\nElementary Preventing and detecting errors Interpreting and misinterpreting probabilities Modeling Clinical trials Mathematical statistics Random number generation Practicalities Pure math\n\nElementary\n\nTukey tallying Honeybee genealogy Coping with exponential growth Selection bias and bombers Canonical example of Bayes' theorem in detail Linear interpolation Quadratic interpolation Sales tax included Six degrees of Paul Erdős Connecting Fibonacci and geometric sequences Divisibility rules in hexadecimal Golden ratio and special angles Silver ratio Three rules of thumb Approximations with Pythagorean triangles Best rational approximation There isn’t a googol of anything\n\nPreventing and detecting errors\n\nReproducible research In praise of tedious proofs Errors in math papers Proofs of false statements Probability that a study result is true Literate programming and statistics Irreproducible analysis Programming the last mile Complementary validation Troubleshooting Sweave Preventing an unpleasant Sweave surprise\n\nInterpreting and misinterpreting probabilities\n\nPlausible reasoning How loud is the evidence? What is a confidence interval? Probability that a number is prime Learning is not the same as gaining information Laws of large numbers and small numbers The law of small numbers Probability and information Example of the law of small numbers What a probability means Irrelevant uncertainty False positives for medical papers False positives for medical tests Most published research results are false Musicians, drunks, and Oliver Cromwell\n\nModeling\n\nC. S. Lewis on modeling Thick tails Metabolism and power laws Time spent in coffee shops Networks and power laws Why heights are normally distributed Why heights are not normally distributed How to linearize data for regression Approximate problems and approximate solutions Wine, Beer, and Statistics What happens when you add a new teller? (queuing theory) Adult heights and mixture distributions The probability that Shakespeare wrote a play Hard disk array failure probabilities Are men better than women at chess? Small advantages show up in the extremes Ignorance doesn't change reality Server utilization: Joel on queuing Cost-benefit analysis versus benefit-only analysis\n\nClinical trials\n\nStopping trials of ineffective drugs earlier Three ways of tuning an adaptively randomized trial Galen and clinical trials Dose-finding: why start at the lowest dose? Innovation II (scurvy trial from 1601) Most published research results are false False positives for medical papers Population drift Randomized trials of parachute use Small effective sample size does not mean uninformative Early evidence-based medicine\n\nMathematical statistics\n\nClickable chart of distribution relationships Upper and lower bounds for normal probabilities Student-t distribution as a mixture of normals Sums of uniform random variables Conjugate prior relationships Robust priors Determining parameters from quantiles Quantifying the error in the central limit theorem Four characterizations of the normal distribution Random inequalities I, II, III, IV, V, VI, VII, VIII Estimating Cauchy distribution location parameter Inverse gamma distribution Negative binomial distribution Relating two definitions of expectation Bias Four types of errors Stochastic independence Normal approximation details: beta binomial gamma Poisson Student-t Logistic Cosine Poisson approximation to binomial Wilson-Hilferty approximation for the Poisson CDF Camp-Paulson approximation for the binomial CDF Relative error in normal approximations Stable distributions Unbiased estimators can be terrible Bias, consistency, and efficiency Criticisms of significance testing Canonical examples from robust statistics Example of efficiency for mean vs. median Estimating reporting rates\n\nRandom number generation\n\nPitfalls in Random Number Generation Random number generator controversy Random number generation in C++ TR1 Rolling dice for normal samples\n\nPracticalities\n\nApproximating a solution that doesn't exist Making a singular matrix non-singular Ten surprises from numerical linear algebra Microsoft equation editor improved Soft maximum and how to compute it IEEE arithmetic exceptions in C++, Python Computing standard deviation Computing regression coefficients Fibonacci numbers at work (numerical integration) Five kinds of subscripts in R Computing the inverse of the normal CDF Approximate problems and approximate solutions How to calculate binomial probabilities How to calculate correlation accurately Overflow and loss of precision What to make “u” in integration by parts Integration and pragmatism Quick TeX to graphic utility LaTeX on Windows Everything begins with “p” Mercator projection Asymptotic notation R for Programmers coming from other languages Step size for numerical differential equations The error function and the normal distribution Working with distributions in Mathematica, R/S-PLUS, Excel, and Python Random number generation in C++ Innovation IV (Tukey on efficiency) Index of tail weight Delta method When the normal approximation to the Student t isn't good enough Double exponential integration Floating point numbers are a leaky abstraction Convex optimization Code for the error function and normal CDF Anatomy of a floating point number\n\nPure math\n\nModes of convergence Breastfeeding, the golden ratio, and rational approximation Finite differences A knight’s random walk How to differentiate a non-differentiable function (intro to distribution theory) Navier-Stokes equations Splitting a convex set through its center Log concave functions The gamma function Binomial coefficient generalizations Multi-index notation Computing binomial coefficients Upper and lower bounds on binomial coefficients Selecting with replacement (Stanley's symbol) Stanley's twelvefold way (combinatorics notes) Jenga mathematics What is the cosine of a matrix? Gilbreath's conjecture Orthogonal polynomials Chebyshev polynomials Hypergeometric functions Functions of regular variation Determining whether a number is a square How many numbers are squares mod m Fast exponentiation Linear congruences Quadratic congruences Constructive proof of the Chinese remainder theorem Old math books The smoothest curve through a set of points Quasi-random sequences in art and integration Fractional derivatives Means and inequalities Polynomial interpolation error Sledgehammer technique for trig integrals Spherical trigonometry Why care about spherical trig? The pqr theorem A surprising theorem in complex variables Perfect numbers: even, odd Fractional integration Diagram of Bessel functions Gamma function identities Special function diagram\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "math_symbols", "canonical_url": "https://www.johndcook.com/blog/math_symbols/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/math_symbols.html", "title": "Common Math Symbols in HTML, XML, TeX, and Unicode", "heading": "Common Math Symbols in HTML, XML, TeX, and Unicode", "description": "Common Math Symbols in HTML, XML, TeX, and Unicode.", "summary": "Here I list some the symbols that come up frequently in math and describe how to represent them in TeX, HTML, XML, and Unicode.", "word_count": 327, "blocks": [{"tag": "h1", "text": "Common Math Symbols in HTML, XML, TeX, and Unicode"}, {"tag": "p", "text": "Here I list some the symbols that come up frequently in math and describe how to represent them in TeX, HTML, XML, and Unicode."}, {"tag": "p", "text": "HTML provides a mnemonic form for 252 of the most common symbols. (See a full list from the W3C.) These are called \"character entities.\" You can simply put Unicode characters directly into an HTML page as long as you have an input method and the content-type of your HTML page is correctly set. However character entities let you specify non-ASCII characters in HTML using only ASCII text."}, {"tag": "p", "text": "See notes below"}, {"tag": "p", "text": "Hex forms XML Browser compatibility notes Other resources"}, {"tag": "h2", "text": "Table of symbols"}, {"tag": "h2", "text": "Hex forms"}, {"tag": "p", "text": "The hex representation of a character in HTML is the Unicode value in hex with &# added on the left and ; on the right. For example, the symbol ∞ can be written &infin; or &#x221E; based on its Unicode value x221E. Internet Explorer 4.01 did not support hex representations, but all newer browsers do."}, {"tag": "h2", "text": "XML"}, {"tag": "p", "text": "Most HTML entities are not legal in XML. You must use the hex representations instead. Note that you can insert other Unicode characters this way, even if they do correspond to an HTML character entity. However, you run the risk of some users not having the necessary fonts installed on their computer."}, {"tag": "h2", "text": "Browser compatibility notes"}, {"tag": "p", "text": "The symbols above display correctly in Internet Explorer 4.01 and later with three exceptions: &empty;. &notin;, and &sdot; do not work in IE until version 7. Otherwise all symbols work in a wide variety of browsers."}, {"tag": "p", "text": "You can access a huge collection of symbols by inserting Unicode characters. However, you cannot count on a client having the necessary fonts installed to display less common symbols. See Unicode Codepoint chart."}, {"tag": "h2", "text": "Other resources"}, {"tag": "p", "text": "A complete list of LaTeX symbols is available here."}, {"tag": "p", "text": "You can find more about Unicode from the Unicode Consortium."}, {"tag": "p", "text": "See also Greek letters in HTML, XML, TeX, and Unicode."}], "content": "Common Math Symbols in HTML, XML, TeX, and Unicode\n\nHere I list some the symbols that come up frequently in math and describe how to represent them in TeX, HTML, XML, and Unicode.\n\nHTML provides a mnemonic form for 252 of the most common symbols. (See a full list from the W3C.) These are called \"character entities.\" You can simply put Unicode characters directly into an HTML page as long as you have an input method and the content-type of your HTML page is correctly set. However character entities let you specify non-ASCII characters in HTML using only ASCII text.\n\nSee notes below\n\nHex forms XML Browser compatibility notes Other resources\n\nTable of symbols\n\nHex forms\n\nThe hex representation of a character in HTML is the Unicode value in hex with &# added on the left and ; on the right. For example, the symbol ∞ can be written &infin; or &#x221E; based on its Unicode value x221E. Internet Explorer 4.01 did not support hex representations, but all newer browsers do.\n\nXML\n\nMost HTML entities are not legal in XML. You must use the hex representations instead. Note that you can insert other Unicode characters this way, even if they do correspond to an HTML character entity. However, you run the risk of some users not having the necessary fonts installed on their computer.\n\nBrowser compatibility notes\n\nThe symbols above display correctly in Internet Explorer 4.01 and later with three exceptions: &empty;. &notin;, and &sdot; do not work in IE until version 7. Otherwise all symbols work in a wide variety of browsers.\n\nYou can access a huge collection of symbols by inserting Unicode characters. However, you cannot count on a client having the necessary fonts installed to display less common symbols. See Unicode Codepoint chart.\n\nOther resources\n\nA complete list of LaTeX symbols is available here.\n\nYou can find more about Unicode from the Unicode Consortium.\n\nSee also Greek letters in HTML, XML, TeX, and Unicode."}
{"slug": "mathematica_regex", "canonical_url": "https://www.johndcook.com/blog/mathematica_regex/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/mathematica_regex.html", "title": "Regular expressions in Mathematica", "heading": "Regular expressions in Mathematica", "description": "Working with regular expressions in Mathematica with some comparisons to Perl.", "summary": "Overview Mathematica's regular expression flavor Matching and replacing Replacing Case-sensitivity More about regular expressions", "word_count": 439, "blocks": [{"tag": "h1", "text": "Regular expressions in Mathematica"}, {"tag": "p", "text": "Overview Mathematica's regular expression flavor Matching and replacing Replacing Case-sensitivity More about regular expressions"}, {"tag": "h2", "text": "Overview"}, {"tag": "p", "text": "This page is written for the benefit of someone familiar with regular expressions but not with the use of regular expressions in Mathematical. Comparisons will be made with Perl for those familiar with the language, though no knowledge of Perl is required."}, {"tag": "h2", "text": "Mathematica's regular expression flavor"}, {"tag": "p", "text": "Mathematica uses essentially the same regular expression flavor as Perl 5. Specifically, Mathematica is compatible with PCRE. Note that metacharacters require two backslashes. For example, the\\d shortcut for matching any digit must be written as \\\\d."}, {"tag": "h2", "text": "Matching"}, {"tag": "p", "text": "The Mathematica function StringFreeQ is analogous to the m// operator in Perl. However, The logic of StringFreeQ is inverted compared to m// because it returns whether a string is \"free\" of a pattern, i.e. it returns True if the string does not contain the pattern and False if it does."}, {"tag": "p", "text": "The first argument to StringFreeQ is the string to search. The second argument can be a simple string or a regular expression."}, {"tag": "p", "text": "Examples:"}, {"tag": "p", "text": "StringFreeQ[\"Hello world\", \"ello\"]"}, {"tag": "p", "text": "returns False because the string \"Hello world\" does contain \"ello\"."}, {"tag": "p", "text": "StringFreeQ[\"Hello world\", \"el+o\"]"}, {"tag": "p", "text": "returns True because \"Hello world\" does not contain the literal string \"el+o\". However"}, {"tag": "p", "text": "StringFreeQ[\"Hello world\", RegularExpression[\"el+o\"]]"}, {"tag": "h2", "text": "Replacing"}, {"tag": "p", "text": "StringReplace[\"Hello world\", RegularExpression[\"world\"] -> \"planet\"]"}, {"tag": "p", "text": "returns \"Hello planet\". Note that StringReplace does not modify its arguments. If the replacement pattern needs to reference captured subexpressions, these can be accessed by $1, $2, etc. just as in Perl."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "StringReplace[\"Hello world\", RegularExpression[\"(world)\"] -> \"planet $1\"]"}, {"tag": "p", "text": "returns \"Hello planet world\"."}, {"tag": "p", "text": "Note that StringReplace replaces all matches in a string by default, and so it is more precisely analogous to s//g than s//. To limit the number of matches, add a third argument specifying the maximum number of replacements. For example, adding 1 as the last argument causes only the first instance to be replaced."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "StringReplace[\"Hello world\", RegularExpression[\"o\"] -> \"x\", 1]"}, {"tag": "p", "text": "returns \"Hellx world\". Without the final argument it would have returned \"Hellx wxrld.\""}, {"tag": "h2", "text": "Case-sensitivity"}, {"tag": "p", "text": "The m// and s/// operators in Perl are case-sensitive by default. Perl has two ways of making these operators case-insensitive. One is by appending an 'i' following the operator. The other is to add (?i) to the beginning of the regular expression."}, {"tag": "p", "text": "Mathematica is also case-sensitive by default, and it also has two ways of changing the case-sensitivity. One is to use the attribute IgnoreCase -> True. The other is to add (?i) to the regular expression as in Perl."}, {"tag": "h2", "text": "More about regular expressions"}, {"tag": "p", "text": "Notes on using regular expression in other languages: C++, Python, R, PowerShell"}, {"tag": "p", "text": "Tips for getting started with regular expressions"}, {"tag": "p", "text": "Daily tips on regular expressions"}], "content": "Regular expressions in Mathematica\n\nOverview Mathematica's regular expression flavor Matching and replacing Replacing Case-sensitivity More about regular expressions\n\nOverview\n\nThis page is written for the benefit of someone familiar with regular expressions but not with the use of regular expressions in Mathematical. Comparisons will be made with Perl for those familiar with the language, though no knowledge of Perl is required.\n\nMathematica's regular expression flavor\n\nMathematica uses essentially the same regular expression flavor as Perl 5. Specifically, Mathematica is compatible with PCRE. Note that metacharacters require two backslashes. For example, the\\d shortcut for matching any digit must be written as \\\\d.\n\nMatching\n\nThe Mathematica function StringFreeQ is analogous to the m// operator in Perl. However, The logic of StringFreeQ is inverted compared to m// because it returns whether a string is \"free\" of a pattern, i.e. it returns True if the string does not contain the pattern and False if it does.\n\nThe first argument to StringFreeQ is the string to search. The second argument can be a simple string or a regular expression.\n\nExamples:\n\nStringFreeQ[\"Hello world\", \"ello\"]\n\nreturns False because the string \"Hello world\" does contain \"ello\".\n\nStringFreeQ[\"Hello world\", \"el+o\"]\n\nreturns True because \"Hello world\" does not contain the literal string \"el+o\". However\n\nStringFreeQ[\"Hello world\", RegularExpression[\"el+o\"]]\n\nReplacing\n\nStringReplace[\"Hello world\", RegularExpression[\"world\"] -> \"planet\"]\n\nreturns \"Hello planet\". Note that StringReplace does not modify its arguments. If the replacement pattern needs to reference captured subexpressions, these can be accessed by $1, $2, etc. just as in Perl.\n\nExample:\n\nStringReplace[\"Hello world\", RegularExpression[\"(world)\"] -> \"planet $1\"]\n\nreturns \"Hello planet world\".\n\nNote that StringReplace replaces all matches in a string by default, and so it is more precisely analogous to s//g than s//. To limit the number of matches, add a third argument specifying the maximum number of replacements. For example, adding 1 as the last argument causes only the first instance to be replaced.\n\nExample:\n\nStringReplace[\"Hello world\", RegularExpression[\"o\"] -> \"x\", 1]\n\nreturns \"Hellx world\". Without the final argument it would have returned \"Hellx wxrld.\"\n\nCase-sensitivity\n\nThe m// and s/// operators in Perl are case-sensitive by default. Perl has two ways of making these operators case-insensitive. One is by appending an 'i' following the operator. The other is to add (?i) to the beginning of the regular expression.\n\nMathematica is also case-sensitive by default, and it also has two ways of changing the case-sensitivity. One is to use the attribute IgnoreCase -> True. The other is to add (?i) to the regular expression as in Perl.\n\nMore about regular expressions\n\nNotes on using regular expression in other languages: C++, Python, R, PowerShell\n\nTips for getting started with regular expressions\n\nDaily tips on regular expressions"}
{"slug": "mixture_distribution", "canonical_url": "https://www.johndcook.com/blog/mixture_distribution/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/mixture_distribution.html", "title": "Adult heights and mixture distributions", "heading": "Adult heights and mixture distributions", "description": "Looking at the distribution of adult heights to illustrate basic properties of mixture distributions.", "summary": "It is well known that adult male heights follow a normal (Gaussian) distribution. The same is true of adult female heights. What does the distribution of adults in general look like? There are several qualitatively different answers depending on minor changes to some basic assumptions.", "word_count": 650, "blocks": [{"tag": "h1", "text": "Adult heights and mixture distributions"}, {"tag": "p", "text": "It is well known that adult male heights follow a normal (Gaussian) distribution. The same is true of adult female heights. What does the distribution of adults in general look like? There are several qualitatively different answers depending on minor changes to some basic assumptions."}, {"tag": "p", "text": "First, assume adult male heights are normally distributed with mean 70 inches and standard deviation 3 inches. Assume also that adult female heights are normally distributed with mean 64 inches and standard deviation 3 inches. These numbers are approximately correct for Americans; the averages vary by a few inches from country to country."}, {"tag": "p", "text": "Under these assumptions, the probability density for a woman's height is as follows."}, {"tag": "p", "text": "The corresponding density for men is the same, shifted to the right."}, {"tag": "p", "text": "If we assume an equal number of men and women, the probability density for the height of an adult without regard to sex is given below."}, {"tag": "p", "text": "Note that this density is not Gaussian at all. Instead, it is very flat on top. You might reason that since the average of normal random variables is normal, adult heights should be normal. But we don't have an average, we have a mixture. The density for the general adult population is a mixture of the male and female distributions. If you assigned a height to married couples as an average of the husband's height and the wife's height, the resulting value would be an average than a mixture and would follow a normal density."}, {"tag": "p", "text": "The flat top of the density above is not typical. If you have two populations with the same standard deviation and take a 50-50 mixture, the mixture will be symmetric about the average of the two population means. The second derivative of the density at the point of symmetry will be negative if the two population means are less than two standard deviations apart. For example, if the standard deviation had been 3.2 rather than 3.0, the two population means, 64 inches and 70 inches, would be less than two standard deviations apart, and the density of the mixture would be rounded at the mode of 67 inches."}, {"tag": "p", "text": "The second derivative of the density will be positive in the middle if the two population means are more than two standard deviations apart. For example, if the standard deviations had been 2.8, the population means would be more than two standard deviations apart and the middle value of 67 inches would be a local minimum. (The value of 2.8 may be fairly accurate. One web site I found said the standard deviation is 2.8, but I have no idea whether that site was reliable.)"}, {"tag": "p", "text": "If the two population means are close to two standard deviations apart, the mixture density is still approximately flat on top, flatter than a normal density. But only when the population means are exactly two standard deviations apart is the mixture distribution completely flat on top, i.e. only then is the second derivative zero in the middle."}, {"tag": "p", "text": "The calculations above have assumed the proportions of men and women were exactly equal. If we assume women form 51% of the population, then the density becomes slightly asymmetrical."}, {"tag": "p", "text": "After this page was first posted, I found out about a couple related references."}, {"tag": "p", "text": "The American Statistician had an article Is Human Height Bimodal? on this topic in 2002. That same year Andrew Gelman and Deborah Nolan published Teaching Statistics: A Bag of Tricks which also deals with this subject. Gelman and Nolan point out that the variance of men's heights is slightly larger than that for women. If we assume a 50-50 split of men and women, but assume male heights have a standard deviation of 3 inches while female heights have a standard deviation of 2.8 inches, this tilts the graph to the left more than assuming equal variance but unequal proportions above."}, {"tag": "p", "text": "See also Why heights are normally distributed."}], "content": "Adult heights and mixture distributions\n\nIt is well known that adult male heights follow a normal (Gaussian) distribution. The same is true of adult female heights. What does the distribution of adults in general look like? There are several qualitatively different answers depending on minor changes to some basic assumptions.\n\nFirst, assume adult male heights are normally distributed with mean 70 inches and standard deviation 3 inches. Assume also that adult female heights are normally distributed with mean 64 inches and standard deviation 3 inches. These numbers are approximately correct for Americans; the averages vary by a few inches from country to country.\n\nUnder these assumptions, the probability density for a woman's height is as follows.\n\nThe corresponding density for men is the same, shifted to the right.\n\nIf we assume an equal number of men and women, the probability density for the height of an adult without regard to sex is given below.\n\nNote that this density is not Gaussian at all. Instead, it is very flat on top. You might reason that since the average of normal random variables is normal, adult heights should be normal. But we don't have an average, we have a mixture. The density for the general adult population is a mixture of the male and female distributions. If you assigned a height to married couples as an average of the husband's height and the wife's height, the resulting value would be an average than a mixture and would follow a normal density.\n\nThe flat top of the density above is not typical. If you have two populations with the same standard deviation and take a 50-50 mixture, the mixture will be symmetric about the average of the two population means. The second derivative of the density at the point of symmetry will be negative if the two population means are less than two standard deviations apart. For example, if the standard deviation had been 3.2 rather than 3.0, the two population means, 64 inches and 70 inches, would be less than two standard deviations apart, and the density of the mixture would be rounded at the mode of 67 inches.\n\nThe second derivative of the density will be positive in the middle if the two population means are more than two standard deviations apart. For example, if the standard deviations had been 2.8, the population means would be more than two standard deviations apart and the middle value of 67 inches would be a local minimum. (The value of 2.8 may be fairly accurate. One web site I found said the standard deviation is 2.8, but I have no idea whether that site was reliable.)\n\nIf the two population means are close to two standard deviations apart, the mixture density is still approximately flat on top, flatter than a normal density. But only when the population means are exactly two standard deviations apart is the mixture distribution completely flat on top, i.e. only then is the second derivative zero in the middle.\n\nThe calculations above have assumed the proportions of men and women were exactly equal. If we assume women form 51% of the population, then the density becomes slightly asymmetrical.\n\nAfter this page was first posted, I found out about a couple related references.\n\nThe American Statistician had an article Is Human Height Bimodal? on this topic in 2002. That same year Andrew Gelman and Deborah Nolan published Teaching Statistics: A Bag of Tricks which also deals with this subject. Gelman and Nolan point out that the variance of men's heights is slightly larger than that for women. If we assume a 50-50 split of men and women, but assume male heights have a standard deviation of 3 inches while female heights have a standard deviation of 2.8 inches, this tilts the graph to the left more than assuming equal variance but unequal proportions above.\n\nSee also Why heights are normally distributed."}
{"slug": "normal_approx_to_beta", "canonical_url": "https://www.johndcook.com/blog/normal_approx_to_beta/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/normal_approx_to_beta.html", "title": "Error in the normal approximation to the beta distribution", "heading": "Error in the normal approximation to the beta distribution", "description": "The error in the normal approximation to the beta distribution", "summary": "A beta(a, b) distribution is approximately normal if the parameters a and b are large and approximately equal. A beta(a,b) distribution has mean a/(a+b) and variance ab/(a+b)2(a+b+1). When a=b, this reduces to mean 1/2 and variance 1/(8a + 4).", "word_count": 415, "blocks": [{"tag": "h1", "text": "Error in the normal approximation to the beta distribution"}, {"tag": "p", "text": "A beta(a, b) distribution is approximately normal if the parameters a and b are large and approximately equal. A beta(a,b) distribution has mean a/(a+b) and variance ab/(a+b)2(a+b+1). When a=b, this reduces to mean 1/2 and variance 1/(8a + 4)."}, {"tag": "p", "text": "The following graph shows the difference between the CDF of a beta(5,5) distribution and the CDF of normal distribution with the same mean and variance, i.e. mean 1/2 and variance 1/44."}, {"tag": "p", "text": "As the beta parameters increase, the amplitude of the error curve decreases and the curve shrinks toward the middle. Below is a graph of the CDF of a beta(20,20) distribution minus the CDF of a normal with mean 1/2 and variance 1/164."}, {"tag": "p", "text": "The error curve for ν = 30 is below."}, {"tag": "p", "text": "Here is the maximum error as a function of the common beta parameter."}, {"tag": "p", "text": "The quality of the approximation degrades when the a and b parameters are not approximately equal. Also, when the parameters are not equal, the error function loses its symmetry. For example, for a beta(40,40) distribution, the error curve resembles the error curves above and has a maximum value of 0.0017. The error curve for the normal approximation to a beta(30,50) distribution is given below."}, {"tag": "p", "text": "The maximum error is about 0.008, over twice the error in the normal approximation to a beta(30,50) distribution and over four times the error in the normal approximation to a beta(40,40) distribution."}, {"tag": "p", "text": "In 1960, Wise published a transformation that improves the normal approximation to the beta (Biometrika vol 47, No. 1/2, June 1960, pp. 173-175). If X is a beta(a, b) random variable with a ≥ b, (-log X)1/3 is more nearly normal than X is."}, {"tag": "p", "text": "Here we show how well Wise's transformation works for X ~ beta(5,4). The follow graph shows the PDFs of X and its normal approximation."}, {"tag": "p", "text": "The graph below shows the difference in the CDFs of the two distributions."}, {"tag": "p", "text": "Next we apply Wise's transformation. The random variable Y = (-log X)1/3 has mean 0.835 and variance 0.0209. When we plot the PDFs of the two distributions, graphs appear to lie on top of each other. The following is the graph of the difference between the CDF functions."}, {"tag": "p", "text": "The maximum error has been reduced from about 0.02 to about 0.003."}, {"tag": "p", "text": "Note that this page has only considered absolute error in the normal approximation. The relative error is a different story."}, {"tag": "p", "text": "See also notes on the normal approximation to the binomial, gamma, Poisson, and student-t distributions."}], "content": "Error in the normal approximation to the beta distribution\n\nA beta(a, b) distribution is approximately normal if the parameters a and b are large and approximately equal. A beta(a,b) distribution has mean a/(a+b) and variance ab/(a+b)2(a+b+1). When a=b, this reduces to mean 1/2 and variance 1/(8a + 4).\n\nThe following graph shows the difference between the CDF of a beta(5,5) distribution and the CDF of normal distribution with the same mean and variance, i.e. mean 1/2 and variance 1/44.\n\nAs the beta parameters increase, the amplitude of the error curve decreases and the curve shrinks toward the middle. Below is a graph of the CDF of a beta(20,20) distribution minus the CDF of a normal with mean 1/2 and variance 1/164.\n\nThe error curve for ν = 30 is below.\n\nHere is the maximum error as a function of the common beta parameter.\n\nThe quality of the approximation degrades when the a and b parameters are not approximately equal. Also, when the parameters are not equal, the error function loses its symmetry. For example, for a beta(40,40) distribution, the error curve resembles the error curves above and has a maximum value of 0.0017. The error curve for the normal approximation to a beta(30,50) distribution is given below.\n\nThe maximum error is about 0.008, over twice the error in the normal approximation to a beta(30,50) distribution and over four times the error in the normal approximation to a beta(40,40) distribution.\n\nIn 1960, Wise published a transformation that improves the normal approximation to the beta (Biometrika vol 47, No. 1/2, June 1960, pp. 173-175). If X is a beta(a, b) random variable with a ≥ b, (-log X)1/3 is more nearly normal than X is.\n\nHere we show how well Wise's transformation works for X ~ beta(5,4). The follow graph shows the PDFs of X and its normal approximation.\n\nThe graph below shows the difference in the CDFs of the two distributions.\n\nNext we apply Wise's transformation. The random variable Y = (-log X)1/3 has mean 0.835 and variance 0.0209. When we plot the PDFs of the two distributions, graphs appear to lie on top of each other. The following is the graph of the difference between the CDF functions.\n\nThe maximum error has been reduced from about 0.02 to about 0.003.\n\nNote that this page has only considered absolute error in the normal approximation. The relative error is a different story.\n\nSee also notes on the normal approximation to the binomial, gamma, Poisson, and student-t distributions."}
{"slug": "normal_approx_to_binomial", "canonical_url": "https://www.johndcook.com/blog/normal_approx_to_binomial/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/normal_approx_to_binomial.html", "title": "Error in the normal approximation to the binomial distribution", "heading": "Error in the normal approximation to the binomial distribution", "description": "A study of the error in the normal approximation to the binomial distribution", "summary": "The binomial distribution can often be well approximated by a normal distribution. But how can you know when the approximation will be good? Most textbooks are vague on this point, saying “n should be large” or “np should be large.” How large? Why?", "word_count": 700, "blocks": [{"tag": "h1", "text": "Error in the normal approximation to the binomial distribution"}, {"tag": "p", "text": "The binomial distribution can often be well approximated by a normal distribution. But how can you know when the approximation will be good? Most textbooks are vague on this point, saying “n should be large” or “np should be large.” How large? Why?"}, {"tag": "p", "text": "These notes will look carefully at the error in the normal approximation to the binomial distribution. We will look at how the error varies as a function of the binomial parameters n and p and demonstrate how the continuity correction improves the approximation."}, {"tag": "h2", "text": "Central Limit Theorem"}, {"tag": "p", "text": "A binomial(n, p) random variable X can be thought of as the sum of n Bernoulli random variables Xi. Applying the Central Limit Theorem to this sum shows that FX, the CDF (cumulative distribution function) of X, is approximately equal to FY, the CDF of a normal random variable Y with the name mean and variance as X. That is, Y has mean np and variance npq where q = 1-p."}, {"tag": "p", "text": "But how good is the approximation? The Berry-Esséen theorem gives an upper bound on the error. It says the error is uniformly bounded by C ρ/σ3√n where C is a constant less than 0.7655, ρ = E(|Xi - p|3) and σ is the standard deviation of Xi. It's easy to calculate ρ = pq(p2 + q2) and σ = √(pq) for the Bernoulli random variables Xi. Therefore the error in the normal approximation to a binomial(n, p) random variable is bounded by C(p2 + q2) /√(npq)."}, {"tag": "h3", "text": "Error as a function of n"}, {"tag": "p", "text": "For fixed p, the bound C(p2 + q2) /√(npq) from the Berry-Esséen theorem says that the maximum error decreases proportional to 1/√n. Hence the recommendation that the approximation be used for large n."}, {"tag": "h3", "text": "Error as a function of p"}, {"tag": "p", "text": "The term (p2 + q2) /√(pq) is smallest when p = 1/2. This suggests that for a given value of n, the normal approximation is best when p is near 1/2. However, the function (p2 + q2) /√(pq) is unbounded as p approaches either 0 or 1."}, {"tag": "p", "text": "Assume p < 1/2 (or else reverse the rolls of p and q). For 0 < p < 1/2, one can show that (p2 + q2) /√(pq) < 1/√p. Therefore the approximation error is bounded by a constant times 1/√(np), hence the suggestion that np should be \"large.\""}, {"tag": "p", "text": "So a conservative estimate on the error is 0.7655/√(np) when p < 1/2."}, {"tag": "h2", "text": "Examples"}, {"tag": "p", "text": "The following plot shows the error in the normal approximation to the CDF of a binomial(10, 0.5) random variable. Here we are computing FX(n) - FY(n) for n = 0, 1, ..., 10."}, {"tag": "p", "text": "Next we compute the error again, but using the continuity correction, FX(n) - FY(n+1/2)."}, {"tag": "p", "text": "The continuity correction lowers the maximum error from 0.123 to 0.00267, making the error 47 times smaller."}, {"tag": "p", "text": "We expect the error would be larger for p = 0.1 than it was for p = 0.5 above. Indeed this is the case. For a binomial(10, 0.1) random variable, the maximum error in the normal approximation is 0.05 even when using the continuity correction."}, {"tag": "p", "text": "For another example we consider a binomial(100, 0.1) random variable. The following graph gives the approximation without continuity correction."}, {"tag": "p", "text": "And here is the error when using the continuity correction."}, {"tag": "p", "text": "In this case, n is larger and the benefit of the continuity correction is not as large. Still, the correction reduces the error by a factor of 4.8."}, {"tag": "h2", "text": "Approximating the PMF"}, {"tag": "p", "text": "Up to this point we have only looked at approximating the CDF. Now we look at approximating the probability of individual points, i.e. we look at the probability mass function (PMF)."}, {"tag": "p", "text": "The naive approximation would be to approximate fX(n) with fY(n). However, the continuity correction requires we approximate fX(n) by the integral"}, {"tag": "p", "text": "As the examples above suggest, the continuity correction greatly improves the accuracy of the approximation."}, {"tag": "h2", "text": "Other normal approximations"}, {"tag": "p", "text": "The Camp-Paulson approximation for the binomial distribution function also uses a normal distribution but requires a non-linear transformation of the argument. The result is an approximation that can be one or two orders of magnitude more accurate."}, {"tag": "p", "text": "See also notes on the normal approximation to the beta, gamma, Poisson, and student-t distributions."}], "content": "Error in the normal approximation to the binomial distribution\n\nThe binomial distribution can often be well approximated by a normal distribution. But how can you know when the approximation will be good? Most textbooks are vague on this point, saying “n should be large” or “np should be large.” How large? Why?\n\nThese notes will look carefully at the error in the normal approximation to the binomial distribution. We will look at how the error varies as a function of the binomial parameters n and p and demonstrate how the continuity correction improves the approximation.\n\nCentral Limit Theorem\n\nA binomial(n, p) random variable X can be thought of as the sum of n Bernoulli random variables Xi. Applying the Central Limit Theorem to this sum shows that FX, the CDF (cumulative distribution function) of X, is approximately equal to FY, the CDF of a normal random variable Y with the name mean and variance as X. That is, Y has mean np and variance npq where q = 1-p.\n\nBut how good is the approximation? The Berry-Esséen theorem gives an upper bound on the error. It says the error is uniformly bounded by C ρ/σ3√n where C is a constant less than 0.7655, ρ = E(|Xi - p|3) and σ is the standard deviation of Xi. It's easy to calculate ρ = pq(p2 + q2) and σ = √(pq) for the Bernoulli random variables Xi. Therefore the error in the normal approximation to a binomial(n, p) random variable is bounded by C(p2 + q2) /√(npq).\n\nError as a function of n\n\nFor fixed p, the bound C(p2 + q2) /√(npq) from the Berry-Esséen theorem says that the maximum error decreases proportional to 1/√n. Hence the recommendation that the approximation be used for large n.\n\nError as a function of p\n\nThe term (p2 + q2) /√(pq) is smallest when p = 1/2. This suggests that for a given value of n, the normal approximation is best when p is near 1/2. However, the function (p2 + q2) /√(pq) is unbounded as p approaches either 0 or 1.\n\nAssume p < 1/2 (or else reverse the rolls of p and q). For 0 < p < 1/2, one can show that (p2 + q2) /√(pq) < 1/√p. Therefore the approximation error is bounded by a constant times 1/√(np), hence the suggestion that np should be \"large.\"\n\nSo a conservative estimate on the error is 0.7655/√(np) when p < 1/2.\n\nExamples\n\nThe following plot shows the error in the normal approximation to the CDF of a binomial(10, 0.5) random variable. Here we are computing FX(n) - FY(n) for n = 0, 1, ..., 10.\n\nNext we compute the error again, but using the continuity correction, FX(n) - FY(n+1/2).\n\nThe continuity correction lowers the maximum error from 0.123 to 0.00267, making the error 47 times smaller.\n\nWe expect the error would be larger for p = 0.1 than it was for p = 0.5 above. Indeed this is the case. For a binomial(10, 0.1) random variable, the maximum error in the normal approximation is 0.05 even when using the continuity correction.\n\nFor another example we consider a binomial(100, 0.1) random variable. The following graph gives the approximation without continuity correction.\n\nAnd here is the error when using the continuity correction.\n\nIn this case, n is larger and the benefit of the continuity correction is not as large. Still, the correction reduces the error by a factor of 4.8.\n\nApproximating the PMF\n\nUp to this point we have only looked at approximating the CDF. Now we look at approximating the probability of individual points, i.e. we look at the probability mass function (PMF).\n\nThe naive approximation would be to approximate fX(n) with fY(n). However, the continuity correction requires we approximate fX(n) by the integral\n\nAs the examples above suggest, the continuity correction greatly improves the accuracy of the approximation.\n\nOther normal approximations\n\nThe Camp-Paulson approximation for the binomial distribution function also uses a normal distribution but requires a non-linear transformation of the argument. The result is an approximation that can be one or two orders of magnitude more accurate.\n\nSee also notes on the normal approximation to the beta, gamma, Poisson, and student-t distributions."}
{"slug": "normal_cdf_inverse", "canonical_url": "https://www.johndcook.com/blog/normal_cdf_inverse/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/normal_cdf_inverse.html", "title": "A literate program to compute the inverse of the normal CDF", "heading": "A literate program to compute the inverse of the normal CDF", "description": "This page describes how to write a program to compute the inverse of the normal CDF, somewhat in the style of a literate program..", "summary": "This page presents C++ code for computing the inverse of the normal (Gaussian) CDF. The emphasis, however, is on the process of writing the code. The discussion points out some of the things people are expected to pick up along the way but may never have been taught explicitly.", "word_count": 1121, "blocks": [{"tag": "h1", "text": "A literate program to compute the inverse of the normal CDF"}, {"tag": "p", "text": "This page presents C++ code for computing the inverse of the normal (Gaussian) CDF. The emphasis, however, is on the process of writing the code. The discussion points out some of the things people are expected to pick up along the way but may never have been taught explicitly."}, {"tag": "p", "text": "This page develops the code in the spirit of a literate program. The code will be developed in small pieces, then the final code presented at the bottom of the page. This page is not actually a literate program but it strives to serve the same purpose."}, {"tag": "p", "text": "Problem statement Basic solution Numerical issues Take away Complete code"}, {"tag": "h2", "text": "Problem statement"}, {"tag": "p", "text": "We need software to compute the inverse of the normal CDF (cumulative density function). That is, given a probability p, compute x such that Prob(Z < x) = p where Z is a standard normal (Gaussian) random variable. We don't need high precision; three or four decimal places will suffice."}, {"tag": "h2", "text": "Basic solution"}, {"tag": "p", "text": "We can look up an algorithm that will essentially compute what we need. But there is some work to do in order to turn the algorithm we find into working code."}, {"tag": "p", "text": "The first place to look for approximations to statistical functions is Abramowitz and Stegun or A&S at it is fondly known. Formula 26.2.23 from A&S is given below."}, {"tag": "p", "text": "A&S describes the formula above as a “rational approximation for xp where Q(xp) = p.” In A&S notation, Q is the CCDF (complementary cumulative distribution function) for a standard normal. Also, A&S says that the approximation is good for 0 < p ≤ 0.5. The reported absolute error in the approximation is less than 4 × 10-4, so the accuracy is sufficient. We have two immediate problems. First, we want to invert the CDF, not the CCDF. Second, we need an algorithm for 0 < p < 1 and not just for p < 0.5."}, {"tag": "p", "text": "Let F(x) be the CDF of a standard normal and let G(x) be the corresponding CCDF. We want to compute the inverse CDF, F-1(p), but A & S gives us a way to compute G-1(p). Now for all x, F(x) + G(x) = 1. So if G(x) = 1-p then F(x) = p. That means that"}, {"tag": "p", "text": "F-1(p) = G-1(1-p)"}, {"tag": "p", "text": "and so our first problem is solved. If p ≥ 0.5, 1 - p ≤ 0.5 and so the approximation from A&S applies. But what if p < 0.5?"}, {"tag": "p", "text": "Since the standard normal distribution is symmetric about zero, the probability of being greater than x is the same as the probability of being less than -x. That is, G(x) = p if and only if F(-x) = p. So x = G-1(p) if and only if x = -F-1(p). Thus"}, {"tag": "p", "text": "F-1(p) = -G-1(p)."}, {"tag": "p", "text": "We can use the above formula to compute F-1(p) for p < 0.5."}, {"tag": "h2", "text": "Numerical issues"}, {"tag": "p", "text": "The first step in applying the formula from A&S is to transform p into sqrt(-2.0*log(p)). So for p < 0.5 we compute t = sqrt(-2.0*log(p)) and evaluate the ratio of polynomials for approximating G-1(p) and flip the sign of the result. For p ≥ 0.5, we compute t = sqrt(-2.0*log(1-p)). If RationalApproximation is the function to evaluate the ratio of polynomials, the code for computing F-1 starts as follows."}, {"tag": "pre", "text": "if (p < 0.5)\n{\n    // F^-1(p) = - G^-1(p)\n    return -RationalApproximation( sqrt(-2.0*log(p)) );\n}\nelse\n{\n    // F^-1(p) = G^-1(1-p)\n    return RationalApproximation( sqrt(-2.0*log(1-p)) );\n}"}, {"tag": "p", "text": "Writing the code for RationalApproximation is easy. However, we introduce a technique along the way. We could evaluate a polynomial of the form a + bx +cx2 + dx3 by computing"}, {"tag": "pre", "text": "a + b*x + c*x*x + d*x*x*x"}, {"tag": "p", "text": "but we could save a few cycles using Horner's method to evaluate the polynomial as"}, {"tag": "pre", "text": "((d*x + c)*x + b)*x + a."}, {"tag": "p", "text": "The time savings isn't much for a low-order polynomial. On the other hand, Horner's rule is easy to apply so it's a good habit to always use it to evaluate polynomials. It could make a difference in evaluating high-order polynomials in the inner loop of a program."}, {"tag": "p", "text": "Using Horner's rule, our code for RationalApproximation is as follows."}, {"tag": "pre", "text": "double RationalApproximation(double t)\n{\n    const double c[] = {2.515517, 0.802853, 0.010328};\n    const double d[] = {1.432788, 0.189269, 0.001308};\n    return t - ((c[2]*t + c[1])*t + c[0]) / \n               (((d[2]*t + d[1])*t + d[0])*t + 1.0);\n}"}, {"tag": "h2", "text": "Take away"}, {"tag": "p", "text": "Here's a summary of a couple things this code illustrates."}, {"tag": "li", "text": "Abramowitz and Stegun is a good place to start when looking for information about statistical functions."}, {"tag": "li", "text": "Horner's method of evaluating polynomials is simple and runs faster than the most direct approach."}, {"tag": "p", "text": "If you want to learn more about literate programming, I recommend Donald Knuth's book Literate Programming."}, {"tag": "h2", "text": "Complete code"}, {"tag": "p", "text": "Here we present the final code with some input validation added. We also include some code to test/demonstrate the code for evaluating F-1."}, {"tag": "pre", "text": "#include <cmath>\n#include <sstream>\n#include <iostream>\n#include <iomanip>\n#include <stdexcept>\n\ndouble LogOnePlusX(double x);\ndouble NormalCDFInverse(double p);\ndouble RationalApproximation(double t);\nvoid demo();\n\ndouble RationalApproximation(double t)\n{\n    // Abramowitz and Stegun formula 26.2.23.\n    // The absolute value of the error should be less than 4.5 e-4.\n    double c[] = {2.515517, 0.802853, 0.010328};\n    double d[] = {1.432788, 0.189269, 0.001308};\n    return t - ((c[2]*t + c[1])*t + c[0]) / \n               (((d[2]*t + d[1])*t + d[0])*t + 1.0);\n}\n\ndouble NormalCDFInverse(double p)\n{\n    if (p <= 0.0 || p >= 1.0)\n    {\n        std::stringstream os;\n        os << \"Invalid input argument (\" << p \n           << \"); must be larger than 0 but less than 1.\";\n        throw std::invalid_argument( os.str() );\n    }\n\n    // See article above for explanation of this section.\n    if (p < 0.5)\n    {\n        // F^-1(p) = - G^-1(p)\n        return -RationalApproximation( sqrt(-2.0*log(p)) );\n    }\n    else\n    {\n        // F^-1(p) = G^-1(1-p)\n        return RationalApproximation( sqrt(-2.0*log(1-p)) );\n    }\n}\n\nvoid demo()\n{\n    std::cout << \"\\nShow that the NormalCDFInverse function is accurate at \\n\"\n              << \"0.05, 0.15, 0.25, ..., 0.95 and at a few extreme values.\\n\\n\";\n\n    double p[] =\n    {\n        0.0000001,\n        0.00001,\n        0.001,\n        0.05,\n        0.15,\n        0.25,\n        0.35,\n        0.45,\n        0.55,\n        0.65,\n        0.75,\n        0.85,\n        0.95,\n        0.999,\n        0.99999,\n        0.9999999\n    };\n\n    // Exact values computed by Mathematica.\n    double exact[] =\n    {\n        -5.199337582187471,\n        -4.264890793922602,\n        -3.090232306167813,\n        -1.6448536269514729,\n        -1.0364333894937896,\n        -0.6744897501960817,\n        -0.38532046640756773,\n        -0.12566134685507402,\n         0.12566134685507402,\n         0.38532046640756773,\n         0.6744897501960817,\n         1.0364333894937896,\n         1.6448536269514729,\n         3.090232306167813,\n         4.264890793922602,\n         5.199337582187471\n    };\n\n    double maxerror = 0.0;\n    int numValues = sizeof(p)/sizeof(double);\n    std::cout << \"p, exact CDF inverse, computed CDF inverse, diff\\n\\n\";\n    std::cout << std::setprecision(7);\n    for (int i = 0; i < numValues; ++i)\n    {\n        double computed = NormalCDFInverse(p[i]);\n        double error = exact[i] - computed;\n        std::cout << p[i] << \", \" << exact[i] << \", \" \n                  << computed << \", \" << error << \"\\n\";\n        if (fabs(error) > maxerror)\n            maxerror = fabs(error);\n    }\n\n    std::cout << \"\\nMaximum error: \" << maxerror << \"\\n\\n\";\n}\n\nint main()\n{\n    demo();\n    return 0;\n}"}], "content": "A literate program to compute the inverse of the normal CDF\n\nThis page presents C++ code for computing the inverse of the normal (Gaussian) CDF. The emphasis, however, is on the process of writing the code. The discussion points out some of the things people are expected to pick up along the way but may never have been taught explicitly.\n\nThis page develops the code in the spirit of a literate program. The code will be developed in small pieces, then the final code presented at the bottom of the page. This page is not actually a literate program but it strives to serve the same purpose.\n\nProblem statement Basic solution Numerical issues Take away Complete code\n\nProblem statement\n\nWe need software to compute the inverse of the normal CDF (cumulative density function). That is, given a probability p, compute x such that Prob(Z < x) = p where Z is a standard normal (Gaussian) random variable. We don't need high precision; three or four decimal places will suffice.\n\nBasic solution\n\nWe can look up an algorithm that will essentially compute what we need. But there is some work to do in order to turn the algorithm we find into working code.\n\nThe first place to look for approximations to statistical functions is Abramowitz and Stegun or A&S at it is fondly known. Formula 26.2.23 from A&S is given below.\n\nA&S describes the formula above as a “rational approximation for xp where Q(xp) = p.” In A&S notation, Q is the CCDF (complementary cumulative distribution function) for a standard normal. Also, A&S says that the approximation is good for 0 < p ≤ 0.5. The reported absolute error in the approximation is less than 4 × 10-4, so the accuracy is sufficient. We have two immediate problems. First, we want to invert the CDF, not the CCDF. Second, we need an algorithm for 0 < p < 1 and not just for p < 0.5.\n\nLet F(x) be the CDF of a standard normal and let G(x) be the corresponding CCDF. We want to compute the inverse CDF, F-1(p), but A & S gives us a way to compute G-1(p). Now for all x, F(x) + G(x) = 1. So if G(x) = 1-p then F(x) = p. That means that\n\nF-1(p) = G-1(1-p)\n\nand so our first problem is solved. If p ≥ 0.5, 1 - p ≤ 0.5 and so the approximation from A&S applies. But what if p < 0.5?\n\nSince the standard normal distribution is symmetric about zero, the probability of being greater than x is the same as the probability of being less than -x. That is, G(x) = p if and only if F(-x) = p. So x = G-1(p) if and only if x = -F-1(p). Thus\n\nF-1(p) = -G-1(p).\n\nWe can use the above formula to compute F-1(p) for p < 0.5.\n\nNumerical issues\n\nThe first step in applying the formula from A&S is to transform p into sqrt(-2.0*log(p)). So for p < 0.5 we compute t = sqrt(-2.0*log(p)) and evaluate the ratio of polynomials for approximating G-1(p) and flip the sign of the result. For p ≥ 0.5, we compute t = sqrt(-2.0*log(1-p)). If RationalApproximation is the function to evaluate the ratio of polynomials, the code for computing F-1 starts as follows.\n\nif (p < 0.5)\n{\n    // F^-1(p) = - G^-1(p)\n    return -RationalApproximation( sqrt(-2.0*log(p)) );\n}\nelse\n{\n    // F^-1(p) = G^-1(1-p)\n    return RationalApproximation( sqrt(-2.0*log(1-p)) );\n}\n\nWriting the code for RationalApproximation is easy. However, we introduce a technique along the way. We could evaluate a polynomial of the form a + bx +cx2 + dx3 by computing\n\na + b*x + c*x*x + d*x*x*x\n\nbut we could save a few cycles using Horner's method to evaluate the polynomial as\n\n((d*x + c)*x + b)*x + a.\n\nThe time savings isn't much for a low-order polynomial. On the other hand, Horner's rule is easy to apply so it's a good habit to always use it to evaluate polynomials. It could make a difference in evaluating high-order polynomials in the inner loop of a program.\n\nUsing Horner's rule, our code for RationalApproximation is as follows.\n\ndouble RationalApproximation(double t)\n{\n    const double c[] = {2.515517, 0.802853, 0.010328};\n    const double d[] = {1.432788, 0.189269, 0.001308};\n    return t - ((c[2]*t + c[1])*t + c[0]) / \n               (((d[2]*t + d[1])*t + d[0])*t + 1.0);\n}\n\nTake away\n\nHere's a summary of a couple things this code illustrates.\n\nAbramowitz and Stegun is a good place to start when looking for information about statistical functions.\n\nHorner's method of evaluating polynomials is simple and runs faster than the most direct approach.\n\nIf you want to learn more about literate programming, I recommend Donald Knuth's book Literate Programming.\n\nComplete code\n\nHere we present the final code with some input validation added. We also include some code to test/demonstrate the code for evaluating F-1.\n\n#include <cmath>\n#include <sstream>\n#include <iostream>\n#include <iomanip>\n#include <stdexcept>\n\ndouble LogOnePlusX(double x);\ndouble NormalCDFInverse(double p);\ndouble RationalApproximation(double t);\nvoid demo();\n\ndouble RationalApproximation(double t)\n{\n    // Abramowitz and Stegun formula 26.2.23.\n    // The absolute value of the error should be less than 4.5 e-4.\n    double c[] = {2.515517, 0.802853, 0.010328};\n    double d[] = {1.432788, 0.189269, 0.001308};\n    return t - ((c[2]*t + c[1])*t + c[0]) / \n               (((d[2]*t + d[1])*t + d[0])*t + 1.0);\n}\n\ndouble NormalCDFInverse(double p)\n{\n    if (p <= 0.0 || p >= 1.0)\n    {\n        std::stringstream os;\n        os << \"Invalid input argument (\" << p \n           << \"); must be larger than 0 but less than 1.\";\n        throw std::invalid_argument( os.str() );\n    }\n\n    // See article above for explanation of this section.\n    if (p < 0.5)\n    {\n        // F^-1(p) = - G^-1(p)\n        return -RationalApproximation( sqrt(-2.0*log(p)) );\n    }\n    else\n    {\n        // F^-1(p) = G^-1(1-p)\n        return RationalApproximation( sqrt(-2.0*log(1-p)) );\n    }\n}\n\nvoid demo()\n{\n    std::cout << \"\\nShow that the NormalCDFInverse function is accurate at \\n\"\n              << \"0.05, 0.15, 0.25, ..., 0.95 and at a few extreme values.\\n\\n\";\n\n    double p[] =\n    {\n        0.0000001,\n        0.00001,\n        0.001,\n        0.05,\n        0.15,\n        0.25,\n        0.35,\n        0.45,\n        0.55,\n        0.65,\n        0.75,\n        0.85,\n        0.95,\n        0.999,\n        0.99999,\n        0.9999999\n    };\n\n    // Exact values computed by Mathematica.\n    double exact[] =\n    {\n        -5.199337582187471,\n        -4.264890793922602,\n        -3.090232306167813,\n        -1.6448536269514729,\n        -1.0364333894937896,\n        -0.6744897501960817,\n        -0.38532046640756773,\n        -0.12566134685507402,\n         0.12566134685507402,\n         0.38532046640756773,\n         0.6744897501960817,\n         1.0364333894937896,\n         1.6448536269514729,\n         3.090232306167813,\n         4.264890793922602,\n         5.199337582187471\n    };\n\n    double maxerror = 0.0;\n    int numValues = sizeof(p)/sizeof(double);\n    std::cout << \"p, exact CDF inverse, computed CDF inverse, diff\\n\\n\";\n    std::cout << std::setprecision(7);\n    for (int i = 0; i < numValues; ++i)\n    {\n        double computed = NormalCDFInverse(p[i]);\n        double error = exact[i] - computed;\n        std::cout << p[i] << \", \" << exact[i] << \", \" \n                  << computed << \", \" << error << \"\\n\";\n        if (fabs(error) > maxerror)\n            maxerror = fabs(error);\n    }\n\n    std::cout << \"\\nMaximum error: \" << maxerror << \"\\n\\n\";\n}\n\nint main()\n{\n    demo();\n    return 0;\n}"}
{"slug": "number_crunching", "canonical_url": "https://www.johndcook.com/blog/number_crunching/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/number_crunching.html", "title": "Number crunching", "heading": "Number crunching", "description": "Number crunching: John D. Cook experience.", "summary": "When I was in graduate school, I specialized in differential equations, studying theory and numerical solution techniques. Along the way I did some consulting in numerical analysis.", "word_count": 213, "blocks": [{"tag": "h1", "text": "Number crunching"}, {"tag": "p", "text": "When I was in graduate school, I specialized in differential equations, studying theory and numerical solution techniques. Along the way I did some consulting in numerical analysis."}, {"tag": "p", "text": "After I finished my Ph.D at University of Texas in 1992, I did a postdoc at Vanderbilt University. There I did research in differential equations and taught, among other things, numerical analysis. I also did some work in finite element error estimation for the U. S. Army Corp of Engineers."}, {"tag": "p", "text": "I then left academia in 1995 to work in the private sector where I did digital signal processing (DSP) and software development."}, {"tag": "p", "text": "In 2000 I came to MD Anderson Cancer Center where I worked in statistical computing, especially Bayesian statistics. There I worked on high-dimensional integration, special function evaluation, simulation, etc. I have also applied these same skills as a consultant for businesses in a variety of industries."}, {"tag": "p", "text": "I have developed scientific software for Windows and Linux using a variety of programming languages: C++, C#, Python, Mathematica, and R. I have written journal articles and technical reports on applied mathematics, computation, and statistics. I have also given numerous presentations in these areas."}, {"tag": "p", "text": "If you would like for me to help your company with numerical computing projects, please let me know."}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Number crunching\n\nWhen I was in graduate school, I specialized in differential equations, studying theory and numerical solution techniques. Along the way I did some consulting in numerical analysis.\n\nAfter I finished my Ph.D at University of Texas in 1992, I did a postdoc at Vanderbilt University. There I did research in differential equations and taught, among other things, numerical analysis. I also did some work in finite element error estimation for the U. S. Army Corp of Engineers.\n\nI then left academia in 1995 to work in the private sector where I did digital signal processing (DSP) and software development.\n\nIn 2000 I came to MD Anderson Cancer Center where I worked in statistical computing, especially Bayesian statistics. There I worked on high-dimensional integration, special function evaluation, simulation, etc. I have also applied these same skills as a consultant for businesses in a variety of industries.\n\nI have developed scientific software for Windows and Linux using a variety of programming languages: C++, C#, Python, Mathematica, and R. I have written journal articles and technical reports on applied mathematics, computation, and statistics. I have also given numerous presentations in these areas.\n\nIf you would like for me to help your company with numerical computing projects, please let me know.\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "poisson_approx_to_binomial", "canonical_url": "https://www.johndcook.com/blog/poisson_approx_to_binomial/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/poisson_approx_to_binomial.html", "title": "Error in the Poisson approximation to the binomial distribution", "heading": "Error in the Poisson approximation to the binomial distribution", "description": "Looking at the error in the Poisson approximation to the binomial distribution", "summary": "Textbooks often present the normal approximation to the binomial, the normal approximation to the Poisson, and the Poisson approximation to the binomial. Is this last approximation redundant? Are the binomial and the Poisson close because they're both close to the normal, or are they closer to each other than either is to the normal?", "word_count": 445, "blocks": [{"tag": "h1", "text": "Error in the Poisson approximation to the binomial distribution"}, {"tag": "p", "text": "Textbooks often present the normal approximation to the binomial, the normal approximation to the Poisson, and the Poisson approximation to the binomial. Is this last approximation redundant? Are the binomial and the Poisson close because they're both close to the normal, or are they closer to each other than either is to the normal?"}, {"tag": "p", "text": "The normal approximation to the binomial works best when n is large. That is, the distribution of a binomial(n, p) random variable is close to the distribution of a normal random variable with mean np and variance np(1-p) when np is \"large.\" (See these notes for more detail on what \"large\" means.)"}, {"tag": "p", "text": "The normal approximation to the Poisson works best when λ is large. That is, the distribution of a Poisson(λ) random variable is close to that of a normal random variable with mean λ and variance λ. (See these notes for details.)"}, {"tag": "p", "text": "The direct approximation of the binomial by the Poisson says that a binomial(n,p) random variable has approximately the same distribution as a Poisson(np) random variable when np is large. This is not surprising because when np is large, both the binomial and Poisson distributions are well approximated by a normal distribution. Two things close to the same thing are close to each other. (Well, not quite the same thing. See note below.) But conceivably the Poisson and binomial distributions could be even closer to each other than they are to their normal approximations. However, it looks like this is not the case."}, {"tag": "p", "text": "The following graph shows the PMF of a Poisson(10) distribution minus the PMF of a binomial(20,0.2) distribution."}, {"tag": "p", "text": "The graph below shows the difference between the PMF of a binomial(20,0.2) distribution and its normal approximation on the same vertical scale as the graph above."}, {"tag": "p", "text": "Here's the normal approximation to the Poisson(10) PMF."}, {"tag": "p", "text": "So at least in this example, binomial distribution is quite a bit closer to its normal approximation than the Poisson is to its normal approximation."}, {"tag": "p", "text": "NB: the normal approximations to the binomial(n, p) and a Poisson(np) distributions are not quite the same. Both normal approximations have mean np, but the former has variance np(1-p) while the latter has variance np."}, {"tag": "p", "text": "From the triangle inequality, we have"}, {"tag": "p", "text": "|Poisson - binomial| ≤ |Poisson - Poisson approx| + |Poisson approx - binomial approx| + |binomial approx - binomial|."}, {"tag": "p", "text": "In this example, the largest term on the right is the difference between the normal approximations to the binomial and the Poisson. In fact,"}, {"tag": "p", "text": "|Poisson - binomial| ≈ |Poisson approx - binomial approx|."}, {"tag": "h2", "text": "Other approximations"}, {"tag": "p", "text": "See also notes on the normal approximation to the beta, binomial, gamma, Poisson, and student-t distributions."}], "content": "Error in the Poisson approximation to the binomial distribution\n\nTextbooks often present the normal approximation to the binomial, the normal approximation to the Poisson, and the Poisson approximation to the binomial. Is this last approximation redundant? Are the binomial and the Poisson close because they're both close to the normal, or are they closer to each other than either is to the normal?\n\nThe normal approximation to the binomial works best when n is large. That is, the distribution of a binomial(n, p) random variable is close to the distribution of a normal random variable with mean np and variance np(1-p) when np is \"large.\" (See these notes for more detail on what \"large\" means.)\n\nThe normal approximation to the Poisson works best when λ is large. That is, the distribution of a Poisson(λ) random variable is close to that of a normal random variable with mean λ and variance λ. (See these notes for details.)\n\nThe direct approximation of the binomial by the Poisson says that a binomial(n,p) random variable has approximately the same distribution as a Poisson(np) random variable when np is large. This is not surprising because when np is large, both the binomial and Poisson distributions are well approximated by a normal distribution. Two things close to the same thing are close to each other. (Well, not quite the same thing. See note below.) But conceivably the Poisson and binomial distributions could be even closer to each other than they are to their normal approximations. However, it looks like this is not the case.\n\nThe following graph shows the PMF of a Poisson(10) distribution minus the PMF of a binomial(20,0.2) distribution.\n\nThe graph below shows the difference between the PMF of a binomial(20,0.2) distribution and its normal approximation on the same vertical scale as the graph above.\n\nHere's the normal approximation to the Poisson(10) PMF.\n\nSo at least in this example, binomial distribution is quite a bit closer to its normal approximation than the Poisson is to its normal approximation.\n\nNB: the normal approximations to the binomial(n, p) and a Poisson(np) distributions are not quite the same. Both normal approximations have mean np, but the former has variance np(1-p) while the latter has variance np.\n\nFrom the triangle inequality, we have\n\n|Poisson - binomial| ≤ |Poisson - Poisson approx| + |Poisson approx - binomial approx| + |binomial approx - binomial|.\n\nIn this example, the largest term on the right is the difference between the normal approximations to the binomial and the Poisson. In fact,\n\n|Poisson - binomial| ≈ |Poisson approx - binomial approx|.\n\nOther approximations\n\nSee also notes on the normal approximation to the beta, binomial, gamma, Poisson, and student-t distributions."}
{"slug": "powershell_gotchas", "canonical_url": "https://www.johndcook.com/blog/powershell_gotchas/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/powershell_gotchas.html", "title": "PowerShell gotchas", "heading": "PowerShell gotchas", "description": "Most surprizing/error-prone features of PowerShell for someone just starting out.", "summary": "Here's my list of top gotchas in PowerShell and an explanation for why PowerShell made the design decisions they did. PowerShell is both a shell and a scripting language.", "word_count": 692, "blocks": [{"tag": "h1", "text": "PowerShell gotchas"}, {"tag": "p", "text": "Here's my list of top gotchas in PowerShell and an explanation for why PowerShell made the design decisions they did. PowerShell is both a shell and a scripting language."}, {"tag": "h2", "text": "5. PowerShell will not run scripts by default."}, {"tag": "p", "text": "This was done for security reasons. The default execution policy in PowerShell prohibits all scripts from running. Before you can run scripts, you have to call Set-ExecutionPolicy with one of the following arguments, in order of increasing paranoia: Unrestricted, RemoteSigned, AllSigned, Restricted (default). If you set your policy to RemoteSigned, you don't have to digitally sign scripts you write, but scripts that you download will need to be signed."}, {"tag": "p", "text": "You only have to call Set-ExecutionPolicy once. Your setting will be saved for future PowerShell sessions."}, {"tag": "h2", "text": "4. PowerShell requires .\\ to run a script in the current directory."}, {"tag": "p", "text": "This was also done for security. Your current working directory is not in your path by default. So to run a script foo.ps1 in your current directory, you have to type .\\foo.ps1. This comes as no surprise for Unix users who often have such a restriction in their shells but seems odd to Windows users accustomed to cmd.exe."}, {"tag": "p", "text": "The motivation behind requiring an explicit path to scripts, even scripts in your current directory, is to prevent someone tricking you into running a script with the same name as the one you think you're running but in a different location."}, {"tag": "h2", "text": "3. PowerShell uses -eq, -gt, etc. for comparison operators."}, {"tag": "p", "text": "PowerShell is both a shell and a scripting language, but first of all it is a shell. Those of us who tend to see it more as a scripting language forget that and are surprised when PowerShell follows shell conventions rather than programming language conventions."}, {"tag": "p", "text": "Firmly established shell convention dictates that > and < are redirection operators, not a comparison operators. There was great uproar when the PowerShell development team floated the idea of using different redirection operators. So PowerShell uses -gt and -lt instead of > and <. And because PowerShell emphasizes consistency, it uses analogous syntax for other comparison operators. So, for example, it uses -eq and -ne rather than == and != to test for whether two objects are equal or not equal."}, {"tag": "h2", "text": "2. PowerShell uses backquote as the escape character."}, {"tag": "p", "text": "This means, for example, that a newline character is denoted `n rather than \\n and a tab is denoted `t rather than \\t. As a mnemonic, think of the backquote as a small backslash."}, {"tag": "p", "text": "PowerShell is a Windows shell, and Windows allows backslashes as path separators. Windows also allows forward slashes as path separators, and the PowerShell designers could have dictated that users restrict themselves to forward slashes so backslash could be reserved for the escape character, but that would have caused great frustration for many Windows users."}, {"tag": "h2", "text": "1. PowerShell separates function arguments with spaces, not commas."}, {"tag": "p", "text": "If you call a PowerShell function foo with a comma-separated list, such as foo(a, b, c), you are sending the function one argument: the three-element list a, b, c. If you want to call foo with three arguments, the correct syntax is foo a b c."}, {"tag": "p", "text": "This is probably the biggest gotcha in PowerShell. I've fallen into this trap, told other people about it, then fallen into the trap again. The worst thing about it is that an erroneous function call may not throw an exception but just work strangely."}, {"tag": "p", "text": "When the interpreter sees foo (a, b, c), it says \"I'm expecting three arguments to foo, but I only see one: a list contained in superfluous parentheses. That must be the first argument, so I will set the other two arguments to the default value null.\""}, {"tag": "p", "text": "What were the PowerShell designers thinking? PowerShell is a shell. The primary usage scenario is a system administrator typing at the command line. Tradition and convenience dictate that shell command arguments are separated by spaces and are not delimited by parentheses and commas."}, {"tag": "p", "text": "There is an exception to the function call rule: When you call methods on .NET objects from PowerShell, use parentheses and commas just as you would in C#."}, {"tag": "p", "text": "Other PowerShell resources on this site:"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "PowerShell gotchas\n\nHere's my list of top gotchas in PowerShell and an explanation for why PowerShell made the design decisions they did. PowerShell is both a shell and a scripting language.\n\n5. PowerShell will not run scripts by default.\n\nThis was done for security reasons. The default execution policy in PowerShell prohibits all scripts from running. Before you can run scripts, you have to call Set-ExecutionPolicy with one of the following arguments, in order of increasing paranoia: Unrestricted, RemoteSigned, AllSigned, Restricted (default). If you set your policy to RemoteSigned, you don't have to digitally sign scripts you write, but scripts that you download will need to be signed.\n\nYou only have to call Set-ExecutionPolicy once. Your setting will be saved for future PowerShell sessions.\n\n4. PowerShell requires .\\ to run a script in the current directory.\n\nThis was also done for security. Your current working directory is not in your path by default. So to run a script foo.ps1 in your current directory, you have to type .\\foo.ps1. This comes as no surprise for Unix users who often have such a restriction in their shells but seems odd to Windows users accustomed to cmd.exe.\n\nThe motivation behind requiring an explicit path to scripts, even scripts in your current directory, is to prevent someone tricking you into running a script with the same name as the one you think you're running but in a different location.\n\n3. PowerShell uses -eq, -gt, etc. for comparison operators.\n\nPowerShell is both a shell and a scripting language, but first of all it is a shell. Those of us who tend to see it more as a scripting language forget that and are surprised when PowerShell follows shell conventions rather than programming language conventions.\n\nFirmly established shell convention dictates that > and < are redirection operators, not a comparison operators. There was great uproar when the PowerShell development team floated the idea of using different redirection operators. So PowerShell uses -gt and -lt instead of > and <. And because PowerShell emphasizes consistency, it uses analogous syntax for other comparison operators. So, for example, it uses -eq and -ne rather than == and != to test for whether two objects are equal or not equal.\n\n2. PowerShell uses backquote as the escape character.\n\nThis means, for example, that a newline character is denoted `n rather than \\n and a tab is denoted `t rather than \\t. As a mnemonic, think of the backquote as a small backslash.\n\nPowerShell is a Windows shell, and Windows allows backslashes as path separators. Windows also allows forward slashes as path separators, and the PowerShell designers could have dictated that users restrict themselves to forward slashes so backslash could be reserved for the escape character, but that would have caused great frustration for many Windows users.\n\n1. PowerShell separates function arguments with spaces, not commas.\n\nIf you call a PowerShell function foo with a comma-separated list, such as foo(a, b, c), you are sending the function one argument: the three-element list a, b, c. If you want to call foo with three arguments, the correct syntax is foo a b c.\n\nThis is probably the biggest gotcha in PowerShell. I've fallen into this trap, told other people about it, then fallen into the trap again. The worst thing about it is that an erroneous function call may not throw an exception but just work strangely.\n\nWhen the interpreter sees foo (a, b, c), it says \"I'm expecting three arguments to foo, but I only see one: a list contained in superfluous parentheses. That must be the first argument, so I will set the other two arguments to the default value null.\"\n\nWhat were the PowerShell designers thinking? PowerShell is a shell. The primary usage scenario is a system administrator typing at the command line. Tradition and convenience dictate that shell command arguments are separated by spaces and are not delimited by parentheses and commas.\n\nThere is an exception to the function call rule: When you call methods on .NET objects from PowerShell, use parentheses and commas just as you would in C#.\n\nOther PowerShell resources on this site:\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "presentations", "canonical_url": "https://www.johndcook.com/blog/presentations/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/presentations.html", "title": "Recent Presentations", "heading": "Presentations", "description": "Presentations by John D. Cook.  Talks, lectures, short courses.", "summary": "Bayesian Statistics as a way to Integrate Intuition and Data, Keen CON 2014, San Francisco, September 11, 2014.", "word_count": 472, "blocks": [{"tag": "h1", "text": "Presentations"}, {"tag": "p", "text": "Bayesian Statistics as a way to Integrate Intuition and Data, Keen CON 2014, San Francisco, September 11, 2014."}, {"tag": "blockquote", "text": "I was ecstatic to have John speak at the very first KeenCon, and his talk was elegant, geeky, and very informative. We learned a lot from it! — Kyle Wild, Keen IO"}, {"tag": "p", "text": "Developer Writing Tips & Tricks panel discussion, AirConf, August 25, 2014."}, {"tag": "p", "text": "Life as a math consultant. University of Houston SIAM student chapter. Houston, Texas. April 18, 2014."}, {"tag": "p", "text": "Erasure coding costs and benefits. Snow Unix Event, Geldermalsen, The Netherlands. March 27–28, 2014."}, {"tag": "blockquote", "text": "John's talk was both precise and practical, one of the most theoretical talks of SUE2014 turned out to be one of the most applicable ones as well. — Joost Helberg, Snow Unix Specialists"}, {"tag": "p", "text": "The infinite as a guide to the big. Amazon.com, Seattle, Washington. January 23, 2014."}, {"tag": "p", "text": "The most common error in applying probability. January 16, 2014. Houston area law firm."}, {"tag": "p", "text": "Robust priors in Bayesian inference. RTI Center for Complex Data Analysis, Research Triangle Park, North Carolina. April 3, 2013."}, {"tag": "p", "text": "How to hire and work with an analyst. Research Triangle Analysis. Durham, North Carolina. April 2, 2013."}, {"tag": "p", "text": "YOW! 2012, Australia Software Developer Conference. Speaking in Melbourne November 29, Brisbane December 3."}, {"tag": "p", "text": "GOTO Conference, Aarhus, Denmark. R language: The good, the bad, and the ugly, October 2, 2012. The mysteries of floating point, October 3, 2012"}, {"tag": "p", "text": "University of Puerto Rico, Rio Piedras, May 2, 2012. Varieties of randomization in clinical trials."}, {"tag": "p", "text": "Microsoft Research, Redmond, Washington. Lang.NEXT Conference, April 4, 2012. Why and how people use R. video"}, {"tag": "p", "text": "University of Houston chapter of SIAM (Society for Industrial and Applied Mathematics), Houston, Texas. March 2, 2012. What kind of math is used in biostatistics?"}, {"tag": "p", "text": "Vanderbilt University Department of Biostatistics, Nashville, Tennessee. January 19, 2011. A whirlwind tour of some Bayesian clinical trial methods and software."}, {"tag": "p", "text": "HACASA (Houston Area Chapter of the American Statistical Association), Houston, Texas. September 14, 2010. Robust priors in Bayesian statistics."}, {"tag": "p", "text": "Fifth Transdisciplinary Research Conference, Puerto Rico Louis Stokes Alliance for Minority Participation (PR-LSAMP). San Juan, May 13-14, 2010."}, {"tag": "p", "text": "Software for designing and conducting adaptive clinical trials, May 25–27, 2009. UPR_MDACC Clinical Trials Workshop. University of Puerto Rico."}, {"tag": "p", "text": "Clinical Trial Monitoring With Bayesian Hypothesis Testing presented at the Joint Statistical Meeting in Denver August 6, 2008."}, {"tag": "p", "text": "“Challenges in developing and supporting clinical trial conduct software” presented to the Houston chapter of HIMSS (Healthcare Information and Management Systems Society) March 28, 2008."}, {"tag": "p", "text": "“Software and practical considerations for designing and conducting innovative clinical trials at MDACC” presented at the Joint Statistical Meeting in Salt Lake City, Utah July 31, 2007."}, {"tag": "p", "text": "Short course “Bayesian Clinical Trial Design: Analysis and Software” given March 28–30, 2007 at University of Puerto Rico."}, {"tag": "p", "text": "Short course “Bayesian Clinical Trial Designs” given along with Peter Thall, June 12–14, 2006 at L’Institut Bergonié in Bordeaux, France."}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Presentations\n\nBayesian Statistics as a way to Integrate Intuition and Data, Keen CON 2014, San Francisco, September 11, 2014.\n\nI was ecstatic to have John speak at the very first KeenCon, and his talk was elegant, geeky, and very informative. We learned a lot from it! — Kyle Wild, Keen IO\n\nDeveloper Writing Tips & Tricks panel discussion, AirConf, August 25, 2014.\n\nLife as a math consultant. University of Houston SIAM student chapter. Houston, Texas. April 18, 2014.\n\nErasure coding costs and benefits. Snow Unix Event, Geldermalsen, The Netherlands. March 27–28, 2014.\n\nJohn's talk was both precise and practical, one of the most theoretical talks of SUE2014 turned out to be one of the most applicable ones as well. — Joost Helberg, Snow Unix Specialists\n\nThe infinite as a guide to the big. Amazon.com, Seattle, Washington. January 23, 2014.\n\nThe most common error in applying probability. January 16, 2014. Houston area law firm.\n\nRobust priors in Bayesian inference. RTI Center for Complex Data Analysis, Research Triangle Park, North Carolina. April 3, 2013.\n\nHow to hire and work with an analyst. Research Triangle Analysis. Durham, North Carolina. April 2, 2013.\n\nYOW! 2012, Australia Software Developer Conference. Speaking in Melbourne November 29, Brisbane December 3.\n\nGOTO Conference, Aarhus, Denmark. R language: The good, the bad, and the ugly, October 2, 2012. The mysteries of floating point, October 3, 2012\n\nUniversity of Puerto Rico, Rio Piedras, May 2, 2012. Varieties of randomization in clinical trials.\n\nMicrosoft Research, Redmond, Washington. Lang.NEXT Conference, April 4, 2012. Why and how people use R. video\n\nUniversity of Houston chapter of SIAM (Society for Industrial and Applied Mathematics), Houston, Texas. March 2, 2012. What kind of math is used in biostatistics?\n\nVanderbilt University Department of Biostatistics, Nashville, Tennessee. January 19, 2011. A whirlwind tour of some Bayesian clinical trial methods and software.\n\nHACASA (Houston Area Chapter of the American Statistical Association), Houston, Texas. September 14, 2010. Robust priors in Bayesian statistics.\n\nFifth Transdisciplinary Research Conference, Puerto Rico Louis Stokes Alliance for Minority Participation (PR-LSAMP). San Juan, May 13-14, 2010.\n\nSoftware for designing and conducting adaptive clinical trials, May 25–27, 2009. UPR_MDACC Clinical Trials Workshop. University of Puerto Rico.\n\nClinical Trial Monitoring With Bayesian Hypothesis Testing presented at the Joint Statistical Meeting in Denver August 6, 2008.\n\n“Challenges in developing and supporting clinical trial conduct software” presented to the Houston chapter of HIMSS (Healthcare Information and Management Systems Society) March 28, 2008.\n\n“Software and practical considerations for designing and conducting innovative clinical trials at MDACC” presented at the Joint Statistical Meeting in Salt Lake City, Utah July 31, 2007.\n\nShort course “Bayesian Clinical Trial Design: Analysis and Software” given March 28–30, 2007 at University of Puerto Rico.\n\nShort course “Bayesian Clinical Trial Designs” given along with Peter Thall, June 12–14, 2006 at L’Institut Bergonié in Bordeaux, France.\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "python_cauchy_rng", "canonical_url": "https://www.johndcook.com/blog/python_cauchy_rng/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/python_cauchy_rng.html", "title": "Python Cauchy random number generator", "heading": "Cauchy random number generator", "description": "Stand-alone Python function for generating random samples from a Cauchy distribution.", "summary": "This code is in the public domain. Do whatever you want to with it, no strings attached.", "word_count": 89, "blocks": [{"tag": "h1", "text": "Cauchy random number generator"}, {"tag": "pre", "text": "import math \nimport random\n\ndef cauchy(location, scale):\n\n    # Start with a uniform random sample from the open interval (0, 1).\n    # But random() returns a sample from the half-open interval [0, 1).\n    # In the unlikely event that random() returns 0, try again.\n    \n    p = 0.0\n    while p == 0.0:\n        p = random.random()\n        \n    return location + scale*math.tan(math.pi*(p - 0.5))"}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want to with it, no strings attached."}, {"tag": "p", "text": "See also: Stand alone code Python random number generation"}], "content": "Cauchy random number generator\n\nimport math \nimport random\n\ndef cauchy(location, scale):\n\n    # Start with a uniform random sample from the open interval (0, 1).\n    # But random() returns a sample from the half-open interval [0, 1).\n    # In the unlikely event that random() returns 0, try again.\n    \n    p = 0.0\n    while p == 0.0:\n        p = random.random()\n        \n    return location + scale*math.tan(math.pi*(p - 0.5))\n\nThis code is in the public domain. Do whatever you want to with it, no strings attached.\n\nSee also: Stand alone code Python random number generation"}
{"slug": "python_erf", "canonical_url": "https://www.johndcook.com/blog/python_erf/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/python_erf.html", "title": "Stand-alone Python implementation of the error function (erf)", "heading": "Stand-alone Python error function erf(x)", "description": "Stand-alone Python code for computing the error function erf(x).", "summary": "The following code first appeared on my blog post Stand-alone error function erf. See that post for documentation. See also Relating erf and Φ.", "word_count": 131, "blocks": [{"tag": "h1", "text": "Stand-alone Python error function erf(x)"}, {"tag": "p", "text": "The following code first appeared on my blog post Stand-alone error function erf. See that post for documentation. See also Relating erf and Φ."}, {"tag": "pre", "text": "import math\n\ndef erf(x):\n    # constants\n    a1 =  0.254829592\n    a2 = -0.284496736\n    a3 =  1.421413741\n    a4 = -1.453152027\n    a5 =  1.061405429\n    p  =  0.3275911\n\n    # Save the sign of x\n    sign = 1\n    if x < 0:\n        sign = -1\n    x = abs(x)\n\n    # A&S formula 7.1.26\n    t = 1.0/(1.0 + p*x)\n    y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*math.exp(-x*x)\n\n    return sign*y"}, {"tag": "p", "text": "A&S refers to Handbook of Mathematical Functions by Abramowitz and Stegun."}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want to with it, no strings attached."}, {"tag": "p", "text": "Other versions: C++, C#"}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Stand-alone Python error function erf(x)\n\nThe following code first appeared on my blog post Stand-alone error function erf. See that post for documentation. See also Relating erf and Φ.\n\nimport math\n\ndef erf(x):\n    # constants\n    a1 =  0.254829592\n    a2 = -0.284496736\n    a3 =  1.421413741\n    a4 = -1.453152027\n    a5 =  1.061405429\n    p  =  0.3275911\n\n    # Save the sign of x\n    sign = 1\n    if x < 0:\n        sign = -1\n    x = abs(x)\n\n    # A&S formula 7.1.26\n    t = 1.0/(1.0 + p*x)\n    y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*math.exp(-x*x)\n\n    return sign*y\n\nA&S refers to Handbook of Mathematical Functions by Abramowitz and Stegun.\n\nThis code is in the public domain. Do whatever you want to with it, no strings attached.\n\nOther versions: C++, C#\n\nStand-alone numerical code"}
{"slug": "python_expm1", "canonical_url": "https://www.johndcook.com/blog/python_expm1/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/python_expm1.html", "title": "Stand-alone Python implementation of expm1", "heading": "Stand-alone Python code for exp(x) - 1", "description": "Stand-alone Python code for computing exp(x) - 1, a.k.a. expm1(x), accurate even for small values of x where direct computation is not accurate.", "summary": "See the corresponding page for C++ for an explanation of the problem and the solution.", "word_count": 47, "blocks": [{"tag": "h1", "text": "Stand-alone Python code for exp(x) - 1"}, {"tag": "p", "text": "See the corresponding page for C++ for an explanation of the problem and the solution."}, {"tag": "p", "text": "The Python code is trivial:"}, {"tag": "pre", "text": "import math\n\ndef expm1(x):\n    if abs(x) < 1e-5:\n        return x + 0.5*x*x\n    else:\n        return math.exp(x) - 1.0"}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Stand-alone Python code for exp(x) - 1\n\nSee the corresponding page for C++ for an explanation of the problem and the solution.\n\nThe Python code is trivial:\n\nimport math\n\ndef expm1(x):\n    if abs(x) < 1e-5:\n        return x + 0.5*x*x\n    else:\n        return math.exp(x) - 1.0\n\nStand-alone numerical code"}
{"slug": "python_log_one_plus_x", "canonical_url": "https://www.johndcook.com/blog/python_log_one_plus_x/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/python_log_one_plus_x.html", "title": "Stand-alone Python implementation of log(1+x)", "heading": "Stand-alone Python code for log(1+x)", "description": "Stand-alone Python code for computing log1p(x) = log(1+x), accurate even for small values of x where direct computation is not accurate.", "summary": "If p is very small, directly computing log(1+p) can be inaccurate. See the C++ version of this page for details.", "word_count": 136, "blocks": [{"tag": "h1", "text": "Stand-alone Python code for log(1+x)"}, {"tag": "p", "text": "If p is very small, directly computing log(1+p) can be inaccurate. See the C++ version of this page for details."}, {"tag": "pre", "text": "import math\n\n# compute log(1+x) without loss of precision for small values of x\ndef log_one_plus_x(x):\n    if x <= -1.0:\n        raise FloatingPointError, \"argument must be > -1\"\n\n    if abs(x) > 1e-4:\n        # x is large enough that the obvious evaluation is OK\n        return math.log(1.0 + x)\n    else:\n        # Use Taylor approx. \n        # log(1 + x) = x - x^2/2 with error roughly x^3/3\n        # Since |x| < 10^-4, |x|^3 < 10^-12, \n        # and the relative error is less than 10^-8\n        return (-0.5*x + 1.0)*x"}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want to with it, no strings attached."}, {"tag": "p", "text": "Other versions of this code: C++, C#"}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Stand-alone Python code for log(1+x)\n\nIf p is very small, directly computing log(1+p) can be inaccurate. See the C++ version of this page for details.\n\nimport math\n\n# compute log(1+x) without loss of precision for small values of x\ndef log_one_plus_x(x):\n    if x <= -1.0:\n        raise FloatingPointError, \"argument must be > -1\"\n\n    if abs(x) > 1e-4:\n        # x is large enough that the obvious evaluation is OK\n        return math.log(1.0 + x)\n    else:\n        # Use Taylor approx. \n        # log(1 + x) = x - x^2/2 with error roughly x^3/3\n        # Since |x| < 10^-4, |x|^3 < 10^-12, \n        # and the relative error is less than 10^-8\n        return (-0.5*x + 1.0)*x\n\nThis code is in the public domain. Do whatever you want to with it, no strings attached.\n\nOther versions of this code: C++, C#\n\nStand-alone numerical code"}
{"slug": "python_longitude_latitude", "canonical_url": "https://www.johndcook.com/blog/python_longitude_latitude/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/python_longitude_latitude.html", "title": "Python code for computing the distance between two (longitude, latitude) pairs", "heading": "Computing the distance between two locations on Earth from coordinates", "description": "Stand-alone Python code for computing the distance between two locations on the globe based on longitude and latitude.", "summary": "The following code returns the distance between to locations based on each point's longitude and latitude. The distance returned is relative to Earth's radius. To get the distance in miles, multiply by 3960. To get the distance in kilometers, multiply by 6373.", "word_count": 302, "blocks": [{"tag": "h1", "text": "Computing the distance between two locations on Earth from coordinates"}, {"tag": "p", "text": "The following code returns the distance between to locations based on each point's longitude and latitude. The distance returned is relative to Earth's radius. To get the distance in miles, multiply by 3960. To get the distance in kilometers, multiply by 6373."}, {"tag": "p", "text": "Latitude is measured in degrees north of the equator; southern locations have negative latitude. Similarly, longitude is measured in degrees east of the Prime Meridian. A location 10° west of the Prime Meridian, for example, could be expressed as either 350° east or as -10° east."}, {"tag": "pre", "text": "import math\n\ndef distance_on_unit_sphere(lat1, long1, lat2, long2):\n\n    # Convert latitude and longitude to \n    # spherical coordinates in radians.\n    degrees_to_radians = math.pi/180.0\n\t\n    # phi = 90 - latitude\n    phi1 = (90.0 - lat1)*degrees_to_radians\n    phi2 = (90.0 - lat2)*degrees_to_radians\n\t\n    # theta = longitude\n    theta1 = long1*degrees_to_radians\n    theta2 = long2*degrees_to_radians\n\t\n    # Compute spherical distance from spherical coordinates.\n\t\n    # For two locations in spherical coordinates \n    # (1, theta, phi) and (1, theta, phi)\n    # cosine( arc length ) = \n    #    sin phi sin phi' cos(theta-theta') + cos phi cos phi'\n    # distance = rho * arc length\n    \n    cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2) + \n           math.cos(phi1)*math.cos(phi2))\n    arc = math.acos( cos )\n\n    # Remember to multiply arc by the radius of the earth \n    # in your favorite set of units to get length.\n    return arc"}, {"tag": "p", "text": "The code above assumes the earth is perfectly spherical. For a discussion of how accurate this assumption is, see my blog post on the shape of the Earth."}, {"tag": "p", "text": "The algorithm used to calculate distances is described in detail here."}, {"tag": "p", "text": "A web page to calculate the distance between to cities based on longitude and latitude is available here."}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want with it, no strings attached."}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Computing the distance between two locations on Earth from coordinates\n\nThe following code returns the distance between to locations based on each point's longitude and latitude. The distance returned is relative to Earth's radius. To get the distance in miles, multiply by 3960. To get the distance in kilometers, multiply by 6373.\n\nLatitude is measured in degrees north of the equator; southern locations have negative latitude. Similarly, longitude is measured in degrees east of the Prime Meridian. A location 10° west of the Prime Meridian, for example, could be expressed as either 350° east or as -10° east.\n\nimport math\n\ndef distance_on_unit_sphere(lat1, long1, lat2, long2):\n\n    # Convert latitude and longitude to \n    # spherical coordinates in radians.\n    degrees_to_radians = math.pi/180.0\n\t\n    # phi = 90 - latitude\n    phi1 = (90.0 - lat1)*degrees_to_radians\n    phi2 = (90.0 - lat2)*degrees_to_radians\n\t\n    # theta = longitude\n    theta1 = long1*degrees_to_radians\n    theta2 = long2*degrees_to_radians\n\t\n    # Compute spherical distance from spherical coordinates.\n\t\n    # For two locations in spherical coordinates \n    # (1, theta, phi) and (1, theta, phi)\n    # cosine( arc length ) = \n    #    sin phi sin phi' cos(theta-theta') + cos phi cos phi'\n    # distance = rho * arc length\n    \n    cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2) + \n           math.cos(phi1)*math.cos(phi2))\n    arc = math.acos( cos )\n\n    # Remember to multiply arc by the radius of the earth \n    # in your favorite set of units to get length.\n    return arc\n\nThe code above assumes the earth is perfectly spherical. For a discussion of how accurate this assumption is, see my blog post on the shape of the Earth.\n\nThe algorithm used to calculate distances is described in detail here.\n\nA web page to calculate the distance between to cities based on longitude and latitude is available here.\n\nThis code is in the public domain. Do whatever you want with it, no strings attached.\n\nStand-alone numerical code"}
{"slug": "python_phi_inverse", "canonical_url": "https://www.johndcook.com/blog/python_phi_inverse/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/python_phi_inverse.html", "title": "Stand-alone Python implementation of Phi inverse", "heading": "Stand-alone Python implementation of Phi inverse", "description": "Stand-alone Python code for computing the inverse of Phi, the CDF of a standard normal (Gaussian) random variable.", "summary": "The following code first appeared as A literate program to compute the inverse of the normal CDF. See that page for a detailed explanation of the algorithm.", "word_count": 293, "blocks": [{"tag": "h1", "text": "Stand-alone Python implementation of Phi inverse"}, {"tag": "p", "text": "The following code first appeared as A literate program to compute the inverse of the normal CDF. See that page for a detailed explanation of the algorithm."}, {"tag": "pre", "text": "import math\n\n\ndef rational_approximation(t):\n\n    # Abramowitz and Stegun formula 26.2.23.\n    # The absolute value of the error should be less than 4.5 e-4.\n    c = [2.515517, 0.802853, 0.010328]\n    d = [1.432788, 0.189269, 0.001308]\n    numerator = (c[2]*t + c[1])*t + c[0]\n    denominator = ((d[2]*t + d[1])*t + d[0])*t + 1.0\n    return t - numerator / denominator\n\n\ndef normal_CDF_inverse(p):\n\n    assert p > 0.0 and p < 1\n\n    # See article above for explanation of this section.\n    if p < 0.5:\n        # F^-1(p) = - G^-1(p)\n        return -rational_approximation( math.sqrt(-2.0*math.log(p)) )\n    else:\n        # F^-1(p) = G^-1(1-p)\n        return rational_approximation( math.sqrt(-2.0*math.log(1.0-p)) )\n\ndef demo():\n\n    print \"\\nShow that the NormalCDFInverse function is accurate at\"\n    print \"0.05, 0.15, 0.25, ..., 0.95 and at a few extreme values.\\n\\n\"\n\n    p = [\n        0.0000001,\n        0.00001,\n        0.001,\n        0.05,\n        0.15,\n        0.25,\n        0.35,\n        0.45,\n        0.55,\n        0.65,\n        0.75,\n        0.85,\n        0.95,\n        0.999,\n        0.99999,\n        0.9999999\n    ]\n\n    # Exact values computed by Mathematica.\n    exact = [\n        -5.199337582187471,\n        -4.264890793922602,\n        -3.090232306167813,\n        -1.6448536269514729,\n        -1.0364333894937896,\n        -0.6744897501960817,\n        -0.38532046640756773,\n        -0.12566134685507402,\n         0.12566134685507402,\n         0.38532046640756773,\n         0.6744897501960817,\n         1.0364333894937896,\n         1.6448536269514729,\n         3.090232306167813,\n         4.264890793922602,\n         5.199337582187471\n    ]\n\n    maxerror = 0.0\n    num_values = len(p)\n    print \"p, exact CDF inverse, computed CDF inverse, diff\\n\\n\";\n    \n    for i in range(num_values):\n        computed = normal_CDF_inverse(p[i])\n        error = exact[i] - computed\n        print p[i], \",\", exact[i], \",\", computed, \",\", error\n        if (abs(error) > maxerror):\n            maxerror = abs(error)\n\n    print \"\\nMaximum error:\" , maxerror , \"\\n\"\n\nif __name__ == \"__main__\":\n    demo()"}, {"tag": "p", "text": "The code is based on an algorithm given in Handbook of Mathematical Functions by Abramowitz and Stegun."}, {"tag": "p", "text": "This code is in the public domain. Do whatever you want to with it, no strings attached."}, {"tag": "p", "text": "Other versions: C++, C#"}, {"tag": "p", "text": "Stand-alone numerical code"}], "content": "Stand-alone Python implementation of Phi inverse\n\nThe following code first appeared as A literate program to compute the inverse of the normal CDF. See that page for a detailed explanation of the algorithm.\n\nimport math\n\n\ndef rational_approximation(t):\n\n    # Abramowitz and Stegun formula 26.2.23.\n    # The absolute value of the error should be less than 4.5 e-4.\n    c = [2.515517, 0.802853, 0.010328]\n    d = [1.432788, 0.189269, 0.001308]\n    numerator = (c[2]*t + c[1])*t + c[0]\n    denominator = ((d[2]*t + d[1])*t + d[0])*t + 1.0\n    return t - numerator / denominator\n\n\ndef normal_CDF_inverse(p):\n\n    assert p > 0.0 and p < 1\n\n    # See article above for explanation of this section.\n    if p < 0.5:\n        # F^-1(p) = - G^-1(p)\n        return -rational_approximation( math.sqrt(-2.0*math.log(p)) )\n    else:\n        # F^-1(p) = G^-1(1-p)\n        return rational_approximation( math.sqrt(-2.0*math.log(1.0-p)) )\n\ndef demo():\n\n    print \"\\nShow that the NormalCDFInverse function is accurate at\"\n    print \"0.05, 0.15, 0.25, ..., 0.95 and at a few extreme values.\\n\\n\"\n\n    p = [\n        0.0000001,\n        0.00001,\n        0.001,\n        0.05,\n        0.15,\n        0.25,\n        0.35,\n        0.45,\n        0.55,\n        0.65,\n        0.75,\n        0.85,\n        0.95,\n        0.999,\n        0.99999,\n        0.9999999\n    ]\n\n    # Exact values computed by Mathematica.\n    exact = [\n        -5.199337582187471,\n        -4.264890793922602,\n        -3.090232306167813,\n        -1.6448536269514729,\n        -1.0364333894937896,\n        -0.6744897501960817,\n        -0.38532046640756773,\n        -0.12566134685507402,\n         0.12566134685507402,\n         0.38532046640756773,\n         0.6744897501960817,\n         1.0364333894937896,\n         1.6448536269514729,\n         3.090232306167813,\n         4.264890793922602,\n         5.199337582187471\n    ]\n\n    maxerror = 0.0\n    num_values = len(p)\n    print \"p, exact CDF inverse, computed CDF inverse, diff\\n\\n\";\n    \n    for i in range(num_values):\n        computed = normal_CDF_inverse(p[i])\n        error = exact[i] - computed\n        print p[i], \",\", exact[i], \",\", computed, \",\", error\n        if (abs(error) > maxerror):\n            maxerror = abs(error)\n\n    print \"\\nMaximum error:\" , maxerror , \"\\n\"\n\nif __name__ == \"__main__\":\n    demo()\n\nThe code is based on an algorithm given in Handbook of Mathematical Functions by Abramowitz and Stegun.\n\nThis code is in the public domain. Do whatever you want to with it, no strings attached.\n\nOther versions: C++, C#\n\nStand-alone numerical code"}
{"slug": "python_random_number_generation", "canonical_url": "https://www.johndcook.com/blog/python_random_number_generation/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/python_random_number_generation.html", "title": "Python random number generation", "heading": "Random number generation in Python", "description": "Python code for generating random samples from various probability distributions.", "summary": "The Python module random contains functions for generating random numbers according to several common distributions.", "word_count": 68, "blocks": [{"tag": "h1", "text": "Random number generation in Python"}, {"tag": "p", "text": "The Python module random contains functions for generating random numbers according to several common distributions."}, {"tag": "p", "text": "There are, however, a few common distributions not on the list above. Code is available here for the following distributions."}, {"tag": "li", "text": "Cauchy"}, {"tag": "li", "text": "Student-t"}, {"tag": "p", "text": "SciPy has a large collection of random number generation functions as methods on distribution objects. See Distributions in SciPy."}, {"tag": "p", "text": "See also: Distribution chart Stand-alone numerical code"}], "content": "Random number generation in Python\n\nThe Python module random contains functions for generating random numbers according to several common distributions.\n\nThere are, however, a few common distributions not on the list above. Code is available here for the following distributions.\n\nCauchy\n\nStudent-t\n\nSciPy has a large collection of random number generation functions as methods on distribution objects. See Distributions in SciPy.\n\nSee also: Distribution chart Stand-alone numerical code"}
{"slug": "python_regex", "canonical_url": "https://www.johndcook.com/blog/python_regex/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/python_regex.html", "title": "Regular expressions in Python", "heading": "Regular expressions in Python and Perl", "description": "Comparing regular expressions in Python and Perl. A quick start guide for Perl programmers to work with regular expressions in Python.", "summary": "Python supports essentially the same regular expression syntax as Perl, as far as the regular expressions themselves. However, the syntax for using regular expressions is substantially different.", "word_count": 442, "blocks": [{"tag": "h1", "text": "Regular expressions in Python and Perl"}, {"tag": "p", "text": "Python supports essentially the same regular expression syntax as Perl, as far as the regular expressions themselves. However, the syntax for using regular expressions is substantially different."}, {"tag": "p", "text": "Regular expression support is not available out of the box; you must import the re module."}, {"tag": "p", "text": "Regular expression patterns are contained in strings, in contrast to Perl's built-in // syntax. This means that some characters need to be escaped in order to be passed on to the regular expression engine. To be safe, always use raw strings (r'' or r\"\") to contain patterns."}, {"tag": "p", "text": "You might think that re.match() is the analog to Perl's m// match operator. It's not! The re.match() function matches regular expressions starting at the beginning of a string. It behaves as if every pattern has ^ prepended. The function re.search() behaves like Perl's m// and is probably what you want to use exclusively."}, {"tag": "p", "text": "The functions match and search return an object with a group method. The group method without any argument returns the entire match. The group method with a positive integer argument returns captured expressions: group(1) returns the first capture, group(2) returns the second, analogous to $1, $2, etc. in Perl. (If there are no matches, match and search return None and so you must check whether the match object is valid before calling methods on it.) The groups() method returns all matches as a tuple."}, {"tag": "p", "text": "Python doesn't have a global modifier like Perl's /g option. To find all matches to a pattern, use re.findall() rather than re.search(). The findall method returns a list of matches rather than a match object."}, {"tag": "p", "text": "To substitute for a pattern, analogous to Perl's s// operator, use re.sub(). Actually, re.sub() is analogous to s//g since it replaces all instances of a pattern by default. To change this behavior, you can specify the maximum number of instances to replace using the max parameter to re.sub(). Setting this parameter to 1 causes only the first instance to be substituted, as in Perl's s//."}, {"tag": "p", "text": "To make a regular expression case-insensitive, pass the argument re.I (or re.IGNORECASE) as the final argument to re.search()."}, {"tag": "p", "text": "The function re.sub() does not take flags such as re.I. So in order to make the regular expression match case-insensitive, one must modify the regular expression itself by adding (?i) to the beginning of the expression. (The modifier (?i) can go anywhere, but the regular expression will be most readable if the modifier goes at the beginning or possibly at the end.)"}, {"tag": "h2", "text": "Resources"}, {"tag": "p", "text": "Notes on using regular expressions in other languages: PowerShell, C++, R, Mathematica"}, {"tag": "p", "text": "Other Python articles: making an XML sitemap, languages easy to pick up"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Regular expressions in Python and Perl\n\nPython supports essentially the same regular expression syntax as Perl, as far as the regular expressions themselves. However, the syntax for using regular expressions is substantially different.\n\nRegular expression support is not available out of the box; you must import the re module.\n\nRegular expression patterns are contained in strings, in contrast to Perl's built-in // syntax. This means that some characters need to be escaped in order to be passed on to the regular expression engine. To be safe, always use raw strings (r'' or r\"\") to contain patterns.\n\nYou might think that re.match() is the analog to Perl's m// match operator. It's not! The re.match() function matches regular expressions starting at the beginning of a string. It behaves as if every pattern has ^ prepended. The function re.search() behaves like Perl's m// and is probably what you want to use exclusively.\n\nThe functions match and search return an object with a group method. The group method without any argument returns the entire match. The group method with a positive integer argument returns captured expressions: group(1) returns the first capture, group(2) returns the second, analogous to $1, $2, etc. in Perl. (If there are no matches, match and search return None and so you must check whether the match object is valid before calling methods on it.) The groups() method returns all matches as a tuple.\n\nPython doesn't have a global modifier like Perl's /g option. To find all matches to a pattern, use re.findall() rather than re.search(). The findall method returns a list of matches rather than a match object.\n\nTo substitute for a pattern, analogous to Perl's s// operator, use re.sub(). Actually, re.sub() is analogous to s//g since it replaces all instances of a pattern by default. To change this behavior, you can specify the maximum number of instances to replace using the max parameter to re.sub(). Setting this parameter to 1 causes only the first instance to be substituted, as in Perl's s//.\n\nTo make a regular expression case-insensitive, pass the argument re.I (or re.IGNORECASE) as the final argument to re.search().\n\nThe function re.sub() does not take flags such as re.I. So in order to make the regular expression match case-insensitive, one must modify the regular expression itself by adding (?i) to the beginning of the expression. (The modifier (?i) can go anywhere, but the regular expression will be most readable if the modifier goes at the beginning or possibly at the end.)\n\nResources\n\nNotes on using regular expressions in other languages: PowerShell, C++, R, Mathematica\n\nOther Python articles: making an XML sitemap, languages easy to pick up\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "quadratic_congruences", "canonical_url": "https://www.johndcook.com/blog/quadratic_congruences/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/quadratic_congruences.html", "title": "Solving quadratic congruences", "heading": "Solving quadratic congruences", "description": "How to solve quadratic congruences modulo composite integers. How to tell whether a solution exists and how to compute it.", "summary": "How do you solve congruences of the form x2 ≡ a (mod m)? Said another way, how do you find square roots in modular arithmetic?", "word_count": 1094, "blocks": [{"tag": "h1", "text": "Solving quadratic congruences"}, {"tag": "p", "text": "How do you solve congruences of the form x2 ≡ a (mod m)? Said another way, how do you find square roots in modular arithmetic?"}, {"tag": "p", "text": "Every number theory book I've seen points out that the general problem of solving x2 ≡ a (mod m) can be reduced to the solving the special case where m is a prime then spends most of the time studying this special case in detail. However, I haven't seen a book that is entirely clear on exactly how to reduce the general problem to the problem of prime moduli, or how you can unwind the reduction to produce a solution to the original problem. I will try to fill in the gaps here."}, {"tag": "p", "text": "Throughout these notes we will assume m and a have no factors in common."}, {"tag": "h2", "text": "Reduce general moduli to prime power moduli"}, {"tag": "p", "text": "The first step in the reduction is clear. Factor the modulus m into prime powers: m = p1e1 p2e2 ... prer. If the congruence x2 ≡ a (mod m) has a solution, that solution is necessarily a solution to each of the prime power congruences x2 ≡ a (mod piei). Conversely, if you find solutions to each of the prime power congruences, you can use the Chinese Remainder Theorem to produce a solution to the original problem."}, {"tag": "h2", "text": "Reduce prime power moduli to prime moduli"}, {"tag": "p", "text": "This is the part that is often not presented clearly. Also, powers of 2 must be handled separately from powers of odd primes, and the former is sometimes neglected."}, {"tag": "p", "text": "For any prime p, a necessary condition for x2 ≡ a (mod pn) to have a solution is for x2 ≡ a (mod p) to have a solution. (To see this, note that if x2 - a is divisible by pn then it is certainly divisible by p.) Perhaps surprisingly, this is also a sufficient condition."}, {"tag": "h3", "text": "Powers of 2"}, {"tag": "p", "text": "Let a be an odd integer. (Since we are assuming m and a are relatively prime, if m has a power of 2 as a factor, a must be odd.)"}, {"tag": "p", "text": "First, x2 ≡ a (mod 2) has a solution, namely x ≡ 1 (mod 2)."}, {"tag": "p", "text": "Next, x2 ≡ a (mod 4) has a solution if and only if a ≡ 1 (mod 4), in which case the solutions are x ≡ 1 (mod 4) and x ≡ 3 (mod 4)."}, {"tag": "p", "text": "Finally, for n ≥ 3, x2 ≡ a (mod 2n) has four unique solutions if a ≡ 1 (mod 8) and no solutions otherwise."}, {"tag": "p", "text": "If a ≡ 1 (mod 8) then x2 ≡ a (mod 8) has four solutions: 1, 3, 5, and 7. The solutions to x2 ≡ a (mod 2n) for n > 3 can be found by the procedure below that starts with each of the solutions (mod 8) and produces solutions by induction for higher powers of 2."}, {"tag": "p", "text": "Suppose xk2 ≡ a (mod 2k) for k ≥ 3. By definition, this means x2 - a is divisible by 2k. If (x2 - a)/2k is odd, let i = 1. Otherwise let i = 0. Then xk+1 defined by xk + i 2k-1 is a solution to xk+12 ≡ a (mod 2k+1)."}, {"tag": "h3", "text": "Powers of odd primes"}, {"tag": "p", "text": "Let p be an odd prime and let a be any integer relatively prime to p. Then there is a procedure based on Hensel's Lemma that can take a solution to x2 ≡ a (mod p) and produce solutions to x2 ≡ a (mod pn) for n = 2, 3, 4, etc."}, {"tag": "p", "text": "Suppose xk is a solution to x2 ≡ a (mod pk) for some k ≥ 1. Let yk be a solution to 2 xk yk ≡ 1 (mod p). Then xk+1 = xk - (xk2 - a)yk is a solution to x2 ≡ a (mod pk+1)."}, {"tag": "p", "text": "The procedure above shows how to construct one solution to x2 ≡ a (mod pn) but it does not tell us whether there are more solutions. Next we'll show that if we find a solution x, there is exactly one other solution, -x. (Thanks to Nemo for providing this proof.)"}, {"tag": "p", "text": "Suppose x2 = y2 ≡ a (mod pn) where p is an odd prime and a is relatively prime to p. Then x2 - y2 ≡ (x - y)(x + y) ≡ 0 (mod pn). Thus pn divides the product (x+y)(x-y) and so p divides the product as well."}, {"tag": "p", "text": "If p divided both x+y and x-y, then p would divide both their sum and their difference, 2x and -2y. Since p is an odd prime, p does not divide 2 and so p would divide both x and y. Now x2 ≡ a (mod pn), and so x2 = k pn + a for some k. If p divided x, then p would divide x2, and therefore p would divide a, and a would not be relatively prime to pn. Therefore p divides neither x nor y. It follows that p either divides (x+y) or (x-y) but not both. Since pn divides (x+y)(x-y), it only divides one of (x+y) and (x-y). Therefore, either x ≡ y (mod pn) or x ≡ -y (mod pn). So if a has any square root modulo pn — call it x — then it has exactly two roots: x and -x."}, {"tag": "h2", "text": "Odd prime moduli"}, {"tag": "p", "text": "We only consider odd primes here because the case p = 2 was handled above. Therefore we assume p is an odd prime in this section."}, {"tag": "p", "text": "If x2 ≡ a (mod p) has a solution, we say a is a \"quadratic reside mod p.\" If this congruence has no solution, we say x is a \"quadratic non-residue mod p.\""}, {"tag": "p", "text": "The congruence x2 ≡ a (mod p) either has no solutions or two solutions. If x is a solution, so is -x."}, {"tag": "p", "text": "Euler's Criterion says that an odd integer a relatively prime to p is a quadratic residue (mod p) if and only if a(p-1)/2 ≡ 1 (mod p). This fact is enough to settle the question of whether a is a quadratic residue. It could be used in a practical algorithm using fast exponentiation. However, there is an extensive and beautiful theory called \"quadratic reciprocity\" that studies this problem further and produces more efficient algorithms."}, {"tag": "p", "text": "If a is a quadratic residue (mod p) and a ≡ 3 (mod 4) then a(p+1)/4 is a solution to x2 ≡ a (mod p). If a ≡ 1 (mod 4) there is no analogous formula. In that case, one may use the Tonelli-Shanks algorithm. (Thanks to Alasdair McAndrew for pointing out this algorithm.)"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Solving quadratic congruences\n\nHow do you solve congruences of the form x2 ≡ a (mod m)? Said another way, how do you find square roots in modular arithmetic?\n\nEvery number theory book I've seen points out that the general problem of solving x2 ≡ a (mod m) can be reduced to the solving the special case where m is a prime then spends most of the time studying this special case in detail. However, I haven't seen a book that is entirely clear on exactly how to reduce the general problem to the problem of prime moduli, or how you can unwind the reduction to produce a solution to the original problem. I will try to fill in the gaps here.\n\nThroughout these notes we will assume m and a have no factors in common.\n\nReduce general moduli to prime power moduli\n\nThe first step in the reduction is clear. Factor the modulus m into prime powers: m = p1e1 p2e2 ... prer. If the congruence x2 ≡ a (mod m) has a solution, that solution is necessarily a solution to each of the prime power congruences x2 ≡ a (mod piei). Conversely, if you find solutions to each of the prime power congruences, you can use the Chinese Remainder Theorem to produce a solution to the original problem.\n\nReduce prime power moduli to prime moduli\n\nThis is the part that is often not presented clearly. Also, powers of 2 must be handled separately from powers of odd primes, and the former is sometimes neglected.\n\nFor any prime p, a necessary condition for x2 ≡ a (mod pn) to have a solution is for x2 ≡ a (mod p) to have a solution. (To see this, note that if x2 - a is divisible by pn then it is certainly divisible by p.) Perhaps surprisingly, this is also a sufficient condition.\n\nPowers of 2\n\nLet a be an odd integer. (Since we are assuming m and a are relatively prime, if m has a power of 2 as a factor, a must be odd.)\n\nFirst, x2 ≡ a (mod 2) has a solution, namely x ≡ 1 (mod 2).\n\nNext, x2 ≡ a (mod 4) has a solution if and only if a ≡ 1 (mod 4), in which case the solutions are x ≡ 1 (mod 4) and x ≡ 3 (mod 4).\n\nFinally, for n ≥ 3, x2 ≡ a (mod 2n) has four unique solutions if a ≡ 1 (mod 8) and no solutions otherwise.\n\nIf a ≡ 1 (mod 8) then x2 ≡ a (mod 8) has four solutions: 1, 3, 5, and 7. The solutions to x2 ≡ a (mod 2n) for n > 3 can be found by the procedure below that starts with each of the solutions (mod 8) and produces solutions by induction for higher powers of 2.\n\nSuppose xk2 ≡ a (mod 2k) for k ≥ 3. By definition, this means x2 - a is divisible by 2k. If (x2 - a)/2k is odd, let i = 1. Otherwise let i = 0. Then xk+1 defined by xk + i 2k-1 is a solution to xk+12 ≡ a (mod 2k+1).\n\nPowers of odd primes\n\nLet p be an odd prime and let a be any integer relatively prime to p. Then there is a procedure based on Hensel's Lemma that can take a solution to x2 ≡ a (mod p) and produce solutions to x2 ≡ a (mod pn) for n = 2, 3, 4, etc.\n\nSuppose xk is a solution to x2 ≡ a (mod pk) for some k ≥ 1. Let yk be a solution to 2 xk yk ≡ 1 (mod p). Then xk+1 = xk - (xk2 - a)yk is a solution to x2 ≡ a (mod pk+1).\n\nThe procedure above shows how to construct one solution to x2 ≡ a (mod pn) but it does not tell us whether there are more solutions. Next we'll show that if we find a solution x, there is exactly one other solution, -x. (Thanks to Nemo for providing this proof.)\n\nSuppose x2 = y2 ≡ a (mod pn) where p is an odd prime and a is relatively prime to p. Then x2 - y2 ≡ (x - y)(x + y) ≡ 0 (mod pn). Thus pn divides the product (x+y)(x-y) and so p divides the product as well.\n\nIf p divided both x+y and x-y, then p would divide both their sum and their difference, 2x and -2y. Since p is an odd prime, p does not divide 2 and so p would divide both x and y. Now x2 ≡ a (mod pn), and so x2 = k pn + a for some k. If p divided x, then p would divide x2, and therefore p would divide a, and a would not be relatively prime to pn. Therefore p divides neither x nor y. It follows that p either divides (x+y) or (x-y) but not both. Since pn divides (x+y)(x-y), it only divides one of (x+y) and (x-y). Therefore, either x ≡ y (mod pn) or x ≡ -y (mod pn). So if a has any square root modulo pn — call it x — then it has exactly two roots: x and -x.\n\nOdd prime moduli\n\nWe only consider odd primes here because the case p = 2 was handled above. Therefore we assume p is an odd prime in this section.\n\nIf x2 ≡ a (mod p) has a solution, we say a is a \"quadratic reside mod p.\" If this congruence has no solution, we say x is a \"quadratic non-residue mod p.\"\n\nThe congruence x2 ≡ a (mod p) either has no solutions or two solutions. If x is a solution, so is -x.\n\nEuler's Criterion says that an odd integer a relatively prime to p is a quadratic residue (mod p) if and only if a(p-1)/2 ≡ 1 (mod p). This fact is enough to settle the question of whether a is a quadratic residue. It could be used in a practical algorithm using fast exponentiation. However, there is an extensive and beautiful theory called \"quadratic reciprocity\" that studies this problem further and produces more efficient algorithms.\n\nIf a is a quadratic residue (mod p) and a ≡ 3 (mod 4) then a(p+1)/4 is a solution to x2 ≡ a (mod p). If a ≡ 1 (mod 4) there is no analogous formula. In that case, one may use the Tonelli-Shanks algorithm. (Thanks to Alasdair McAndrew for pointing out this algorithm.)\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "r_language_regex", "canonical_url": "https://www.johndcook.com/blog/r_language_regex/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/r_language_regex.html", "title": "Regular expressions in R", "heading": "Regular expressions in R", "description": "Notes on working with regular expressions in R.", "summary": "Doing extensive text manipulation in R would be painful; the R language was developed for analyzing data sets, not for munging text files. However, R does have some facilities for working with text using regular expressions. This comes in handy, for example, when selecting rows of a data set according to regular expression pattern matches in some columns.", "word_count": 516, "blocks": [{"tag": "h1", "text": "Regular expressions in R"}, {"tag": "p", "text": "Doing extensive text manipulation in R would be painful; the R language was developed for analyzing data sets, not for munging text files. However, R does have some facilities for working with text using regular expressions. This comes in handy, for example, when selecting rows of a data set according to regular expression pattern matches in some columns."}, {"tag": "p", "text": "R supports two regular expression flavors: POSIX 1003.2 and Perl. Regular expression functions in R contain two arguments: extended, which defaults to TRUE, and perl, which defaults to FALSE. By default R uses POSIX extended regular expressions, though if extended is set to FALSE, it will use basic POSIX regular expressions. If perl is set to TRUE, R will use the Perl 5 flavor of regular expressions as implemented in the PCRE library."}, {"tag": "p", "text": "Regular expressions are represented as strings. Metacharacters often need to be escaped. For example, the metacharacter \\w must be entered as \\\\w to prevent R from interpreting the leading backslash before sending the string to the regular expression parser."}, {"tag": "p", "text": "The grep function requires two arguments. The first is a string containing a regular expression. The second is a vector of strings to search for matches. The grep function returns a list of indices. If the regular expression matches a particular vector component, that component's index is part of the list."}, {"tag": "p", "text": "Example:"}, {"tag": "p", "text": "grep(\"apple\", c(\"crab apple\", \"Apple jack\", \"apple sauce\"))"}, {"tag": "p", "text": "returns the vector (1, 3) because the first and third elements of the array contain \"apple.\" Note that grep is case-sensitive by default and so \"apple\" does not match \"Apple.\" To perform a case-insensitive match, add ignore.case = TRUE to the function call."}, {"tag": "p", "text": "There is an optional argument value that defaults to FALSE. If this argument is set to TRUE, grep will return the actual matches rather than their indices."}, {"tag": "p", "text": "The function sub replaces one pattern with another. It requires three arguments: a regular expression, a replacement pattern, and a vector of strings to process. It is analogous to s/// in Perl. Note that if you use the Perl regular expression flavor by adding perl = TRUE and want to use capture references such as \\1 or \\2 in the replacement pattern, these must be entered as \\\\1 or \\\\2."}, {"tag": "p", "text": "The sub function replaces only the first instance of a regular expression. To replace all instances of a pattern, use gsub. The gsub function is analogous to s///g in Perl."}, {"tag": "p", "text": "The function regexpr requires two arguments, a regular expression and a vector of text to process. It is similar to grep, but returns the locations of the regular expression matches. If a particular component does not match the regular expression, the return vector contains a -1 for that component. The function gregexpr is a variation on regexpr that returns the number of matches in each component."}, {"tag": "p", "text": "The function strsplit also uses regular expressions, splitting its input according to a specified regular expression."}, {"tag": "h2", "text": "Resources"}, {"tag": "p", "text": "Notes on using regular expressions in other languages:"}, {"tag": "li", "text": "PowerShell"}, {"tag": "li", "text": "C++"}, {"tag": "li", "text": "Python"}, {"tag": "li", "text": "Mathematica"}, {"tag": "p", "text": "Daily tips on regular expressions"}, {"tag": "p", "text": "See also R for programmers coming from other languages."}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Regular expressions in R\n\nDoing extensive text manipulation in R would be painful; the R language was developed for analyzing data sets, not for munging text files. However, R does have some facilities for working with text using regular expressions. This comes in handy, for example, when selecting rows of a data set according to regular expression pattern matches in some columns.\n\nR supports two regular expression flavors: POSIX 1003.2 and Perl. Regular expression functions in R contain two arguments: extended, which defaults to TRUE, and perl, which defaults to FALSE. By default R uses POSIX extended regular expressions, though if extended is set to FALSE, it will use basic POSIX regular expressions. If perl is set to TRUE, R will use the Perl 5 flavor of regular expressions as implemented in the PCRE library.\n\nRegular expressions are represented as strings. Metacharacters often need to be escaped. For example, the metacharacter \\w must be entered as \\\\w to prevent R from interpreting the leading backslash before sending the string to the regular expression parser.\n\nThe grep function requires two arguments. The first is a string containing a regular expression. The second is a vector of strings to search for matches. The grep function returns a list of indices. If the regular expression matches a particular vector component, that component's index is part of the list.\n\nExample:\n\ngrep(\"apple\", c(\"crab apple\", \"Apple jack\", \"apple sauce\"))\n\nreturns the vector (1, 3) because the first and third elements of the array contain \"apple.\" Note that grep is case-sensitive by default and so \"apple\" does not match \"Apple.\" To perform a case-insensitive match, add ignore.case = TRUE to the function call.\n\nThere is an optional argument value that defaults to FALSE. If this argument is set to TRUE, grep will return the actual matches rather than their indices.\n\nThe function sub replaces one pattern with another. It requires three arguments: a regular expression, a replacement pattern, and a vector of strings to process. It is analogous to s/// in Perl. Note that if you use the Perl regular expression flavor by adding perl = TRUE and want to use capture references such as \\1 or \\2 in the replacement pattern, these must be entered as \\\\1 or \\\\2.\n\nThe sub function replaces only the first instance of a regular expression. To replace all instances of a pattern, use gsub. The gsub function is analogous to s///g in Perl.\n\nThe function regexpr requires two arguments, a regular expression and a vector of text to process. It is similar to grep, but returns the locations of the regular expression matches. If a particular component does not match the regular expression, the return vector contains a -1 for that component. The function gregexpr is a variation on regexpr that returns the number of matches in each component.\n\nThe function strsplit also uses regular expressions, splitting its input according to a specified regular expression.\n\nResources\n\nNotes on using regular expressions in other languages:\n\nPowerShell\n\nC++\n\nPython\n\nMathematica\n\nDaily tips on regular expressions\n\nSee also R for programmers coming from other languages.\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "powershell_perl_regex", "canonical_url": "https://www.johndcook.com/blog/powershell_perl_regex/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/regex.html", "title": "Regular expressions in PowerShell and Perl", "heading": "Regular expressions in PowerShell and Perl", "description": "Working with regular expressions in PowerShell. Comparing PowerShell's regular expression support to Perl.", "summary": "Overview PowerShell's regular expression flavor Matching and replacing Case-sensitivity Retrieving single matches Global matching and replacing Replacing with captures", "word_count": 974, "blocks": [{"tag": "h1", "text": "Regular expressions in PowerShell and Perl"}, {"tag": "p", "text": "Overview PowerShell's regular expression flavor Matching and replacing Case-sensitivity Retrieving single matches Global matching and replacing Replacing with captures"}, {"tag": "h2", "text": "Overview"}, {"tag": "p", "text": "This page is written for the benefit of someone familiar with regular expressions but not with the use of regular expressions in Microsoft's PowerShell. Comparisons will be made with Perl for those familiar with the language, though no knowledge of Perl is required. The focus is not on the syntax of regular expressions per se but rather how to use regular expressions to search for patterns and make replacements."}, {"tag": "h2", "text": "PowerShell's regular expression flavor"}, {"tag": "p", "text": "PowerShell is built upon Microsoft's .NET framework. In regular expressions, as in much else, PowerShell uses the .NET implementation. And .NET in turn essentially uses Perl 5's regular expression syntax, with a few added features such as named captures. For someone familiar with regular expressions, especially as they are implemented in .NET or Perl, the difficulty in using PowerShell is not in the syntax of regular expressions themselves, but rather in using regular expressions to do work. Because PowerShell is new, detailed documentation and examples are harder to find than for .NET or Perl."}, {"tag": "h2", "text": "Matching and replacing"}, {"tag": "p", "text": "PowerShell has -match and -replace operators that are roughly analogous to the m// and s/// operators in Perl. For example, start with the command"}, {"tag": "p", "text": "$greeting = \"Hello world\""}, {"tag": "p", "text": "which would be legal PowerShell or Perl code. The PowerShell statement"}, {"tag": "p", "text": "$greeting -match \"ello\""}, {"tag": "p", "text": "would return a Boolean true value just as the Perl statement"}, {"tag": "p", "text": "$greeting =~ m/ello/;"}, {"tag": "p", "text": "would return 1. Similarly, the PowerShell statement"}, {"tag": "p", "text": "$greeting -replace \"world\", \"planet\""}, {"tag": "p", "text": "would return \"Hello planet\", as would the statement"}, {"tag": "p", "text": "$greeting =~ s/world/planet/;"}, {"tag": "p", "text": "in Perl. However, the PowerShell statement returns a new string, leaving $greeting unchanged, while the corresponding Perl statement changes the string $greeting in place."}, {"tag": "h2", "text": "Case-sensitivity"}, {"tag": "p", "text": "Perl modifies the behavior of the m// and s/// operators by adding characters to indicate such things as case-sensitivity. Both operators, like nearly everything else in Perl, are case-sensitive by default. Appending an 'i' causes them to be case-insensitive."}, {"tag": "p", "text": "PowerShell's -match and -replace operators, like nearly everything else in PowerShell, are case-insensitive by default. PowerShell offers the highly recommended option of specifying -imatch and -ireplace to make case-insensitivity explicit. The case-sensitive counterparts are -cmatch and -creplace."}, {"tag": "h2", "text": "Retrieving single matches"}, {"tag": "p", "text": "After performing a match in Perl, the captured matches are stored in the variables $1, $2, etc. Similarly, after a match PowerShell creates an array $matches with $matches[n] corresponding to Perl's $n."}, {"tag": "h2", "text": "Global matching and replacing"}, {"tag": "h3", "text": "Perl"}, {"tag": "p", "text": "Perl uses a 'g' option to specify global matches and replacements. The unmodified operator m// returns a Boolean value, but the modified operator m//g returns an array containing all substrings matching the specified pattern. Both the s/// and s///g operators modify their string in place. The unmodified s/// operator replaces only the first match in the string, while the modified s///g operator replaces all occurrences of the pattern."}, {"tag": "p", "text": "For example, first set"}, {"tag": "p", "text": "$str = \"Cookbook\";"}, {"tag": "p", "text": "Then executing"}, {"tag": "p", "text": "$b = $str =~ m/(\\woo\\w)/;"}, {"tag": "p", "text": "sets $b to 1 (true) and populates $1 with \"Cook\". Executing"}, {"tag": "p", "text": "@a = $str =~ m/(\\woo\\w)/g;"}, {"tag": "p", "text": "sets @a to the array containing \"Cook\" and \"book\"."}, {"tag": "p", "text": "The statement"}, {"tag": "p", "text": "$str =~ s/oo/u/;"}, {"tag": "p", "text": "would convert \"Cookbook\" into \"Cukbook\", while the statement"}, {"tag": "p", "text": "$str =~ s/oo/u/g;"}, {"tag": "p", "text": "would convert \"Cookbook\" into \"Cukbuk\"."}, {"tag": "h3", "text": "PowerShell"}, {"tag": "p", "text": "PowerShell's -creplace and -ireplace work very much like the s///g and s///ig operators in Perl: all matches are replaced. PowerShell version 1.0 does not have a direct analog to Perl's s/// and s///i operators without the 'g' option."}, {"tag": "p", "text": "Neither does PowerShell 1.0 have an analog to the m//g option in Perl, though Keith Hill has proposed that Microsoft add a -matches operator in a future release of PowerShell analogous to Perl's m//g."}, {"tag": "p", "text": "In order to have more fine control over match and replace operations in PowerShell, one must use the [regex] class rather than the -match and -replace operators. The following example shows how to do a global match in PowerShell."}, {"tag": "p", "text": "$str = \"Cookbook\" [regex]::matches($str, \"\\woo\\w\")"}, {"tag": "p", "text": "The last command returns a compound structure containing more information than just the matches. To fill an array $words with just the matches, use the command"}, {"tag": "p", "text": "$words = ([regex]::matches($str, \"\\woo\\w\") | %{$_.value})"}, {"tag": "p", "text": "which pipes the [regex]::matches output through the foreach operator % and selects the matched text."}, {"tag": "p", "text": "The [regex] class in PowerShell is invoking the Regex class in .NET, and so all the options of the .NET class are available. For example, the .NET class library has a RegexOptions enumeration with useful values such as IgnoreCase, RightToLeft, etc. However, it is not obvious how one should translate from .NET to PowerShell syntax. You can't write RegexOptions.IgnoreCase in PowerShell code the way you would in C#. It turns out the thing to do is to quote the option as a string. For example, the code"}, {"tag": "p", "text": "[regex]::matches($str, \"[a-z]ook\", \"IgnoreCase\")"}, {"tag": "p", "text": "will match \"Cook\" and \"book\", whereas without the IgnoreCase option only \"book\" would match."}, {"tag": "h2", "text": "Replacing with captures"}, {"tag": "p", "text": "Often you want to replace a pattern not with a constant string but with a string based on the text that matched. For example, suppose you want to replace italicized words with double words. If $str contained \"<i>big</i>\" then after executing the Perl statement"}, {"tag": "p", "text": "$str =~ s{<i>(\\w+)</i>}{$1 $1};"}, {"tag": "p", "text": "$str would contain \"big big\". (This illustrates an alternative Perl syntax for s///. The s{}{} alternative prevents having to escape backslashes as in </i> above.)"}, {"tag": "p", "text": "The equivalent PowerShell code would be"}, {"tag": "p", "text": "$str = [regex]::Replace($str, \"<i>(\\w+)</i>\", '$1 $1');"}, {"tag": "p", "text": "Notice the single quotes around the replacement pattern. This is to keep PowerShell from interpreting \"$1\" before passing it to the Regex class. We could use double quotes if we also put back ticks in front of the dollar signs to escape them."}, {"tag": "h2", "text": "Further resources"}, {"tag": "p", "text": "Notes on using regular expression in other languages:"}, {"tag": "li", "text": "C++"}, {"tag": "li", "text": "Python"}, {"tag": "li", "text": "R"}, {"tag": "li", "text": "Mathematica"}, {"tag": "p", "text": "Daily tips on regular expressions"}, {"tag": "p", "text": "Other PowerShell articles:"}, {"tag": "li", "text": "Introduction"}, {"tag": "li", "text": "Gotchas"}, {"tag": "li", "text": "Cookbook"}, {"tag": "li", "text": "Day 1"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}, {"tag": "p", "text": "jdc"}], "content": "Regular expressions in PowerShell and Perl\n\nOverview PowerShell's regular expression flavor Matching and replacing Case-sensitivity Retrieving single matches Global matching and replacing Replacing with captures\n\nOverview\n\nThis page is written for the benefit of someone familiar with regular expressions but not with the use of regular expressions in Microsoft's PowerShell. Comparisons will be made with Perl for those familiar with the language, though no knowledge of Perl is required. The focus is not on the syntax of regular expressions per se but rather how to use regular expressions to search for patterns and make replacements.\n\nPowerShell's regular expression flavor\n\nPowerShell is built upon Microsoft's .NET framework. In regular expressions, as in much else, PowerShell uses the .NET implementation. And .NET in turn essentially uses Perl 5's regular expression syntax, with a few added features such as named captures. For someone familiar with regular expressions, especially as they are implemented in .NET or Perl, the difficulty in using PowerShell is not in the syntax of regular expressions themselves, but rather in using regular expressions to do work. Because PowerShell is new, detailed documentation and examples are harder to find than for .NET or Perl.\n\nMatching and replacing\n\nPowerShell has -match and -replace operators that are roughly analogous to the m// and s/// operators in Perl. For example, start with the command\n\n$greeting = \"Hello world\"\n\nwhich would be legal PowerShell or Perl code. The PowerShell statement\n\n$greeting -match \"ello\"\n\nwould return a Boolean true value just as the Perl statement\n\n$greeting =~ m/ello/;\n\nwould return 1. Similarly, the PowerShell statement\n\n$greeting -replace \"world\", \"planet\"\n\nwould return \"Hello planet\", as would the statement\n\n$greeting =~ s/world/planet/;\n\nin Perl. However, the PowerShell statement returns a new string, leaving $greeting unchanged, while the corresponding Perl statement changes the string $greeting in place.\n\nCase-sensitivity\n\nPerl modifies the behavior of the m// and s/// operators by adding characters to indicate such things as case-sensitivity. Both operators, like nearly everything else in Perl, are case-sensitive by default. Appending an 'i' causes them to be case-insensitive.\n\nPowerShell's -match and -replace operators, like nearly everything else in PowerShell, are case-insensitive by default. PowerShell offers the highly recommended option of specifying -imatch and -ireplace to make case-insensitivity explicit. The case-sensitive counterparts are -cmatch and -creplace.\n\nRetrieving single matches\n\nAfter performing a match in Perl, the captured matches are stored in the variables $1, $2, etc. Similarly, after a match PowerShell creates an array $matches with $matches[n] corresponding to Perl's $n.\n\nGlobal matching and replacing\n\nPerl\n\nPerl uses a 'g' option to specify global matches and replacements. The unmodified operator m// returns a Boolean value, but the modified operator m//g returns an array containing all substrings matching the specified pattern. Both the s/// and s///g operators modify their string in place. The unmodified s/// operator replaces only the first match in the string, while the modified s///g operator replaces all occurrences of the pattern.\n\nFor example, first set\n\n$str = \"Cookbook\";\n\nThen executing\n\n$b = $str =~ m/(\\woo\\w)/;\n\nsets $b to 1 (true) and populates $1 with \"Cook\". Executing\n\n@a = $str =~ m/(\\woo\\w)/g;\n\nsets @a to the array containing \"Cook\" and \"book\".\n\nThe statement\n\n$str =~ s/oo/u/;\n\nwould convert \"Cookbook\" into \"Cukbook\", while the statement\n\n$str =~ s/oo/u/g;\n\nwould convert \"Cookbook\" into \"Cukbuk\".\n\nPowerShell\n\nPowerShell's -creplace and -ireplace work very much like the s///g and s///ig operators in Perl: all matches are replaced. PowerShell version 1.0 does not have a direct analog to Perl's s/// and s///i operators without the 'g' option.\n\nNeither does PowerShell 1.0 have an analog to the m//g option in Perl, though Keith Hill has proposed that Microsoft add a -matches operator in a future release of PowerShell analogous to Perl's m//g.\n\nIn order to have more fine control over match and replace operations in PowerShell, one must use the [regex] class rather than the -match and -replace operators. The following example shows how to do a global match in PowerShell.\n\n$str = \"Cookbook\" [regex]::matches($str, \"\\woo\\w\")\n\nThe last command returns a compound structure containing more information than just the matches. To fill an array $words with just the matches, use the command\n\n$words = ([regex]::matches($str, \"\\woo\\w\") | %{$_.value})\n\nwhich pipes the [regex]::matches output through the foreach operator % and selects the matched text.\n\nThe [regex] class in PowerShell is invoking the Regex class in .NET, and so all the options of the .NET class are available. For example, the .NET class library has a RegexOptions enumeration with useful values such as IgnoreCase, RightToLeft, etc. However, it is not obvious how one should translate from .NET to PowerShell syntax. You can't write RegexOptions.IgnoreCase in PowerShell code the way you would in C#. It turns out the thing to do is to quote the option as a string. For example, the code\n\n[regex]::matches($str, \"[a-z]ook\", \"IgnoreCase\")\n\nwill match \"Cook\" and \"book\", whereas without the IgnoreCase option only \"book\" would match.\n\nReplacing with captures\n\nOften you want to replace a pattern not with a constant string but with a string based on the text that matched. For example, suppose you want to replace italicized words with double words. If $str contained \"<i>big</i>\" then after executing the Perl statement\n\n$str =~ s{<i>(\\w+)</i>}{$1 $1};\n\n$str would contain \"big big\". (This illustrates an alternative Perl syntax for s///. The s{}{} alternative prevents having to escape backslashes as in </i> above.)\n\nThe equivalent PowerShell code would be\n\n$str = [regex]::Replace($str, \"<i>(\\w+)</i>\", '$1 $1');\n\nNotice the single quotes around the replacement pattern. This is to keep PowerShell from interpreting \"$1\" before passing it to the Regex class. We could use double quotes if we also put back ticks in front of the dollar signs to escape them.\n\nFurther resources\n\nNotes on using regular expression in other languages:\n\nC++\n\nPython\n\nR\n\nMathematica\n\nDaily tips on regular expressions\n\nOther PowerShell articles:\n\nIntroduction\n\nGotchas\n\nCookbook\n\nDay 1\n\nHome\n\nBlog\n\nConsulting\n\nContact\n\njdc"}
{"slug": "regextip", "canonical_url": "https://www.johndcook.com/blog/regextip/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/regextip.html", "title": "Regex tip", "heading": "Regex tip", "description": "Learn regular expressions by following RegexTip on Twitter", "summary": "Regex tip is a Twitter account that gives one tip per day for learning regular expressions. Most of the tips apply to many flavors of regular expressions: Perl, Python, .NET, etc.", "word_count": 122, "blocks": [{"tag": "h1", "text": "Regex tip"}, {"tag": "p", "text": "Regex tip is a Twitter account that gives one tip per day for learning regular expressions. Most of the tips apply to many flavors of regular expressions: Perl, Python, .NET, etc."}, {"tag": "p", "text": "If you use an RSS reader such as Google Reader but don't use Twitter, you can subscribe to the RegexTip RSS feed."}, {"tag": "p", "text": "Here are a few regular expression resources."}, {"tag": "p", "text": "Four tips for learning regular expressions Regular expressions in PowerShell and Perl Regular expressions in Python and Perl C++ TR1 regular expressions"}, {"tag": "p", "text": "If you'd like to learn about regular expressions in depth, the best book on regular expressions is Jeffrey Friedl's Mastering Regular Expressions."}, {"tag": "p", "text": "See my list of Twitter accounts for my personal account and my other daily tip accounts."}], "content": "Regex tip\n\nRegex tip is a Twitter account that gives one tip per day for learning regular expressions. Most of the tips apply to many flavors of regular expressions: Perl, Python, .NET, etc.\n\nIf you use an RSS reader such as Google Reader but don't use Twitter, you can subscribe to the RegexTip RSS feed.\n\nHere are a few regular expression resources.\n\nFour tips for learning regular expressions Regular expressions in PowerShell and Perl Regular expressions in Python and Perl C++ TR1 regular expressions\n\nIf you'd like to learn about regular expressions in depth, the best book on regular expressions is Jeffrey Friedl's Mastering Regular Expressions.\n\nSee my list of Twitter accounts for my personal account and my other daily tip accounts."}
{"slug": "relative_error_normal_approx", "canonical_url": "https://www.johndcook.com/blog/relative_error_normal_approx/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/relative_error_normal_approx.html", "title": "Relative error in normal approximations", "heading": "Relative error in normal approximations", "description": "The relative error in the normal approximation a distribution may be huge even when the absolute error is small", "summary": "The normal distribution is a good approximation to the Student-t distribution when the t distribution has a large degrees of freedom parameter ν. Many books recommend using the normal approximation to the t distribution when ν ≥ 30.", "word_count": 292, "blocks": [{"tag": "h1", "text": "Relative error in normal approximations"}, {"tag": "p", "text": "The normal distribution is a good approximation to the Student-t distribution when the t distribution has a large degrees of freedom parameter ν. Many books recommend using the normal approximation to the t distribution when ν ≥ 30."}, {"tag": "p", "text": "When ν = 30, the maximum difference between the CDFs of the normal and t distribution is 0.005244. That means that the maximum absolute error in computing the probability of any interval [a, b] by subtracting the CDF values at the two ends is going to be small. But the relative error may be enormous."}, {"tag": "p", "text": "This problem of large relative error is not limited to the t distribution but is typical of normal approximations in general. The normal distribution has very thin tails, and in most applications the normal will be used to approximate a distribution with thicker tails. In that case the relative error in the normal approximation will be large in the tails."}, {"tag": "p", "text": "Let GN be the CCDF of a standard normal random variable and let Gt be the CCDF of a Student t random variable with 30 degrees of freedom. (I use the CCDF, the complementary CDF, rather than the CDF because that makes it easier to read the graphs from left to right.) The absolute error, |GN - Gt| on the interval [0, 5] is given below."}, {"tag": "p", "text": "As mentioned above, the absolute error is 0.005244. The absolute error decreases as the argument |GN(x) - Gt(x)| decreases as x increases past the location of the maximum error."}, {"tag": "p", "text": "The relative error curve, |GN - Gt| / Gt is very different."}, {"tag": "p", "text": "The relative error |GN(x) - Gt(x)| / Gt(x) increases as x increases and is practically 1 for large x. That is, the relative error is nearly 100%."}], "content": "Relative error in normal approximations\n\nThe normal distribution is a good approximation to the Student-t distribution when the t distribution has a large degrees of freedom parameter ν. Many books recommend using the normal approximation to the t distribution when ν ≥ 30.\n\nWhen ν = 30, the maximum difference between the CDFs of the normal and t distribution is 0.005244. That means that the maximum absolute error in computing the probability of any interval [a, b] by subtracting the CDF values at the two ends is going to be small. But the relative error may be enormous.\n\nThis problem of large relative error is not limited to the t distribution but is typical of normal approximations in general. The normal distribution has very thin tails, and in most applications the normal will be used to approximate a distribution with thicker tails. In that case the relative error in the normal approximation will be large in the tails.\n\nLet GN be the CCDF of a standard normal random variable and let Gt be the CCDF of a Student t random variable with 30 degrees of freedom. (I use the CCDF, the complementary CDF, rather than the CDF because that makes it easier to read the graphs from left to right.) The absolute error, |GN - Gt| on the interval [0, 5] is given below.\n\nAs mentioned above, the absolute error is 0.005244. The absolute error decreases as the argument |GN(x) - Gt(x)| decreases as x increases past the location of the maximum error.\n\nThe relative error curve, |GN - Gt| / Gt is very different.\n\nThe relative error |GN(x) - Gt(x)| / Gt(x) increases as x increases and is practically 1 for large x. That is, the relative error is nearly 100%."}
{"slug": "reproducible", "canonical_url": "https://www.johndcook.com/blog/reproducible/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/reproducible.html", "title": "Reproducible Analysis", "heading": "Reproducible Analysis", "description": "Reproducible analysis.  Resources for making data analysis intelligible and reproducible using literate programming with R and Sweave.", "summary": "Imagine someone asking you one of the following requests. If any of these give you a sinking feeling that this is going to be more difficult than it should be, you see the need for reproducible analysis.", "word_count": 281, "blocks": [{"tag": "h1", "text": "Reproducible Analysis"}, {"tag": "p", "text": "Imagine someone asking you one of the following requests. If any of these give you a sinking feeling that this is going to be more difficult than it should be, you see the need for reproducible analysis."}, {"tag": "li", "text": "I just read this interesting paper. Can you perform the same analysis on my data?"}, {"tag": "li", "text": "Remember that microarray analysis you did six months ago? We ran a few more arrays. Can you add them to the project and repeat the same analysis?"}, {"tag": "li", "text": "The statistical analyst who looked at the data I generated previously is no longer available. Can you get someone else to analyze my new data set using the same methods (and thus producing a report I can expect to understand)?"}, {"tag": "li", "text": "Please write/edit the methods sections for the abstract/paper/grant proposal I am submitting based on the analysis you did several months ago."}, {"tag": "p", "text": "The scenarios above come from the presentation Sweave: First Steps Toward Reproducible Analyses by Kevin Coombes. This presentation motivates the need for reproducible analysis and gives an introduction to Sweave, a tool designed to make it easier to reproduce statistical analyses. The talk itself was created from an Sweave document. The source is available at the same link as the talk."}, {"tag": "h2", "text": "Example of irreproducible analysis"}, {"tag": "p", "text": "“Microarrays: retracing steps” by Kevin Coombes, Jing Wang, and Keith Baggerly in Nature Medicine, November 2007, pp 1276-1277. The authors report their experience trying to reconstruct the analysis that went into a previous article in the same journal. They recount some of the errors they believe must have occurred and explain why the conclusions are unsupported."}, {"tag": "h2", "text": "Articles and other resources"}, {"tag": "p", "text": "See ReproducibleResearch.org for a list of articles and other resources related to reproducible research."}], "content": "Reproducible Analysis\n\nImagine someone asking you one of the following requests. If any of these give you a sinking feeling that this is going to be more difficult than it should be, you see the need for reproducible analysis.\n\nI just read this interesting paper. Can you perform the same analysis on my data?\n\nRemember that microarray analysis you did six months ago? We ran a few more arrays. Can you add them to the project and repeat the same analysis?\n\nThe statistical analyst who looked at the data I generated previously is no longer available. Can you get someone else to analyze my new data set using the same methods (and thus producing a report I can expect to understand)?\n\nPlease write/edit the methods sections for the abstract/paper/grant proposal I am submitting based on the analysis you did several months ago.\n\nThe scenarios above come from the presentation Sweave: First Steps Toward Reproducible Analyses by Kevin Coombes. This presentation motivates the need for reproducible analysis and gives an introduction to Sweave, a tool designed to make it easier to reproduce statistical analyses. The talk itself was created from an Sweave document. The source is available at the same link as the talk.\n\nExample of irreproducible analysis\n\n“Microarrays: retracing steps” by Kevin Coombes, Jing Wang, and Keith Baggerly in Nature Medicine, November 2007, pp 1276-1277. The authors report their experience trying to reconstruct the analysis that went into a previous article in the same journal. They recount some of the errors they believe must have occurred and explain why the conclusions are unsupported.\n\nArticles and other resources\n\nSee ReproducibleResearch.org for a list of articles and other resources related to reproducible research."}
{"slug": "running_regression", "canonical_url": "https://www.johndcook.com/blog/running_regression/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/running_regression.html", "title": "Computing linear regression in one pass", "heading": "Computing linear regression in one pass", "description": "How to accurately compute a simple linear regression with one pass through your data.", "summary": "The RunningRegression class is the analog of the RunningStats class described here and uses that class. You add pairs of (x, y) values by using the Push. At any point along the way you can call the Slope, Intercept, or Correlation functions to see the current value of these statistics.", "word_count": 305, "blocks": [{"tag": "h1", "text": "Computing linear regression in one pass"}, {"tag": "p", "text": "The RunningRegression class is the analog of the RunningStats class described here and uses that class. You add pairs of (x, y) values by using the Push. At any point along the way you can call the Slope, Intercept, or Correlation functions to see the current value of these statistics."}, {"tag": "p", "text": "You can also combine two RunningRegression objects by using the + and += operators. For example, you might accrue data on several different threads in parallel then add their RunningRegression objects together."}, {"tag": "p", "text": "Here is the header file RunningRegression.h:"}, {"tag": "pre", "text": "#ifndef RUNNINGREGRESSION\n#define RUNNINGREGRESSION\n\n#include \"RunningStats.h\"\n\nclass RunningRegression\n{\npublic:\n    RunningRegression();\n    void Clear();\n    void Push(double x, double y);\n    long long NumDataValues() const;\n    double Slope() const;\n    double Intercept() const;\n    double Correlation() const;\n    \n    friend RunningRegression operator+(\n        const RunningRegression a, const RunningRegression b);\n    RunningRegression& operator+=(const RunningRegression &rhs);\n\nprivate:\n    RunningStats x_stats;\n    RunningStats y_stats;\n    double S_xy;\n    long long n;\n};\n\n#endif"}, {"tag": "p", "text": "Here is the implementation file RunningRegression.cpp."}, {"tag": "pre", "text": "#include \"RunningRegression.h\"\n\nRunningRegression::RunningRegression()\n{\n    Clear();\n}\n\nvoid RunningRegression::Clear()\n{\n    x_stats.Clear();\n    y_stats.Clear();\n    S_xy = 0.0;\n    n = 0;\n}\n\nvoid RunningRegression::Push(double x, double y)\n{\n    S_xy += (x_stats.Mean() -x)*(y_stats.Mean() - y)*double(n)/double(n+1);\n    \n    x_stats.Push(x);\n    y_stats.Push(y);\n    n++;\n}\n\nlong long RunningRegression::NumDataValues() const\n{\n    return n;\n}\n\ndouble RunningRegression::Slope() const\n{\n    double S_xx = x_stats.Variance()*(n - 1.0);\n\n    return S_xy / S_xx;\n}\n\ndouble RunningRegression::Intercept() const\n{\n    return y_stats.Mean() - Slope()*x_stats.Mean();\n}\n\ndouble RunningRegression::Correlation() const\n{\n    double t = x_stats.StandardDeviation() * y_stats.StandardDeviation();\n    return S_xy / ( (n-1) * t );\n}\n\nRunningRegression operator+(const RunningRegression a, const RunningRegression b)\n{\n    RunningRegression combined;\n    \n    combined.x_stats = a.x_stats + b.x_stats;\n    combined.y_stats = a.y_stats + b.y_stats;\n    combined.n       = a.n       + b.n;\n    \n    double delta_x = b.x_stats.Mean() - a.x_stats.Mean();\n    double delta_y = b.y_stats.Mean() - a.y_stats.Mean();\n    combined.S_xy = a.S_xy + b.S_xy + \n        double(a.n*b.n)*delta_x*delta_y/double(combined.n);\n    \n    return combined;\n}\n\nRunningRegression& RunningRegression::operator+=(const RunningRegression &rhs)\n{\n\tRunningRegression combined = *this + rhs;\n\t*this = combined;\n\treturn *this;\n}"}], "content": "Computing linear regression in one pass\n\nThe RunningRegression class is the analog of the RunningStats class described here and uses that class. You add pairs of (x, y) values by using the Push. At any point along the way you can call the Slope, Intercept, or Correlation functions to see the current value of these statistics.\n\nYou can also combine two RunningRegression objects by using the + and += operators. For example, you might accrue data on several different threads in parallel then add their RunningRegression objects together.\n\nHere is the header file RunningRegression.h:\n\n#ifndef RUNNINGREGRESSION\n#define RUNNINGREGRESSION\n\n#include \"RunningStats.h\"\n\nclass RunningRegression\n{\npublic:\n    RunningRegression();\n    void Clear();\n    void Push(double x, double y);\n    long long NumDataValues() const;\n    double Slope() const;\n    double Intercept() const;\n    double Correlation() const;\n    \n    friend RunningRegression operator+(\n        const RunningRegression a, const RunningRegression b);\n    RunningRegression& operator+=(const RunningRegression &rhs);\n\nprivate:\n    RunningStats x_stats;\n    RunningStats y_stats;\n    double S_xy;\n    long long n;\n};\n\n#endif\n\nHere is the implementation file RunningRegression.cpp.\n\n#include \"RunningRegression.h\"\n\nRunningRegression::RunningRegression()\n{\n    Clear();\n}\n\nvoid RunningRegression::Clear()\n{\n    x_stats.Clear();\n    y_stats.Clear();\n    S_xy = 0.0;\n    n = 0;\n}\n\nvoid RunningRegression::Push(double x, double y)\n{\n    S_xy += (x_stats.Mean() -x)*(y_stats.Mean() - y)*double(n)/double(n+1);\n    \n    x_stats.Push(x);\n    y_stats.Push(y);\n    n++;\n}\n\nlong long RunningRegression::NumDataValues() const\n{\n    return n;\n}\n\ndouble RunningRegression::Slope() const\n{\n    double S_xx = x_stats.Variance()*(n - 1.0);\n\n    return S_xy / S_xx;\n}\n\ndouble RunningRegression::Intercept() const\n{\n    return y_stats.Mean() - Slope()*x_stats.Mean();\n}\n\ndouble RunningRegression::Correlation() const\n{\n    double t = x_stats.StandardDeviation() * y_stats.StandardDeviation();\n    return S_xy / ( (n-1) * t );\n}\n\nRunningRegression operator+(const RunningRegression a, const RunningRegression b)\n{\n    RunningRegression combined;\n    \n    combined.x_stats = a.x_stats + b.x_stats;\n    combined.y_stats = a.y_stats + b.y_stats;\n    combined.n       = a.n       + b.n;\n    \n    double delta_x = b.x_stats.Mean() - a.x_stats.Mean();\n    double delta_y = b.y_stats.Mean() - a.y_stats.Mean();\n    combined.S_xy = a.S_xy + b.S_xy + \n        double(a.n*b.n)*delta_x*delta_y/double(combined.n);\n    \n    return combined;\n}\n\nRunningRegression& RunningRegression::operator+=(const RunningRegression &rhs)\n{\n\tRunningRegression combined = *this + rhs;\n\t*this = combined;\n\treturn *this;\n}"}
{"slug": "select_with_replacement", "canonical_url": "https://www.johndcook.com/blog/select_with_replacement/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/select_with_replacement.html", "title": "Selecting with replacement", "heading": "Counting selections with replacement", "description": "Counting the number of ways to select k things with replacement from a set of size n.", "summary": "It is well known that the number of ways to select k objects from a set of size n is given by the binomial coefficient", "word_count": 743, "blocks": [{"tag": "h1", "text": "Counting selections with replacement"}, {"tag": "p", "text": "It is well known that the number of ways to select k objects from a set of size n is given by the binomial coefficient"}, {"tag": "p", "text": "Implicit in the above statement is the restriction that an object can only be selected once. But what if we allow an object to be selected multiple times? For example, suppose we want to know how many ways a board of 12 people could select 5 officers. If no one can hold more than one office, the answer is given by the binomial coefficient (12, 5). But what if we allow the possibility that one person could hold more than one office?"}, {"tag": "p", "text": "Richard Stanley introduced the following symbol for the number of ways to select k things from a set of n with replacement."}, {"tag": "p", "text": "There are several ways to think of what this symbol represents. The first is the number of ways to select k things from a set of n with replacement. A more concrete variation on the same idea is the number of ways to place k unlabelled balls into n labeled urns."}, {"tag": "p", "text": "Where the binomial coefficient counts the number of subsets of size k drawn from a set of size n, Stanley's symbol counts the number of multisets of size k than can be drawn from a set of size n. (A multiset is like a set, except elements are allowed to appear more than once.)"}, {"tag": "p", "text": "Another way to view Stanley's symbol is the number of solutions to x1 + x2 + ... + xn = n + k with positive integers. To see this, every xi corresponds to an urn. Initially every xi is is set to 1, because we must have positive integer values. Then we have k 1's to add to the variables, increasing the sum to n+k. The additional 1's to add are analogous to balls."}, {"tag": "p", "text": "As we will demonstrate below, Stanley's symbol has a simple relation to binomial coefficients, namely"}, {"tag": "p", "text": "This may be why Stanley's symbol is not well known: it's always possible to represent values of his symbol in terms of the more familiar symbols. However, the advantage to Stanley's symbol is that it directly represents the problem being solved. It is not at all obvious that the equivalent binomial coefficient is the solution to the combinatorial problem defining Stanley's symbol."}, {"tag": "p", "text": "Here are a couple examples to show how Stanley's symbol comes up in applications. First, suppose people are classified into 6 demographic groups in a survey of 15 people. How many ways could the summary of the survey turn out when you only count the number of people in each group? Stanley's symbol (6, 15). Think of each demographic category as an urn and each person as a ball."}, {"tag": "p", "text": "Next, how many 3rd order partial derivatives does a smooth function of 5 variables have? Think of each variable as an urn, labeled x1 up through x5. We have three unlabeled balls. For every ball an urn gets, we take a partial derivative with respect to that variable. And so the number of distinct partial derivatives is Stanley's symbol (5, 3)."}, {"tag": "p", "text": "(Since the function is smooth, the order of partial derivatives doesn't matter. For example, differentiating with respect to x1 and then with respect to x3 gives the same result as differentiating in the opposite order. Otherwise, we could not have used the analogy with unlabeled balls. The balls would be labeled because the order in which they enter an urn would matter.)"}, {"tag": "p", "text": "Now we establish the equation above by using William Feller's \"stars and bars\" argument given in his book Introduction to Probability."}, {"tag": "p", "text": "Imagine the n urns as the spaces between n+1 vertical bars. Represent the balls as stars between the bars. For example, |*||***| gives an illustration of one way to assign four balls to three urns. There must be a bar in the first and last positions, but otherwise there are as many ways to assign k balls to n urns as there are ways to arrange the stars and bars. There are n-1 bars that we are free to move and k stars, for a total of n + k - 1 symbols. Among the n + k -1 positions for these symbols, we choose k in which to put stars and fill the rest with bars. Thus the number of possibilities is the binomial coefficient (n + k - 1, k) possibilities."}, {"tag": "p", "text": "See also binomial coefficients."}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Counting selections with replacement\n\nIt is well known that the number of ways to select k objects from a set of size n is given by the binomial coefficient\n\nImplicit in the above statement is the restriction that an object can only be selected once. But what if we allow an object to be selected multiple times? For example, suppose we want to know how many ways a board of 12 people could select 5 officers. If no one can hold more than one office, the answer is given by the binomial coefficient (12, 5). But what if we allow the possibility that one person could hold more than one office?\n\nRichard Stanley introduced the following symbol for the number of ways to select k things from a set of n with replacement.\n\nThere are several ways to think of what this symbol represents. The first is the number of ways to select k things from a set of n with replacement. A more concrete variation on the same idea is the number of ways to place k unlabelled balls into n labeled urns.\n\nWhere the binomial coefficient counts the number of subsets of size k drawn from a set of size n, Stanley's symbol counts the number of multisets of size k than can be drawn from a set of size n. (A multiset is like a set, except elements are allowed to appear more than once.)\n\nAnother way to view Stanley's symbol is the number of solutions to x1 + x2 + ... + xn = n + k with positive integers. To see this, every xi corresponds to an urn. Initially every xi is is set to 1, because we must have positive integer values. Then we have k 1's to add to the variables, increasing the sum to n+k. The additional 1's to add are analogous to balls.\n\nAs we will demonstrate below, Stanley's symbol has a simple relation to binomial coefficients, namely\n\nThis may be why Stanley's symbol is not well known: it's always possible to represent values of his symbol in terms of the more familiar symbols. However, the advantage to Stanley's symbol is that it directly represents the problem being solved. It is not at all obvious that the equivalent binomial coefficient is the solution to the combinatorial problem defining Stanley's symbol.\n\nHere are a couple examples to show how Stanley's symbol comes up in applications. First, suppose people are classified into 6 demographic groups in a survey of 15 people. How many ways could the summary of the survey turn out when you only count the number of people in each group? Stanley's symbol (6, 15). Think of each demographic category as an urn and each person as a ball.\n\nNext, how many 3rd order partial derivatives does a smooth function of 5 variables have? Think of each variable as an urn, labeled x1 up through x5. We have three unlabeled balls. For every ball an urn gets, we take a partial derivative with respect to that variable. And so the number of distinct partial derivatives is Stanley's symbol (5, 3).\n\n(Since the function is smooth, the order of partial derivatives doesn't matter. For example, differentiating with respect to x1 and then with respect to x3 gives the same result as differentiating in the opposite order. Otherwise, we could not have used the analogy with unlabeled balls. The balls would be labeled because the order in which they enter an urn would matter.)\n\nNow we establish the equation above by using William Feller's \"stars and bars\" argument given in his book Introduction to Probability.\n\nImagine the n urns as the spaces between n+1 vertical bars. Represent the balls as stars between the bars. For example, |*||***| gives an illustration of one way to assign four balls to three urns. There must be a bar in the first and last positions, but otherwise there are as many ways to assign k balls to n urns as there are ways to arrange the stars and bars. There are n-1 bars that we are free to move and k stars, for a total of n + k - 1 symbols. Among the n + k -1 positions for these symbols, we choose k in which to put stars and fill the rest with bars. Thus the number of possibilities is the binomial coefficient (n + k - 1, k) possibilities.\n\nSee also binomial coefficients.\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "skewness_kurtosis", "canonical_url": "https://www.johndcook.com/blog/skewness_kurtosis/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/skewness_kurtosis.html", "title": "Computing skewness and kurtosis in one pass", "heading": "Computing skewness and kurtosis in one pass", "description": "How to accurately compute mean, variance, standard deviation, skewness, and kurtosis in one pass through your data.", "summary": "The code is an extension of the method of Knuth and Welford for computing standard deviation in one pass through the data. It computes skewness and kurtosis as well with a similar interface. In addition to only requiring one pass through the data, the algorithm is numerically stable and accurate.", "word_count": 547, "blocks": [{"tag": "h1", "text": "Computing skewness and kurtosis in one pass"}, {"tag": "p", "text": "The code is an extension of the method of Knuth and Welford for computing standard deviation in one pass through the data. It computes skewness and kurtosis as well with a similar interface. In addition to only requiring one pass through the data, the algorithm is numerically stable and accurate."}, {"tag": "p", "text": "To use the code, create a RunningStats object and use the Push method to insert data values. After accruing your data, you can obtain sample statistics by calling one of these methods:"}, {"tag": "li", "text": "Mean"}, {"tag": "li", "text": "Variance"}, {"tag": "li", "text": "StandardDeviation"}, {"tag": "li", "text": "Skewness"}, {"tag": "li", "text": "Kurtosis"}, {"tag": "p", "text": "You can also combine two RunningStats objects by using the + and += operators. For example, you might accrue data on several different threads in parallel then add their RunningStats objects together to create a single object with the state that it would have had if all the data had been accumulated by it alone."}, {"tag": "p", "text": "Update: Here is analogous code for computing a simple linear regression on data in one pass."}, {"tag": "p", "text": "Here is the header file RunningStats.h:"}, {"tag": "pre", "text": "#ifndef RUNNINGSTATS_H\n#define RUNNINGSTATS_H\n\nclass RunningStats\n{\npublic:\n    RunningStats();\n    void Clear();\n    void Push(double x);\n    long long NumDataValues() const;\n    double Mean() const;\n    double Variance() const;\n    double StandardDeviation() const;\n    double Skewness() const;\n    double Kurtosis() const;\n\n    friend RunningStats operator+(const RunningStats a, const RunningStats b);\n    RunningStats& operator+=(const RunningStats &rhs);\n\nprivate:\n    long long n;\n    double M1, M2, M3, M4;\n};\n\n#endif"}, {"tag": "p", "text": "And here is the implementation file RunningStats.cpp."}, {"tag": "pre", "text": "#include \"RunningStats.h\"\n#include <cmath>\n#include <vector>\n\nRunningStats::RunningStats() \n{\n    Clear();\n}\n\nvoid RunningStats::Clear()\n{\n    n = 0;\n    M1 = M2 = M3 = M4 = 0.0;\n}\n\nvoid RunningStats::Push(double x)\n{\n    double delta, delta_n, delta_n2, term1;\n\n    long long n1 = n;\n    n++;\n    delta = x - M1;\n    delta_n = delta / n;\n    delta_n2 = delta_n * delta_n;\n    term1 = delta * delta_n * n1;\n    M1 += delta_n;\n    M4 += term1 * delta_n2 * (n*n - 3*n + 3) + 6 * delta_n2 * M2 - 4 * delta_n * M3;\n    M3 += term1 * delta_n * (n - 2) - 3 * delta_n * M2;\n    M2 += term1;\n}\n\nlong long RunningStats::NumDataValues() const\n{\n    return n;\n}\n\ndouble RunningStats::Mean() const\n{\n    return M1;\n}\n\ndouble RunningStats::Variance() const\n{\n    return M2/(n-1.0);\n}\n\ndouble RunningStats::StandardDeviation() const\n{\n    return sqrt( Variance() );\n}\n\ndouble RunningStats::Skewness() const\n{\n    return sqrt(double(n)) * M3/ pow(M2, 1.5);\n}\n\ndouble RunningStats::Kurtosis() const\n{\n    return double(n)*M4 / (M2*M2) - 3.0;\n}\n\nRunningStats operator+(const RunningStats a, const RunningStats b)\n{\n    RunningStats combined;\n    \n    combined.n = a.n + b.n;\n    \n    double delta = b.M1 - a.M1;\n    double delta2 = delta*delta;\n    double delta3 = delta*delta2;\n    double delta4 = delta2*delta2;\n    \n    combined.M1 = (a.n*a.M1 + b.n*b.M1) / combined.n;\n    \n    combined.M2 = a.M2 + b.M2 + \n                  delta2 * a.n * b.n / combined.n;\n    \n    combined.M3 = a.M3 + b.M3 + \n                  delta3 * a.n * b.n * (a.n - b.n)/(combined.n*combined.n);\n    combined.M3 += 3.0*delta * (a.n*b.M2 - b.n*a.M2) / combined.n;\n    \n    combined.M4 = a.M4 + b.M4 + delta4*a.n*b.n * (a.n*a.n - a.n*b.n + b.n*b.n) / \n                  (combined.n*combined.n*combined.n);\n    combined.M4 += 6.0*delta2 * (a.n*a.n*b.M2 + b.n*b.n*a.M2)/(combined.n*combined.n) + \n                  4.0*delta*(a.n*b.M3 - b.n*a.M3) / combined.n;\n    \n    return combined;\n}\n\nRunningStats& RunningStats::operator+=(const RunningStats& rhs)\n{ \n\tRunningStats combined = *this + rhs;\n\t*this = combined;\n\treturn *this;\n}"}, {"tag": "p", "text": "References:"}, {"tag": "li", "text": "Wikipedia"}, {"tag": "li", "text": "Timothy B. Terriberry. Computing Higher-Order Moments Online."}, {"tag": "li", "text": "Philippe Pébay. SANDIA REPORT SAND2008-6212 (2008). Formulas for Robust, One-Pass Parallel Computation of Co-variances and Arbitrary-Order Statistical Moments."}], "content": "Computing skewness and kurtosis in one pass\n\nThe code is an extension of the method of Knuth and Welford for computing standard deviation in one pass through the data. It computes skewness and kurtosis as well with a similar interface. In addition to only requiring one pass through the data, the algorithm is numerically stable and accurate.\n\nTo use the code, create a RunningStats object and use the Push method to insert data values. After accruing your data, you can obtain sample statistics by calling one of these methods:\n\nMean\n\nVariance\n\nStandardDeviation\n\nSkewness\n\nKurtosis\n\nYou can also combine two RunningStats objects by using the + and += operators. For example, you might accrue data on several different threads in parallel then add their RunningStats objects together to create a single object with the state that it would have had if all the data had been accumulated by it alone.\n\nUpdate: Here is analogous code for computing a simple linear regression on data in one pass.\n\nHere is the header file RunningStats.h:\n\n#ifndef RUNNINGSTATS_H\n#define RUNNINGSTATS_H\n\nclass RunningStats\n{\npublic:\n    RunningStats();\n    void Clear();\n    void Push(double x);\n    long long NumDataValues() const;\n    double Mean() const;\n    double Variance() const;\n    double StandardDeviation() const;\n    double Skewness() const;\n    double Kurtosis() const;\n\n    friend RunningStats operator+(const RunningStats a, const RunningStats b);\n    RunningStats& operator+=(const RunningStats &rhs);\n\nprivate:\n    long long n;\n    double M1, M2, M3, M4;\n};\n\n#endif\n\nAnd here is the implementation file RunningStats.cpp.\n\n#include \"RunningStats.h\"\n#include <cmath>\n#include <vector>\n\nRunningStats::RunningStats() \n{\n    Clear();\n}\n\nvoid RunningStats::Clear()\n{\n    n = 0;\n    M1 = M2 = M3 = M4 = 0.0;\n}\n\nvoid RunningStats::Push(double x)\n{\n    double delta, delta_n, delta_n2, term1;\n\n    long long n1 = n;\n    n++;\n    delta = x - M1;\n    delta_n = delta / n;\n    delta_n2 = delta_n * delta_n;\n    term1 = delta * delta_n * n1;\n    M1 += delta_n;\n    M4 += term1 * delta_n2 * (n*n - 3*n + 3) + 6 * delta_n2 * M2 - 4 * delta_n * M3;\n    M3 += term1 * delta_n * (n - 2) - 3 * delta_n * M2;\n    M2 += term1;\n}\n\nlong long RunningStats::NumDataValues() const\n{\n    return n;\n}\n\ndouble RunningStats::Mean() const\n{\n    return M1;\n}\n\ndouble RunningStats::Variance() const\n{\n    return M2/(n-1.0);\n}\n\ndouble RunningStats::StandardDeviation() const\n{\n    return sqrt( Variance() );\n}\n\ndouble RunningStats::Skewness() const\n{\n    return sqrt(double(n)) * M3/ pow(M2, 1.5);\n}\n\ndouble RunningStats::Kurtosis() const\n{\n    return double(n)*M4 / (M2*M2) - 3.0;\n}\n\nRunningStats operator+(const RunningStats a, const RunningStats b)\n{\n    RunningStats combined;\n    \n    combined.n = a.n + b.n;\n    \n    double delta = b.M1 - a.M1;\n    double delta2 = delta*delta;\n    double delta3 = delta*delta2;\n    double delta4 = delta2*delta2;\n    \n    combined.M1 = (a.n*a.M1 + b.n*b.M1) / combined.n;\n    \n    combined.M2 = a.M2 + b.M2 + \n                  delta2 * a.n * b.n / combined.n;\n    \n    combined.M3 = a.M3 + b.M3 + \n                  delta3 * a.n * b.n * (a.n - b.n)/(combined.n*combined.n);\n    combined.M3 += 3.0*delta * (a.n*b.M2 - b.n*a.M2) / combined.n;\n    \n    combined.M4 = a.M4 + b.M4 + delta4*a.n*b.n * (a.n*a.n - a.n*b.n + b.n*b.n) / \n                  (combined.n*combined.n*combined.n);\n    combined.M4 += 6.0*delta2 * (a.n*a.n*b.M2 + b.n*b.n*a.M2)/(combined.n*combined.n) + \n                  4.0*delta*(a.n*b.M3 - b.n*a.M3) / combined.n;\n    \n    return combined;\n}\n\nRunningStats& RunningStats::operator+=(const RunningStats& rhs)\n{ \n\tRunningStats combined = *this + rhs;\n\t*this = combined;\n\treturn *this;\n}\n\nReferences:\n\nWikipedia\n\nTimothy B. Terriberry. Computing Higher-Order Moments Online.\n\nPhilippe Pébay. SANDIA REPORT SAND2008-6212 (2008). Formulas for Robust, One-Pass Parallel Computation of Co-variances and Arbitrary-Order Statistical Moments."}
{"slug": "software", "canonical_url": "https://www.johndcook.com/blog/software/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/software.html", "title": "Software", "heading": "Software", "description": "Software developed in part by John D. Cook for Bayesian statistical methods in phase I and phase II clinical trials", "summary": "This page lists some software I've written or written about.", "word_count": 379, "blocks": [{"tag": "h1", "text": "Software"}, {"tag": "p", "text": "This page lists some software I've written or written about."}, {"tag": "p", "text": "Stand-alone numerical software Clinical trial software Statistical utilities PowerShell R Miscellaneous My blog"}, {"tag": "h2", "text": "Stand-alone numerical software"}, {"tag": "p", "text": "Software for numerical computing, primarily special functions and random number generation. Code available in C++, C#, and Python."}, {"tag": "h2", "text": "Clinical trial software"}, {"tag": "p", "text": "CRMSimulator is used to design CRM trials, dose-finding based only on toxicity outcomes. BMA-CRMSimulator is a variation on CRMSimulator using Bayesian model averaging."}, {"tag": "p", "text": "EffTox is used for dose-finding based on toxicity and efficacy outcomes."}, {"tag": "p", "text": "TTEConduct and TTEDesigner are for safety monitoring of single-arm trials with time-to-event outcomes."}, {"tag": "p", "text": "Multc Lean monitors two binary outcomes, efficacy and toxicity."}, {"tag": "p", "text": "Adaptive Randomization is for multiple-arm trials using outcome-adaptive randomization."}, {"tag": "h2", "text": "Statistical utilities"}, {"tag": "p", "text": "I have worked a lot with on random inequalities, and Inequality Calculator often comes in handy. Parameter Solver is also popular, used for determining distribution parameters based on quantiles or mean and variance."}, {"tag": "p", "text": "For more software from M. D. Anderson, see the statistical software [link died] download site or search this site using the custom search engine below."}, {"tag": "h2", "text": "PowerShell"}, {"tag": "p", "text": "PowerShell is a new shell for Windows. After decades of having a wimpy command line compared to Unix, Microsoft produced a shell more powerful than its Unix counterparts. The biggest difference is that while Unix shells pipe text, PowerShell pipes objects."}, {"tag": "p", "text": "PowerShell gotchas lists the top five features a new users might find frustrating and gives a justification for each."}, {"tag": "p", "text": "PowerShell Cookbook is a set of things I found useful while learning to use PowerShell."}, {"tag": "p", "text": "Regular Expressions in PowerShell and Perl explains how to use regular expressions in PowerShell, comparing PowerShell's syntax to Perl's syntax. A similar page compares Regular Expressions in Python and Perl."}, {"tag": "p", "text": "Learn PowerShell as a shell first"}, {"tag": "p", "text": "Automated Extract and Build from Team System using PowerShell article and source code on CodeProject."}, {"tag": "p", "text": "PowerShell script to report catch blocks"}, {"tag": "h2", "text": "R"}, {"tag": "p", "text": "R for programmers"}, {"tag": "p", "text": "Five kinds of subscripts in R"}, {"tag": "p", "text": "Moving data between R, Excel, and the Windows clipboard"}, {"tag": "h2", "text": "Miscellaneous"}, {"tag": "p", "text": "Notes on how to use LaTeX on Windows."}, {"tag": "p", "text": "Unicode resources"}, {"tag": "p", "text": "Getting started with C++ TR1 regular expressions"}, {"tag": "p", "text": "Comparing various implementations of math.h"}, {"tag": "p", "text": "Greek letters and math symbols in Unicode, (X)HTML, and LaTeX"}, {"tag": "p", "text": "Accented letters in HTML, (La)TeX, and Microsoft Word"}, {"tag": "p", "text": "Working with statistical distributions in Mathematica, R/S-PLUS, and Python"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Software\n\nThis page lists some software I've written or written about.\n\nStand-alone numerical software Clinical trial software Statistical utilities PowerShell R Miscellaneous My blog\n\nStand-alone numerical software\n\nSoftware for numerical computing, primarily special functions and random number generation. Code available in C++, C#, and Python.\n\nClinical trial software\n\nCRMSimulator is used to design CRM trials, dose-finding based only on toxicity outcomes. BMA-CRMSimulator is a variation on CRMSimulator using Bayesian model averaging.\n\nEffTox is used for dose-finding based on toxicity and efficacy outcomes.\n\nTTEConduct and TTEDesigner are for safety monitoring of single-arm trials with time-to-event outcomes.\n\nMultc Lean monitors two binary outcomes, efficacy and toxicity.\n\nAdaptive Randomization is for multiple-arm trials using outcome-adaptive randomization.\n\nStatistical utilities\n\nI have worked a lot with on random inequalities, and Inequality Calculator often comes in handy. Parameter Solver is also popular, used for determining distribution parameters based on quantiles or mean and variance.\n\nFor more software from M. D. Anderson, see the statistical software [link died] download site or search this site using the custom search engine below.\n\nPowerShell\n\nPowerShell is a new shell for Windows. After decades of having a wimpy command line compared to Unix, Microsoft produced a shell more powerful than its Unix counterparts. The biggest difference is that while Unix shells pipe text, PowerShell pipes objects.\n\nPowerShell gotchas lists the top five features a new users might find frustrating and gives a justification for each.\n\nPowerShell Cookbook is a set of things I found useful while learning to use PowerShell.\n\nRegular Expressions in PowerShell and Perl explains how to use regular expressions in PowerShell, comparing PowerShell's syntax to Perl's syntax. A similar page compares Regular Expressions in Python and Perl.\n\nLearn PowerShell as a shell first\n\nAutomated Extract and Build from Team System using PowerShell article and source code on CodeProject.\n\nPowerShell script to report catch blocks\n\nR\n\nR for programmers\n\nFive kinds of subscripts in R\n\nMoving data between R, Excel, and the Windows clipboard\n\nMiscellaneous\n\nNotes on how to use LaTeX on Windows.\n\nUnicode resources\n\nGetting started with C++ TR1 regular expressions\n\nComparing various implementations of math.h\n\nGreek letters and math symbols in Unicode, (X)HTML, and LaTeX\n\nAccented letters in HTML, (La)TeX, and Microsoft Word\n\nWorking with statistical distributions in Mathematica, R/S-PLUS, and Python\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "special_function_diagram", "canonical_url": "https://www.johndcook.com/blog/special_function_diagram/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/special_function_diagram.html", "title": "Relations between special functions", "heading": "Relations between special functions", "description": "A diagram of special functions showing how the functions are related.", "summary": "The diagram below illustrates the relationships between many common special functions.", "word_count": 364, "blocks": [{"tag": "h1", "text": "Relations between special functions"}, {"tag": "p", "text": "The diagram below illustrates the relationships between many common special functions."}, {"tag": "p", "text": "This page is a work in progress. Please contact me with corrections or suggestions."}, {"tag": "p", "text": "The hypergeometric functon 2F1 has many special cases. (See these notes on hypergeometric functions for definitions and notation.)"}, {"tag": "p", "text": "The Jacobi polynomials are related to 2F1 by"}, {"tag": "p", "text": "Jacobi polynomials with α = β are called Gegenbauer or ultraspherical polynomials and are denoted C(α)n. The Legendre polynomials Pn are Gegenbauer polynomials with α = 0. The Chebyshev polynomials of the first kind Tn are Gegenbauer polynomials with α = -½. The Chebyshev polynomials of the second kind Un are Gegenbauer polynomials with α = ½."}, {"tag": "p", "text": "The incomplete beta function Bx(a, b) is related to 2F1(x) by"}, {"tag": "p", "text": "The beta function B(a,b) is defined to be Γ(a) Γ(b) / Γ(a+b) and equals B1(a, b)."}, {"tag": "p", "text": "The complete elliptic integrals K(z) and E(z) are related to 2F1 by"}, {"tag": "p", "text": "The complete elliptic integrals are the values of the incomplete elliptic integrals F(φ, z) and E(φ, z) at φ = φ/2."}, {"tag": "p", "text": "The inverse of the integral F(φ, z) is the Jacobi elliptic function sn. The Jacobi functions sn, cn, and dn are intimately related, much like the elementary trigonometric functions."}, {"tag": "p", "text": "The hypergeometric functon 1F1 is also known as the confluent hypergeometric function. It is related to the hypergeometric function 2F1 by"}, {"tag": "p", "text": "The incomplete gamma function γ(a, z) is related to 1F1 by"}, {"tag": "p", "text": "The limit of γ(a, z) as z goes to infinity is the gamma function Γ(a)."}, {"tag": "p", "text": "The derivative of the logarithm of the gamma function Γ(z) is the function ψ(z)."}, {"tag": "p", "text": "The error function erf(z) is related to 1F1 by"}, {"tag": "p", "text": "The error function erf(z) is related to the Fresnel integrals C(z) and S(z) by"}, {"tag": "p", "text": "The Laguerre polynomials Lαn are related to 1F1 by"}, {"tag": "p", "text": "The Hermite polynomials are related to the Laguerre polynomials by"}, {"tag": "p", "text": "The hypergeometric functon 0F1 is related to the hypergeometric function 1F1 by"}, {"tag": "p", "text": "Only Bessel functions of the first kind Jν are shown on the diagram. Other Bessel functions are related to Jν as described here."}, {"tag": "h2", "text": "Other diagrams on this site"}, {"tag": "p", "text": "See this page for more diagrams on this site including diagrams for probability and statistics, analysis, topology, and category theory."}], "content": "Relations between special functions\n\nThe diagram below illustrates the relationships between many common special functions.\n\nThis page is a work in progress. Please contact me with corrections or suggestions.\n\nThe hypergeometric functon 2F1 has many special cases. (See these notes on hypergeometric functions for definitions and notation.)\n\nThe Jacobi polynomials are related to 2F1 by\n\nJacobi polynomials with α = β are called Gegenbauer or ultraspherical polynomials and are denoted C(α)n. The Legendre polynomials Pn are Gegenbauer polynomials with α = 0. The Chebyshev polynomials of the first kind Tn are Gegenbauer polynomials with α = -½. The Chebyshev polynomials of the second kind Un are Gegenbauer polynomials with α = ½.\n\nThe incomplete beta function Bx(a, b) is related to 2F1(x) by\n\nThe beta function B(a,b) is defined to be Γ(a) Γ(b) / Γ(a+b) and equals B1(a, b).\n\nThe complete elliptic integrals K(z) and E(z) are related to 2F1 by\n\nThe complete elliptic integrals are the values of the incomplete elliptic integrals F(φ, z) and E(φ, z) at φ = φ/2.\n\nThe inverse of the integral F(φ, z) is the Jacobi elliptic function sn. The Jacobi functions sn, cn, and dn are intimately related, much like the elementary trigonometric functions.\n\nThe hypergeometric functon 1F1 is also known as the confluent hypergeometric function. It is related to the hypergeometric function 2F1 by\n\nThe incomplete gamma function γ(a, z) is related to 1F1 by\n\nThe limit of γ(a, z) as z goes to infinity is the gamma function Γ(a).\n\nThe derivative of the logarithm of the gamma function Γ(z) is the function ψ(z).\n\nThe error function erf(z) is related to 1F1 by\n\nThe error function erf(z) is related to the Fresnel integrals C(z) and S(z) by\n\nThe Laguerre polynomials Lαn are related to 1F1 by\n\nThe Hermite polynomials are related to the Laguerre polynomials by\n\nThe hypergeometric functon 0F1 is related to the hypergeometric function 1F1 by\n\nOnly Bessel functions of the first kind Jν are shown on the diagram. Other Bessel functions are related to Jν as described here.\n\nOther diagrams on this site\n\nSee this page for more diagrams on this site including diagrams for probability and statistics, analysis, topology, and category theory."}
{"slug": "spherical_trigonometry", "canonical_url": "https://www.johndcook.com/blog/spherical_trigonometry/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/spherical_trigonometry.html", "title": "Spherical Trigonometry", "heading": "Notes on Spherical Trigonometry", "description": "Notes on spherical trigonometry, solving trigonometry problems on a sphere.", "summary": "Spherical trigonometry is the study of curved triangles, triangles drawn on the surface of a sphere. The subject is practical, for example, because we live on a sphere. The subject has numerous elegant and unexpected theorems. We give a few below.", "word_count": 557, "blocks": [{"tag": "h1", "text": "Notes on Spherical Trigonometry"}, {"tag": "p", "text": "Spherical trigonometry is the study of curved triangles, triangles drawn on the surface of a sphere. The subject is practical, for example, because we live on a sphere. The subject has numerous elegant and unexpected theorems. We give a few below."}, {"tag": "p", "text": "The diagram shows the spherical triangle with vertices A, B, and C. The angles at each vertex are denoted with Greek letters α, β, and γ. The arcs forming the sides of the triangle are labeled by the lower-case form of the letter labeling the opposite vertex."}, {"tag": "h2", "text": "Basic properties"}, {"tag": "p", "text": "On the plane, the sum of the interior angles of any triangle is exactly 180°. On a sphere, however, the corresponding sum is always greater than 180° but also less than 540°. That is, 180° < α + β + γ < 540° in the diagram above. The positive quantity E = α + β + γ - 180° is called the spherical excess of the triangle."}, {"tag": "p", "text": "Since the sides of a spherical triangle are arcs, they can be described as angles, and so we have two kinds of angles:"}, {"tag": "li", "text": "The angles at the vertices of the triangle, formed by the great circles intersecting at the vertices and denoted by Greek letters."}, {"tag": "li", "text": "The sides of the triangle, measured by the angle formed by the lines connecting the vertices to the center of the sphere and denoted by lower-case Roman letters."}, {"tag": "p", "text": "The second kind of angle is most interesting. In contrast to plane trigonometry, the sides of a spherical triangle are themselves are angles, and so we can take sines and cosines etc. of the sides as well as the vertex angles."}, {"tag": "h2", "text": "Right spherical triangles"}, {"tag": "p", "text": "For this section, assume the angle γ = 90°, i.e. we have a spherical right triangle. Then the following identities hold."}, {"tag": "li", "text": "sin a = sin α sin c = tan b cot β"}, {"tag": "li", "text": "sin b = sin β sin c = tan a cot α"}, {"tag": "li", "text": "cos α = cos a sin β = tan b cot c"}, {"tag": "li", "text": "cos β = cos b sin α = tan a cot c"}, {"tag": "li", "text": "cos c = cot α cot β = cos a cos b"}, {"tag": "p", "text": "Napier's rule is a mnemonic for memorizing the above identities."}, {"tag": "h2", "text": "General spherical triangle"}, {"tag": "p", "text": "For this section we drop the assumption that γ = 90°. Many identities hold. Here are a few examples."}, {"tag": "h3", "text": "Law of sines"}, {"tag": "p", "text": "sin α / sin a = sin β / sin b = sin γ / sin c"}, {"tag": "h3", "text": "Law of cosines"}, {"tag": "p", "text": "cos a = cos b cos c + sin b sin c cos α"}, {"tag": "p", "text": "cos α = -cos β cos γ + sin β sin γ cos a"}, {"tag": "h3", "text": "Tangents"}, {"tag": "p", "text": "Let s = (a + b + c)/2 and let σ = (α + β + γ)/2. The following formulas can used to solve for a vertex angle from knowing the side arcs or solve for a side arc from knowing the vertex angle."}, {"tag": "p", "text": "tan (α/2) = sqrt( sin(s-b) sin(s-c) / sin s sin(s-a) )"}, {"tag": "p", "text": "tan (a /2) = sqrt( -cos(σ) cos(σ-α) / cos(σ-β) cos(σ-γ) )"}, {"tag": "h3", "text": "Area"}, {"tag": "p", "text": "Let R be the radius of the sphere on which a triangle resides. If angles are measured in radians, the area of a triangle is simply R2E where E is the spherical excess, defined above. In degrees the formula for area is πR2E/180."}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Notes on Spherical Trigonometry\n\nSpherical trigonometry is the study of curved triangles, triangles drawn on the surface of a sphere. The subject is practical, for example, because we live on a sphere. The subject has numerous elegant and unexpected theorems. We give a few below.\n\nThe diagram shows the spherical triangle with vertices A, B, and C. The angles at each vertex are denoted with Greek letters α, β, and γ. The arcs forming the sides of the triangle are labeled by the lower-case form of the letter labeling the opposite vertex.\n\nBasic properties\n\nOn the plane, the sum of the interior angles of any triangle is exactly 180°. On a sphere, however, the corresponding sum is always greater than 180° but also less than 540°. That is, 180° < α + β + γ < 540° in the diagram above. The positive quantity E = α + β + γ - 180° is called the spherical excess of the triangle.\n\nSince the sides of a spherical triangle are arcs, they can be described as angles, and so we have two kinds of angles:\n\nThe angles at the vertices of the triangle, formed by the great circles intersecting at the vertices and denoted by Greek letters.\n\nThe sides of the triangle, measured by the angle formed by the lines connecting the vertices to the center of the sphere and denoted by lower-case Roman letters.\n\nThe second kind of angle is most interesting. In contrast to plane trigonometry, the sides of a spherical triangle are themselves are angles, and so we can take sines and cosines etc. of the sides as well as the vertex angles.\n\nRight spherical triangles\n\nFor this section, assume the angle γ = 90°, i.e. we have a spherical right triangle. Then the following identities hold.\n\nsin a = sin α sin c = tan b cot β\n\nsin b = sin β sin c = tan a cot α\n\ncos α = cos a sin β = tan b cot c\n\ncos β = cos b sin α = tan a cot c\n\ncos c = cot α cot β = cos a cos b\n\nNapier's rule is a mnemonic for memorizing the above identities.\n\nGeneral spherical triangle\n\nFor this section we drop the assumption that γ = 90°. Many identities hold. Here are a few examples.\n\nLaw of sines\n\nsin α / sin a = sin β / sin b = sin γ / sin c\n\nLaw of cosines\n\ncos a = cos b cos c + sin b sin c cos α\n\ncos α = -cos β cos γ + sin β sin γ cos a\n\nTangents\n\nLet s = (a + b + c)/2 and let σ = (α + β + γ)/2. The following formulas can used to solve for a vertex angle from knowing the side arcs or solve for a side arc from knowing the vertex angle.\n\ntan (α/2) = sqrt( sin(s-b) sin(s-c) / sin s sin(s-a) )\n\ntan (a /2) = sqrt( -cos(σ) cos(σ-α) / cos(σ-β) cos(σ-γ) )\n\nArea\n\nLet R be the radius of the sphere on which a triangle resides. If angles are measured in radians, the area of a triangle is simply R2E where E is the spherical excess, defined above. In degrees the formula for area is πR2E/180.\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "stand_alone_code", "canonical_url": "https://www.johndcook.com/blog/stand_alone_code/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/stand_alone_code.html", "title": "Stand-alone code for numerical computing", "heading": "Stand-alone code for numerical computing", "description": "Stand-alone code for scientific computing. Software snippets designed for cut-and-paste with minimal dependencies.", "summary": "Sometimes you need a little code to do some scientific computing and you don't want to take on a dependence to a huge and possibly unfamiliar library. This page is an index to code snippets in multiple languages that solve that problem.", "word_count": 261, "blocks": [{"tag": "h1", "text": "Stand-alone code for numerical computing"}, {"tag": "p", "text": "Sometimes you need a little code to do some scientific computing and you don't want to take on a dependence to a huge and possibly unfamiliar library. This page is an index to code snippets in multiple languages that solve that problem."}, {"tag": "p", "text": "The software listed here has been designed to minimize dependencies, minimize complexity, and maximize transparency. That sometimes means sacrificing efficiency."}, {"tag": "p", "text": "All code here is in the public domain. Do whatever you want with it, no strings attached. Use at your own risk."}, {"tag": "p", "text": "I'm just getting started with this project. I intend to add functions and programming languages over time. Feedback is always appreciated."}, {"tag": "h2", "text": "Special functions"}, {"tag": "p", "text": "Error function: C++, Python, C#"}, {"tag": "p", "text": "Phi (standard normal CDF): C++, Python, C#"}, {"tag": "p", "text": "Phi inverse: C++, Python, C#"}, {"tag": "p", "text": "Gamma: C++, C#"}, {"tag": "p", "text": "Log Gamma: C++, C#"}, {"tag": "p", "text": "log(1 + x) for small x: C++, Python, C#"}, {"tag": "p", "text": "exp(x) - 1 for small x: C++, Python, C#"}, {"tag": "p", "text": "log( n! ): C++, C#"}, {"tag": "h2", "text": "Random number generation"}, {"tag": "p", "text": "Beta: C++, Python,"}, {"tag": "p", "text": "Cauchy: C++, Python,"}, {"tag": "p", "text": "Chi-square: C++,"}, {"tag": "p", "text": "Exponential: C++, Python,"}, {"tag": "p", "text": "Gamma: C++, Python,"}, {"tag": "p", "text": "Inverse gamma: C++,"}, {"tag": "p", "text": "Laplace: C++,"}, {"tag": "p", "text": "Log normal: C++,"}, {"tag": "p", "text": "Normal: C++, Python,"}, {"tag": "p", "text": "Poisson: C++"}, {"tag": "p", "text": "Student-t: C++, Python,"}, {"tag": "p", "text": "Uniform: C++,"}, {"tag": "p", "text": "Weibull: C++,"}, {"tag": "p", "text": "C++ TR1 has code for generating random samples from normal, exponential, gamma, and Poisson distributions directly. Random number generation using C++ TR1 explains how to use this built-in functionality and now to bootstrap the built-in functions to generate samples from Cauchy, Student-t, Snedecor-F, and Weibull distributions."}, {"tag": "p", "text": "See this page for Non-uniform random number generation in Julia."}, {"tag": "h2", "text": "Miscellaneous"}, {"tag": "p", "text": "Distance based on longitude and latitude: Python, F#"}], "content": "Stand-alone code for numerical computing\n\nSometimes you need a little code to do some scientific computing and you don't want to take on a dependence to a huge and possibly unfamiliar library. This page is an index to code snippets in multiple languages that solve that problem.\n\nThe software listed here has been designed to minimize dependencies, minimize complexity, and maximize transparency. That sometimes means sacrificing efficiency.\n\nAll code here is in the public domain. Do whatever you want with it, no strings attached. Use at your own risk.\n\nI'm just getting started with this project. I intend to add functions and programming languages over time. Feedback is always appreciated.\n\nSpecial functions\n\nError function: C++, Python, C#\n\nPhi (standard normal CDF): C++, Python, C#\n\nPhi inverse: C++, Python, C#\n\nGamma: C++, C#\n\nLog Gamma: C++, C#\n\nlog(1 + x) for small x: C++, Python, C#\n\nexp(x) - 1 for small x: C++, Python, C#\n\nlog( n! ): C++, C#\n\nRandom number generation\n\nBeta: C++, Python,\n\nCauchy: C++, Python,\n\nChi-square: C++,\n\nExponential: C++, Python,\n\nGamma: C++, Python,\n\nInverse gamma: C++,\n\nLaplace: C++,\n\nLog normal: C++,\n\nNormal: C++, Python,\n\nPoisson: C++\n\nStudent-t: C++, Python,\n\nUniform: C++,\n\nWeibull: C++,\n\nC++ TR1 has code for generating random samples from normal, exponential, gamma, and Poisson distributions directly. Random number generation using C++ TR1 explains how to use this built-in functionality and now to bootstrap the built-in functions to generate samples from Cauchy, Student-t, Snedecor-F, and Weibull distributions.\n\nSee this page for Non-uniform random number generation in Julia.\n\nMiscellaneous\n\nDistance based on longitude and latitude: Python, F#"}
{"slug": "test_TR1_random", "canonical_url": "https://www.johndcook.com/blog/test_TR1_random/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/test_TR1_random.html", "title": "Random number generation using C++ TR1", "heading": "Testing C++ TR1 random number generation", "description": "Testing the means of random values generated using C++ TR1 extensions. Performs a crude statistical test and serves as sample code.", "summary": "Before you use a random number generation library, it's a good idea to do a few tests. The tests might uncover a flaw in the library, but more likely they'll uncover a flaw in your understanding of the library. Parameterization expectations in particular are a common source of errors.", "word_count": 370, "blocks": [{"tag": "h1", "text": "Testing C++ TR1 random number generation"}, {"tag": "p", "text": "Before you use a random number generation library, it's a good idea to do a few tests. The tests might uncover a flaw in the library, but more likely they'll uncover a flaw in your understanding of the library. Parameterization expectations in particular are a common source of errors."}, {"tag": "p", "text": "The following code does a crude test of the C++ TR1 random number generation classes. It computes the mean of 10,000 samples from each of the supported distributions and does a normal approximation to see whether the sample mean is roughly what one would expect. Don't be surprised if values fall outside the expected interval. But if you run the code several times with different RNG seeds, you shouldn't expect the same sample to go outside its predicted range very often."}, {"tag": "p", "text": "This code is hardly a rigorous test of the distributions. But it provides sample code for working with the distributions and tests your understanding of the parameterizations."}, {"tag": "pre", "text": "#include <random>\n#include <iostream>\n#include <string>\n\ntemplate <typename TDistribution>\ndouble sample_mean(TDistribution dist, int sample_size = 10000)\n{\n    std::tr1::mt19937 mt; // Mersenne Twister generator\n    double sum = 0;\n    for (int i = 0; i < sample_size; ++i)\n        sum += dist(mt);\n    return sum / sample_size;\n}\n\ntemplate <typename TDistribution>\nvoid test_mean(TDistribution dist, std::string name, double true_mean, double true_variance)\n{\n    double true_stdev = sqrt(true_variance);\n    std::cout << \"Testing \" << name << \" distribution\\n\";\n    int sample_size = 10000;\n    double mean = sample_mean(dist, sample_size);\n    std::cout << \"Computed mean: \" << mean << \"\\n\";\n    double lower = true_mean - true_stdev/sqrt((double)sample_size);\n    double upper = true_mean + true_stdev/sqrt((double)sample_size);\n    std::cout << \"Expected a value between \" << lower << \" and \" << upper << \"\\n\\n\";\n}\n\nint main()\n{\n    int n;\n    double p, lambda, shape, mu, sigma;\n\n    n = 5; p = 0.3;\n    std::tr1::binomial_distribution<int, double> binomial(n, p);\n    test_mean(binomial, \"binomial\", n*p, n*p*(1-p));\n\n    lambda = 4.0;\n    std::tr1::exponential_distribution<double> exponential(lambda);\n    test_mean(exponential, \"exponential\", 1.0/lambda, 1.0/(lambda*lambda));\n\n    shape = 3.0;\n    std::tr1::gamma_distribution<double> gamma(shape);\n    test_mean(gamma, \"gamma\", shape, shape);\n\n    p = 0.1;\n    std::tr1::geometric_distribution<int, double> geometric(p);\n    test_mean(geometric, \"geometric\", 1.0/p, (1.0 - p)/(p*p));\n\n    mu = 3.0; sigma = 4.0;\n    std::tr1::normal_distribution<double> normal(mu, sigma);\n    test_mean(normal, \"normal\", mu, sigma*sigma);\n\n    lambda = 7.0;\n    std::tr1::poisson_distribution<double> poisson(7.0);\n    test_mean(poisson, \"poisson\", lambda, lambda);\n\n    p = 0.6;\n    std::tr1::bernoulli_distribution bernoulli(p);\n    test_mean(bernoulli, \"bernoulli\", p, p*(1-p));\n\n    return (0);\n}"}], "content": "Testing C++ TR1 random number generation\n\nBefore you use a random number generation library, it's a good idea to do a few tests. The tests might uncover a flaw in the library, but more likely they'll uncover a flaw in your understanding of the library. Parameterization expectations in particular are a common source of errors.\n\nThe following code does a crude test of the C++ TR1 random number generation classes. It computes the mean of 10,000 samples from each of the supported distributions and does a normal approximation to see whether the sample mean is roughly what one would expect. Don't be surprised if values fall outside the expected interval. But if you run the code several times with different RNG seeds, you shouldn't expect the same sample to go outside its predicted range very often.\n\nThis code is hardly a rigorous test of the distributions. But it provides sample code for working with the distributions and tests your understanding of the parameterizations.\n\n#include <random>\n#include <iostream>\n#include <string>\n\ntemplate <typename TDistribution>\ndouble sample_mean(TDistribution dist, int sample_size = 10000)\n{\n    std::tr1::mt19937 mt; // Mersenne Twister generator\n    double sum = 0;\n    for (int i = 0; i < sample_size; ++i)\n        sum += dist(mt);\n    return sum / sample_size;\n}\n\ntemplate <typename TDistribution>\nvoid test_mean(TDistribution dist, std::string name, double true_mean, double true_variance)\n{\n    double true_stdev = sqrt(true_variance);\n    std::cout << \"Testing \" << name << \" distribution\\n\";\n    int sample_size = 10000;\n    double mean = sample_mean(dist, sample_size);\n    std::cout << \"Computed mean: \" << mean << \"\\n\";\n    double lower = true_mean - true_stdev/sqrt((double)sample_size);\n    double upper = true_mean + true_stdev/sqrt((double)sample_size);\n    std::cout << \"Expected a value between \" << lower << \" and \" << upper << \"\\n\\n\";\n}\n\nint main()\n{\n    int n;\n    double p, lambda, shape, mu, sigma;\n\n    n = 5; p = 0.3;\n    std::tr1::binomial_distribution<int, double> binomial(n, p);\n    test_mean(binomial, \"binomial\", n*p, n*p*(1-p));\n\n    lambda = 4.0;\n    std::tr1::exponential_distribution<double> exponential(lambda);\n    test_mean(exponential, \"exponential\", 1.0/lambda, 1.0/(lambda*lambda));\n\n    shape = 3.0;\n    std::tr1::gamma_distribution<double> gamma(shape);\n    test_mean(gamma, \"gamma\", shape, shape);\n\n    p = 0.1;\n    std::tr1::geometric_distribution<int, double> geometric(p);\n    test_mean(geometric, \"geometric\", 1.0/p, (1.0 - p)/(p*p));\n\n    mu = 3.0; sigma = 4.0;\n    std::tr1::normal_distribution<double> normal(mu, sigma);\n    test_mean(normal, \"normal\", mu, sigma*sigma);\n\n    lambda = 7.0;\n    std::tr1::poisson_distribution<double> poisson(7.0);\n    test_mean(poisson, \"poisson\", lambda, lambda);\n\n    p = 0.6;\n    std::tr1::bernoulli_distribution bernoulli(p);\n    test_mean(bernoulli, \"bernoulli\", p, p*(1-p));\n\n    return (0);\n}"}
{"slug": "topological_vector_spaces", "canonical_url": "https://www.johndcook.com/blog/topological_vector_spaces/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/topological_vector_spaces.html", "title": "Topological Vector Spaces", "heading": null, "description": "Diagram showing the relationships between various kinds of topological vector spaces: Banach space, Fréchet space, Barreled, etc. Diagram and GraphViz source", "summary": null, "word_count": 0, "blocks": [], "content": ""}
{"slug": "topology_diagram", "canonical_url": "https://www.johndcook.com/blog/topology_diagram/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/topology_diagram.html", "title": "Topological properties diagram", "heading": null, "description": "Diagram showing which topological properties imply others: Hausdorff, compact, matrizable, etc. Diagram and GraphViz source", "summary": null, "word_count": 0, "blocks": [], "content": ""}
{"slug": "tr1regexquickstart", "canonical_url": "https://www.johndcook.com/blog/tr1regexquickstart/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/tr1regexquickstart.html", "title": "Quick start FAQ for C++ TR1 regular expressions", "heading": "Quick start FAQ for C++ TR1 regular expressions", "description": "A quick start guide for working with regular expressions in C++ using the TR1 extension to the Standard Library arranged in FAQ format.", "summary": "This is a FAQ in the sense of first asked questions. It's not intended to be a long list of questions but only the first questions people are likely to ask. This page is intentionally very short. For more details, see Getting started with C++ TR1 regular expressions.", "word_count": 226, "blocks": [{"tag": "h1", "text": "Quick start FAQ for C++ TR1 regular expressions"}, {"tag": "p", "text": "This is a FAQ in the sense of first asked questions. It's not intended to be a long list of questions but only the first questions people are likely to ask. This page is intentionally very short. For more details, see Getting started with C++ TR1 regular expressions."}, {"tag": "p", "text": "Q: Where can I get TR1?"}, {"tag": "p", "text": "A: Support for TR1 extensions in Visual Studio 2008 is added as a feature pack. Other implementations include Boost and Dinkumware."}, {"tag": "p", "text": "Q: What regular expression flavors are supported?"}, {"tag": "p", "text": "A: Depends on your implementation. Visual Studio 2008 supports these options: basic, extended, ECMAScript, awk, grep, egrep."}, {"tag": "p", "text": "Q: What header do I include?"}, {"tag": "p", "text": "A: <regex>"}, {"tag": "p", "text": "Q: What namespace are things in?"}, {"tag": "p", "text": "A: std::tr1"}, {"tag": "p", "text": "Q: How do I do a match?"}, {"tag": "p", "text": "A: Construct a regex object and pass it to regex_search."}, {"tag": "p", "text": "Q: How to I retrieve a match?"}, {"tag": "p", "text": "A: Use a form of regex_search that takes a match_result object as a parameter."}, {"tag": "p", "text": "Q: How do a I a replace?"}, {"tag": "p", "text": "A: Use regex_replace."}, {"tag": "p", "text": "Q: How do I do a global replace?"}, {"tag": "p", "text": "A: The function regex_replace does global replacements by default."}, {"tag": "p", "text": "Q: How do I keep from doing a global replace?"}, {"tag": "p", "text": "A: Use the format_first_only flag with regex_replace."}, {"tag": "p", "text": "Q: How do I make a regular expression case-insensitive?"}, {"tag": "p", "text": "A: Use the icase flag as a parameter to the regex constructor."}], "content": "Quick start FAQ for C++ TR1 regular expressions\n\nThis is a FAQ in the sense of first asked questions. It's not intended to be a long list of questions but only the first questions people are likely to ask. This page is intentionally very short. For more details, see Getting started with C++ TR1 regular expressions.\n\nQ: Where can I get TR1?\n\nA: Support for TR1 extensions in Visual Studio 2008 is added as a feature pack. Other implementations include Boost and Dinkumware.\n\nQ: What regular expression flavors are supported?\n\nA: Depends on your implementation. Visual Studio 2008 supports these options: basic, extended, ECMAScript, awk, grep, egrep.\n\nQ: What header do I include?\n\nA: <regex>\n\nQ: What namespace are things in?\n\nA: std::tr1\n\nQ: How do I do a match?\n\nA: Construct a regex object and pass it to regex_search.\n\nQ: How to I retrieve a match?\n\nA: Use a form of regex_search that takes a match_result object as a parameter.\n\nQ: How do a I a replace?\n\nA: Use regex_replace.\n\nQ: How do I do a global replace?\n\nA: The function regex_replace does global replacements by default.\n\nQ: How do I keep from doing a global replace?\n\nA: Use the format_first_only flag with regex_replace.\n\nQ: How do I make a regular expression case-insensitive?\n\nA: Use the icase flag as a parameter to the regex constructor."}
{"slug": "troubleshooting_sweave", "canonical_url": "https://www.johndcook.com/blog/troubleshooting_sweave/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/troubleshooting_sweave.html", "title": "Troubleshooting Sweave", "heading": "Troubleshooting Sweave", "description": "Tips for troubleshooting Sweave, a tool for reproducible data analysis through literate programming with R and LaTeX", "summary": "The purpose of Sweave is to make statistical analyses reproducible. However, technical difficulties can keep the Sweave document from being itself reproducible. Here are some things that could go wrong.", "word_count": 961, "blocks": [{"tag": "h1", "text": "Troubleshooting Sweave"}, {"tag": "p", "text": "The purpose of Sweave is to make statistical analyses reproducible. However, technical difficulties can keep the Sweave document from being itself reproducible. Here are some things that could go wrong."}, {"tag": "li", "text": "Session contamination"}, {"tag": "li", "text": "Missing R packages"}, {"tag": "li", "text": "Missing LaTeX packages"}, {"tag": "li", "text": "Missing or modified data"}, {"tag": "li", "text": "Referenced files and paths"}, {"tag": "p", "text": "These notes are based on my experience using Sweave on Windows XP using R version 2.4.1 and MiKTeX version 2.5."}, {"tag": "h2", "text": "Session contamination"}, {"tag": "p", "text": "When you launch Sweave from an R session, Sweave inherits your session state. Any libraries that are loaded, any variables defined, etc. This makes it easy to mislead yourself into thinking that an Sweave file is self-contained, when in fact it implicitly depends on session state."}, {"tag": "p", "text": "You can protect yourself by running Sweave in batch mode or starting a new session before running Sweave. Here's a batch file to run Sweave from a Windows commandline. Assuming R and pdflatex are in your path, you can save this code to a file, say sw.bat, and process a file foo.Rnw with the command sw foo."}, {"tag": "pre", "text": "R.exe -e \"Sweave('%1.Rnw')\"\n    pdflatex.exe %1.tex"}, {"tag": "p", "text": "This will protect you from writing Sweave files that appear to be self-contained but implicitly depend on session state."}, {"tag": "p", "text": "However, it's still possible that someone else receiving your Sweave file will could start Sweave with a session state that interferes with your file. One precaution would be to include sessionInfo() and ls() R commands at the top of your file. That wouldn't prevent contamination, but it would make it obvious."}, {"tag": "p", "text": "A more aggressive approach would be to include something like the command rm(list=ls()) at the top of the file to clean out the environment. That would be effective, but might upset the person running Sweave. You could also put the rm command at the bottom of your file to clean out the changes that Sweave makes to the environment that launched it."}, {"tag": "h2", "text": "Missing R packages"}, {"tag": "p", "text": "The Sweave package itself may be missing from R, although it is now a standard part of the R distribution. (At least as of version 2.4.1, possibly further back.)"}, {"tag": "p", "text": "If you're missing a library named foo, you will get an error message saying"}, {"tag": "p", "text": "Error in library(foo) : there is no package called 'foo'."}, {"tag": "p", "text": "Installing a package depends on where it came from. Standard R packages can be installed from CRAN. These are trivial to install from the R user interface by clicking on the Packages menu."}, {"tag": "p", "text": "BioConductor packages can also be installed from the Packages menu if you go to \"Select Repositories\" and add BioConductor to the list."}, {"tag": "p", "text": "Also, BioConductor packages can be installed from the command line by typing"}, {"tag": "p", "text": "source('http://bioconductor.org/biocLite.R'); biocLite('foo')"}, {"tag": "p", "text": "Do not install BioConductor packages by going to the BioConductor site and manually downloading files. For one, the search feature is buggy (as of February 2007). For another, the automatic installation options make sure the right version is downloaded along with its dependencies."}, {"tag": "p", "text": "Other packages, such as OOMPA for example, are distributed as binary libraries. To install such libraries, first save the binary files to your local disk. Then from the Packages menu select \"Install package(s) from local zip files...\""}, {"tag": "p", "text": "Then browse the location where the downloaded files were saved and control-click on each package you wish to install."}, {"tag": "h2", "text": "Missing LaTeX packages"}, {"tag": "p", "text": "MiKTeX version 2.5 does an excellent job of automatically installing LaTeX packages as needed. Installing packages with earlier versions was much more work."}, {"tag": "h2", "text": "Missing or modified data"}, {"tag": "p", "text": "One way to reduce the problem of missing data would be to have a version control system or at least a standard file system location for data sets. The Sweave file could extract the data from version control as its first step."}, {"tag": "p", "text": "Modifications to data can be detected by a checksum. An Sweave document could assert the checksum of the data file before doing any further processing."}, {"tag": "h2", "text": "Referenced files and paths"}, {"tag": "p", "text": "Complex LaTeX files often reference external files, assuming a given location for these files. Absolute paths are not portable, unless everyone organizes their local hard disk the same way. Such standardization might work within an organization, with effort, but will not work if you want to share files with the world at large. With relative paths, you can zip up your main file and all its dependencies, and anyone who unzips the bundle will have the file structure they need."}, {"tag": "p", "text": "Unfortunately, Sweave hard-codes the absolute path to its style files in its LaTeX output. The path to the style files depends on where R was installed on the local system. This means that while an Sweave document may be portable, the LaTeX file it produces is not. If you receive a LaTeX document produced by Sweave on someone else's computer, you will need to edit the path inside the \\usepackage statement so that it points to the Sweave style file location on your computer. If you want to give your LaTeX file to someone who does not have R, you can delete the reference to Sweave and paste in the contents of the Sweave.sty file."}, {"tag": "p", "text": "If R is installed under \"Program Files\", you will not be able to run LaTeX even on your own Sweave output without modification. Since the path to your R installation contains spaces, Sweave will insert a DOS-mangled path that LaTeX will choke on. Installing R in a location without spaces in the path, something like C:\\bin\\R, avoids the problem."}, {"tag": "h2", "text": "Compiling to LaTeX"}, {"tag": "p", "text": "Once everything is set up, you can compile an Sweave file to LaTeX by calling Sweave from R with the program file as an argument. For example, Sweave(\"C:/temp/foo.Rnw\"). This will produce a LaTeX file foo.tex, but the file may not appear where you expect. Rather than creating the file in the same directory as its source, R drops the file in R's current working directory."}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Blog"}, {"tag": "li", "text": "Consulting"}, {"tag": "li", "text": "Contact"}], "content": "Troubleshooting Sweave\n\nThe purpose of Sweave is to make statistical analyses reproducible. However, technical difficulties can keep the Sweave document from being itself reproducible. Here are some things that could go wrong.\n\nSession contamination\n\nMissing R packages\n\nMissing LaTeX packages\n\nMissing or modified data\n\nReferenced files and paths\n\nThese notes are based on my experience using Sweave on Windows XP using R version 2.4.1 and MiKTeX version 2.5.\n\nSession contamination\n\nWhen you launch Sweave from an R session, Sweave inherits your session state. Any libraries that are loaded, any variables defined, etc. This makes it easy to mislead yourself into thinking that an Sweave file is self-contained, when in fact it implicitly depends on session state.\n\nYou can protect yourself by running Sweave in batch mode or starting a new session before running Sweave. Here's a batch file to run Sweave from a Windows commandline. Assuming R and pdflatex are in your path, you can save this code to a file, say sw.bat, and process a file foo.Rnw with the command sw foo.\n\nR.exe -e \"Sweave('%1.Rnw')\"\n    pdflatex.exe %1.tex\n\nThis will protect you from writing Sweave files that appear to be self-contained but implicitly depend on session state.\n\nHowever, it's still possible that someone else receiving your Sweave file will could start Sweave with a session state that interferes with your file. One precaution would be to include sessionInfo() and ls() R commands at the top of your file. That wouldn't prevent contamination, but it would make it obvious.\n\nA more aggressive approach would be to include something like the command rm(list=ls()) at the top of the file to clean out the environment. That would be effective, but might upset the person running Sweave. You could also put the rm command at the bottom of your file to clean out the changes that Sweave makes to the environment that launched it.\n\nMissing R packages\n\nThe Sweave package itself may be missing from R, although it is now a standard part of the R distribution. (At least as of version 2.4.1, possibly further back.)\n\nIf you're missing a library named foo, you will get an error message saying\n\nError in library(foo) : there is no package called 'foo'.\n\nInstalling a package depends on where it came from. Standard R packages can be installed from CRAN. These are trivial to install from the R user interface by clicking on the Packages menu.\n\nBioConductor packages can also be installed from the Packages menu if you go to \"Select Repositories\" and add BioConductor to the list.\n\nAlso, BioConductor packages can be installed from the command line by typing\n\nsource('http://bioconductor.org/biocLite.R'); biocLite('foo')\n\nDo not install BioConductor packages by going to the BioConductor site and manually downloading files. For one, the search feature is buggy (as of February 2007). For another, the automatic installation options make sure the right version is downloaded along with its dependencies.\n\nOther packages, such as OOMPA for example, are distributed as binary libraries. To install such libraries, first save the binary files to your local disk. Then from the Packages menu select \"Install package(s) from local zip files...\"\n\nThen browse the location where the downloaded files were saved and control-click on each package you wish to install.\n\nMissing LaTeX packages\n\nMiKTeX version 2.5 does an excellent job of automatically installing LaTeX packages as needed. Installing packages with earlier versions was much more work.\n\nMissing or modified data\n\nOne way to reduce the problem of missing data would be to have a version control system or at least a standard file system location for data sets. The Sweave file could extract the data from version control as its first step.\n\nModifications to data can be detected by a checksum. An Sweave document could assert the checksum of the data file before doing any further processing.\n\nReferenced files and paths\n\nComplex LaTeX files often reference external files, assuming a given location for these files. Absolute paths are not portable, unless everyone organizes their local hard disk the same way. Such standardization might work within an organization, with effort, but will not work if you want to share files with the world at large. With relative paths, you can zip up your main file and all its dependencies, and anyone who unzips the bundle will have the file structure they need.\n\nUnfortunately, Sweave hard-codes the absolute path to its style files in its LaTeX output. The path to the style files depends on where R was installed on the local system. This means that while an Sweave document may be portable, the LaTeX file it produces is not. If you receive a LaTeX document produced by Sweave on someone else's computer, you will need to edit the path inside the \\usepackage statement so that it points to the Sweave style file location on your computer. If you want to give your LaTeX file to someone who does not have R, you can delete the reference to Sweave and paste in the contents of the Sweave.sty file.\n\nIf R is installed under \"Program Files\", you will not be able to run LaTeX even on your own Sweave output without modification. Since the path to your R installation contains spaces, Sweave will insert a DOS-mangled path that LaTeX will choke on. Installing R in a location without spaces in the path, something like C:\\bin\\R, avoids the problem.\n\nCompiling to LaTeX\n\nOnce everything is set up, you can compile an Sweave file to LaTeX by calling Sweave from R with the program file as an argument. For example, Sweave(\"C:/temp/foo.Rnw\"). This will produce a LaTeX file foo.tex, but the file may not appear where you expect. Rather than creating the file in the same directory as its source, R drops the file in R's current working directory.\n\nHome\n\nBlog\n\nConsulting\n\nContact"}
{"slug": "unicode", "canonical_url": "https://www.johndcook.com/blog/unicode/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/unicode.html", "title": "Unicode resources", "heading": "Unicode resources", "description": "Resources for Unicode: introductions to Unicode, resources for using Unicode in HTML, Python, LaTeX, XML, etc.", "summary": "Unicode is essentially a universal character set. It contains nearly every character in every human language. However, Unicode is subtle. As I point out in my blog article on Unicode, it's hard to say anything pithy about Unicode that is entirely correct. Every simple statement requires footnotes. Here are some resources I've found useful in understanding and using Unicode.", "word_count": 351, "blocks": [{"tag": "h1", "text": "Unicode resources"}, {"tag": "p", "text": "Unicode is essentially a universal character set. It contains nearly every character in every human language. However, Unicode is subtle. As I point out in my blog article on Unicode, it's hard to say anything pithy about Unicode that is entirely correct. Every simple statement requires footnotes. Here are some resources I've found useful in understanding and using Unicode."}, {"tag": "p", "text": "The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky Great introduction to Unicode for developers, as the title suggests."}, {"tag": "p", "text": "Unicode Standard by the Unicode Consortium The 1472-page tome is the indispensable reference for Unicode. The Unicode Consortium has made much of the information in this book available online."}, {"tag": "p", "text": "In general, Unicode characters can be inserted into HTML by putting their hexadecimal representation between &#x and a semicolon. For example, the Greek theta (θ) can be inserted into HTML by typing &#x03b8;. Some commonly used characters have mnemonic counterparts, such as &theta; for θ. However, there are only 252 such HTML entities and over 40,000 Unicode characters. Also, in general HTML mnemonic entities cannot be used in XML. There are four exceptions: &amp;, &gt;, &lt;, and &quot;. Note that just because a character is legal HTML does not mean the client's browser will display it or display it correctly. See also math symbols and Greek letters."}, {"tag": "p", "text": "Unicode in XML Unicode characters can be inserted into XML by quoting their code point numbers in hexadecimal, much like HTML. However, some characters are illegal or at least discouraged because they could confuse XML processors."}, {"tag": "p", "text": "XeTeX XeTeX is a version of TeX that works with Unicode. There is a XeLaTeX version of LaTeX as well."}, {"tag": "p", "text": "There Ain't No Such Thing as Plain Text by Jeff Atwood Mostly about Unicode encodings such as UTF-8."}, {"tag": "p", "text": "Unicode and ISO 10646 Why these are not exactly the same thing and just what the relationship between the two is."}, {"tag": "p", "text": "Unicode Explained by Jukka Korpela. This book gets into many of the issues surrounding Unicode that are not part of Unicode per se, such as internationalization and software compatibility."}], "content": "Unicode resources\n\nUnicode is essentially a universal character set. It contains nearly every character in every human language. However, Unicode is subtle. As I point out in my blog article on Unicode, it's hard to say anything pithy about Unicode that is entirely correct. Every simple statement requires footnotes. Here are some resources I've found useful in understanding and using Unicode.\n\nThe Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky Great introduction to Unicode for developers, as the title suggests.\n\nUnicode Standard by the Unicode Consortium The 1472-page tome is the indispensable reference for Unicode. The Unicode Consortium has made much of the information in this book available online.\n\nIn general, Unicode characters can be inserted into HTML by putting their hexadecimal representation between &#x and a semicolon. For example, the Greek theta (θ) can be inserted into HTML by typing &#x03b8;. Some commonly used characters have mnemonic counterparts, such as &theta; for θ. However, there are only 252 such HTML entities and over 40,000 Unicode characters. Also, in general HTML mnemonic entities cannot be used in XML. There are four exceptions: &amp;, &gt;, &lt;, and &quot;. Note that just because a character is legal HTML does not mean the client's browser will display it or display it correctly. See also math symbols and Greek letters.\n\nUnicode in XML Unicode characters can be inserted into XML by quoting their code point numbers in hexadecimal, much like HTML. However, some characters are illegal or at least discouraged because they could confuse XML processors.\n\nXeTeX XeTeX is a version of TeX that works with Unicode. There is a XeLaTeX version of LaTeX as well.\n\nThere Ain't No Such Thing as Plain Text by Jeff Atwood Mostly about Unicode encodings such as UTF-8.\n\nUnicode and ISO 10646 Why these are not exactly the same thing and just what the relationship between the two is.\n\nUnicode Explained by Jukka Korpela. This book gets into many of the issues surrounding Unicode that are not part of Unicode per se, such as internationalization and software compatibility."}
{"slug": "wilson_hilferty", "canonical_url": "https://www.johndcook.com/blog/wilson_hilferty/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/wilson_hilferty.html", "title": "Wilson-Hilferty normal approximation to the Poisson distribution", "heading": "The Wilson-Hilferty approximation to the Poisson distribution", "description": "Notes on the Wilson-Hilferty normal approximation to the Poisson distribution.", "summary": "If X ~ Poisson(λ) with λ “large” then X is well approximated by a normal distribution. The approximation that falls out of the central limit theorem approximates the CDF of X by", "word_count": 241, "blocks": [{"tag": "h1", "text": "The Wilson-Hilferty approximation to the Poisson distribution"}, {"tag": "p", "text": "If X ~ Poisson(λ) with λ “large” then X is well approximated by a normal distribution. The approximation that falls out of the central limit theorem approximates the CDF of X by"}, {"tag": "p", "text": "FX(k) ≈ Φ((k + 0.5 - λ)/√ λ)."}, {"tag": "p", "text": "Here Φ is the CDF of a standard normal (Gaussian) random variable. The central limit theorem approximation is studied in these notes. The Wilson-Hilferty approximation improves on the classical approximation by using a non-linear transformation of the argument k. This approximation uses"}, {"tag": "p", "text": "FX(k) ≈ Φ((c - μ)/σ)"}, {"tag": "p", "text": "where c = (λ/(1+k))1/3, μ = 1 - 1/(9k + 9), and σ = 1/(3 √(1 + k))."}, {"tag": "h2", "text": "Example"}, {"tag": "p", "text": "The graph below gives the error for the normal approximation to the CDF of a Poission(10) random variable using FX(k) ≈ Φ((k + 0.5 - λ)/√ λ)."}, {"tag": "p", "text": "Here is the corresponding graph using the Wilson-Hilferty approximation."}, {"tag": "p", "text": "The maximum error in the classical approximation is 0.0207. The maximum error in the W-H approximation is 0.00049, about 42 times smaller. This is typical: the error is often a couple orders of magnitude smaller in the W-H approximation than the classical approximation."}, {"tag": "p", "text": "For more information, see \"Some Suggestions for Teaching About Normal Approximation to Poisson and Binomial Distribution Functions\" by Scott M. Lesch and Daniel R. Jeske, The American Statistician, August 2009, Vol 63, No 3."}, {"tag": "p", "text": "See also notes on the normal approximation to the beta, binomial, gamma, and student-t distributions."}], "content": "The Wilson-Hilferty approximation to the Poisson distribution\n\nIf X ~ Poisson(λ) with λ “large” then X is well approximated by a normal distribution. The approximation that falls out of the central limit theorem approximates the CDF of X by\n\nFX(k) ≈ Φ((k + 0.5 - λ)/√ λ).\n\nHere Φ is the CDF of a standard normal (Gaussian) random variable. The central limit theorem approximation is studied in these notes. The Wilson-Hilferty approximation improves on the classical approximation by using a non-linear transformation of the argument k. This approximation uses\n\nFX(k) ≈ Φ((c - μ)/σ)\n\nwhere c = (λ/(1+k))1/3, μ = 1 - 1/(9k + 9), and σ = 1/(3 √(1 + k)).\n\nExample\n\nThe graph below gives the error for the normal approximation to the CDF of a Poission(10) random variable using FX(k) ≈ Φ((k + 0.5 - λ)/√ λ).\n\nHere is the corresponding graph using the Wilson-Hilferty approximation.\n\nThe maximum error in the classical approximation is 0.0207. The maximum error in the W-H approximation is 0.00049, about 42 times smaller. This is typical: the error is often a couple orders of magnitude smaller in the W-H approximation than the classical approximation.\n\nFor more information, see \"Some Suggestions for Teaching About Normal Approximation to Poisson and Binomial Distribution Functions\" by Scott M. Lesch and Daniel R. Jeske, The American Statistician, August 2009, Vol 63, No 3.\n\nSee also notes on the normal approximation to the beta, binomial, gamma, and student-t distributions."}
{"slug": "windows_sans_mouse", "canonical_url": "https://www.johndcook.com/blog/windows_sans_mouse/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/windows_sans_mouse.html", "title": "Windows sans mouse", "heading": "Windows sans mouse", "description": "Resources for using Windows without a mouse", "summary": "Windows sans mouse is a project to help Windows users learn to use their keyboard more and their mouse less. Why would you want to do that? To save time. There's nothing wrong with using a mouse, and sometimes it's absolutely essential. But it's often faster to do without it. See more discussion here.", "word_count": 128, "blocks": [{"tag": "h1", "text": "Windows sans mouse"}, {"tag": "p", "text": "Windows sans mouse is a project to help Windows users learn to use their keyboard more and their mouse less. Why would you want to do that? To save time. There's nothing wrong with using a mouse, and sometimes it's absolutely essential. But it's often faster to do without it. See more discussion here."}, {"tag": "p", "text": "I've made a Twitter account @SansMouse with daily tips. You might want to get into the habit of relying more on the keyboard by practicing one tip per day. I also have a personal Twitter account @JohnDCook."}, {"tag": "p", "text": "I have 15 other Twitter tip accounts that are active. See the full list here."}, {"tag": "p", "text": "Related links:"}, {"tag": "p", "text": "Grouping basic keyboard shortcuts into patterns Keyboard shortcuts for Microsoft's most popular software Keyboard shortcuts for Firefox"}], "content": "Windows sans mouse\n\nWindows sans mouse is a project to help Windows users learn to use their keyboard more and their mouse less. Why would you want to do that? To save time. There's nothing wrong with using a mouse, and sometimes it's absolutely essential. But it's often faster to do without it. See more discussion here.\n\nI've made a Twitter account @SansMouse with daily tips. You might want to get into the habit of relying more on the keyboard by practicing one tip per day. I also have a personal Twitter account @JohnDCook.\n\nI have 15 other Twitter tip accounts that are active. See the full list here.\n\nRelated links:\n\nGrouping basic keyboard shortcuts into patterns Keyboard shortcuts for Microsoft's most popular software Keyboard shortcuts for Firefox"}
{"slug": "articles", "canonical_url": "https://www.johndcook.com/blog/articles/", "source_path": "/Users/owen.d.goode/Desktop/jbsites/johndcook/app/public/writing.html", "title": "Articles", "heading": "Writing", "description": "Journal articles and other writing by John D. Cook", "summary": "Here are various categories of things I've written.", "word_count": 33, "blocks": [{"tag": "h1", "text": "Writing"}, {"tag": "p", "text": "Here are various categories of things I've written."}, {"tag": "h2", "text": "Informal articles"}, {"tag": "p", "text": "Math and statistics Software Book reviews"}, {"tag": "h2", "text": "Formal articles"}, {"tag": "p", "text": "Journal articles"}, {"tag": "h2", "text": "Bloging and microblogging"}, {"tag": "p", "text": "My blog My Twitter accounts"}, {"tag": "h2", "text": "Search"}, {"tag": "p", "text": "Search"}, {"tag": "li", "text": "Home"}, {"tag": "li", "text": "Contact"}], "content": "Writing\n\nHere are various categories of things I've written.\n\nInformal articles\n\nMath and statistics Software Book reviews\n\nFormal articles\n\nJournal articles\n\nBloging and microblogging\n\nMy blog My Twitter accounts\n\nSearch\n\nSearch\n\nHome\n\nContact"}
